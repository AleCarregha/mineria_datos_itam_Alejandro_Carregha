<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Métodos de Boosting – Minería de Datos</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./05-arboles.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-2453fe3dad938b07a2e5eff64ea8abce.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-c1367505ed6638c8d4e510e1459ae853.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-2453fe3dad938b07a2e5eff64ea8abce.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Métodos de Boosting – Minería de Datos">
<meta property="og:description" content="">
<meta property="og:image" content="06-boosting_files/figure-html/fig-gradient-boosting-feature-importance-output-1.png">
<meta property="og:site_name" content="Minería de Datos">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./06-boosting.html"><span class="chapter-title">Métodos de Boosting</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Buscar" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Minería de Datos</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Alternar modo oscuro"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Buscar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Temario</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-requerimientos-computacion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Requerimientos computacionales</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-introduccion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introducción</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-principios.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Principios de aprendizaje supervisado</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Regresion lineal</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-regresion_lineal.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Regresión lineal</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./violaciones_supuestos_regresion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Violaciones de los Supuestos de Regresión Lineal</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./analisis_advertising_dataset.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Análisis de Regresión Lineal con el Dataset Advertising</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ejercicio_wine_quality.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Ejercicio: Análisis de Regresión con el Dataset Wine Quality</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-clasificacion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Clasificación</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-arboles.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Árboles de Decisión</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-boosting.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Métodos de Boosting</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Referencias</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Ejemplos</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduccion.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introducción a Python para Minería de Datos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regresion_lineal.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Descenso en gradiente con regresión lineal</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./california_housing_case_study.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Caso de Estudio: Predicción de Precios de Vivienda en California</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#introducción-al-boosting" id="toc-introducción-al-boosting" class="nav-link active" data-scroll-target="#introducción-al-boosting">Introducción al Boosting</a>
  <ul class="collapse">
  <li><a href="#el-concepto-central-del-boosting" id="toc-el-concepto-central-del-boosting" class="nav-link" data-scroll-target="#el-concepto-central-del-boosting">El Concepto Central del Boosting</a></li>
  <li><a href="#intuición-visual-boosting-en-acción" id="toc-intuición-visual-boosting-en-acción" class="nav-link" data-scroll-target="#intuición-visual-boosting-en-acción">Intuición Visual: Boosting en Acción</a></li>
  </ul></li>
  <li><a href="#boosting-vs-bagging-vs-random-forest" id="toc-boosting-vs-bagging-vs-random-forest" class="nav-link" data-scroll-target="#boosting-vs-bagging-vs-random-forest">Boosting vs Bagging vs Random Forest</a>
  <ul class="collapse">
  <li><a href="#tabla-comparativa" id="toc-tabla-comparativa" class="nav-link" data-scroll-target="#tabla-comparativa">Tabla Comparativa</a></li>
  <li><a href="#perspectiva-de-sesgo-varianza" id="toc-perspectiva-de-sesgo-varianza" class="nav-link" data-scroll-target="#perspectiva-de-sesgo-varianza">Perspectiva de Sesgo-Varianza</a></li>
  <li><a href="#comparación-de-curvas-de-aprendizaje" id="toc-comparación-de-curvas-de-aprendizaje" class="nav-link" data-scroll-target="#comparación-de-curvas-de-aprendizaje">Comparación de Curvas de Aprendizaje</a></li>
  <li><a href="#cuándo-usar-cada-método" id="toc-cuándo-usar-cada-método" class="nav-link" data-scroll-target="#cuándo-usar-cada-método">¿Cuándo Usar Cada Método?</a></li>
  </ul></li>
  <li><a href="#adaboost-adaptive-boosting" id="toc-adaboost-adaptive-boosting" class="nav-link" data-scroll-target="#adaboost-adaptive-boosting">AdaBoost: Adaptive Boosting</a>
  <ul class="collapse">
  <li><a href="#contexto-histórico-e-importancia" id="toc-contexto-histórico-e-importancia" class="nav-link" data-scroll-target="#contexto-histórico-e-importancia">Contexto Histórico e Importancia</a></li>
  <li><a href="#el-algoritmo-adaboost" id="toc-el-algoritmo-adaboost" class="nav-link" data-scroll-target="#el-algoritmo-adaboost">El Algoritmo AdaBoost</a></li>
  <li><a href="#ejemplo-paso-a-paso" id="toc-ejemplo-paso-a-paso" class="nav-link" data-scroll-target="#ejemplo-paso-a-paso">Ejemplo Paso a Paso</a></li>
  <li><a href="#adaboost-en-acción-visualización-completa" id="toc-adaboost-en-acción-visualización-completa" class="nav-link" data-scroll-target="#adaboost-en-acción-visualización-completa">AdaBoost en Acción: Visualización Completa</a></li>
  <li><a href="#análisis-de-rendimiento-y-comparación" id="toc-análisis-de-rendimiento-y-comparación" class="nav-link" data-scroll-target="#análisis-de-rendimiento-y-comparación">Análisis de Rendimiento y Comparación</a></li>
  <li><a href="#implementación-y-detalles-prácticos" id="toc-implementación-y-detalles-prácticos" class="nav-link" data-scroll-target="#implementación-y-detalles-prácticos">Implementación y Detalles Prácticos</a></li>
  </ul></li>
  <li><a href="#gradient-boosting" id="toc-gradient-boosting" class="nav-link" data-scroll-target="#gradient-boosting">Gradient Boosting</a>
  <ul class="collapse">
  <li><a href="#más-allá-de-adaboost-una-generalización-poderosa" id="toc-más-allá-de-adaboost-una-generalización-poderosa" class="nav-link" data-scroll-target="#más-allá-de-adaboost-una-generalización-poderosa">Más Allá de AdaBoost: Una Generalización Poderosa</a></li>
  <li><a href="#la-perspectiva-del-descenso-por-gradiente" id="toc-la-perspectiva-del-descenso-por-gradiente" class="nav-link" data-scroll-target="#la-perspectiva-del-descenso-por-gradiente">La Perspectiva del Descenso por Gradiente</a></li>
  <li><a href="#el-algoritmo-de-gradient-boosting" id="toc-el-algoritmo-de-gradient-boosting" class="nav-link" data-scroll-target="#el-algoritmo-de-gradient-boosting">El Algoritmo de Gradient Boosting</a></li>
  <li><a href="#funciones-de-pérdida" id="toc-funciones-de-pérdida" class="nav-link" data-scroll-target="#funciones-de-pérdida">Funciones de Pérdida</a></li>
  <li><a href="#ejemplo-de-regresión-ajuste-iterativo-de-residuales" id="toc-ejemplo-de-regresión-ajuste-iterativo-de-residuales" class="nav-link" data-scroll-target="#ejemplo-de-regresión-ajuste-iterativo-de-residuales">Ejemplo de Regresión: Ajuste Iterativo de Residuales</a></li>
  <li><a href="#hiperparámetros-un-análisis-profundo" id="toc-hiperparámetros-un-análisis-profundo" class="nav-link" data-scroll-target="#hiperparámetros-un-análisis-profundo">Hiperparámetros: Un Análisis Profundo</a></li>
  <li><a href="#early-stopping-detención-automática" id="toc-early-stopping-detención-automática" class="nav-link" data-scroll-target="#early-stopping-detención-automática">Early Stopping: Detención Automática</a></li>
  <li><a href="#feature-importance-e-interpretabilidad" id="toc-feature-importance-e-interpretabilidad" class="nav-link" data-scroll-target="#feature-importance-e-interpretabilidad">Feature Importance e Interpretabilidad</a></li>
  </ul></li>
  <li><a href="#sec-modern-boosting" id="toc-sec-modern-boosting" class="nav-link" data-scroll-target="#sec-modern-boosting">5. Implementaciones Modernas de Boosting</a>
  <ul class="collapse">
  <li><a href="#sec-xgboost" id="toc-sec-xgboost" class="nav-link" data-scroll-target="#sec-xgboost">5.1 XGBoost (eXtreme Gradient Boosting)</a></li>
  <li><a href="#sec-lightgbm" id="toc-sec-lightgbm" class="nav-link" data-scroll-target="#sec-lightgbm">5.2 LightGBM (Light Gradient Boosting Machine)</a></li>
  <li><a href="#sec-catboost" id="toc-sec-catboost" class="nav-link" data-scroll-target="#sec-catboost">5.3 CatBoost (Categorical Boosting)</a></li>
  <li><a href="#sec-comparative-analysis" id="toc-sec-comparative-analysis" class="nav-link" data-scroll-target="#sec-comparative-analysis">5.4 Análisis Comparativo</a></li>
  </ul></li>
  <li><a href="#sec-hyperparameters" id="toc-sec-hyperparameters" class="nav-link" data-scroll-target="#sec-hyperparameters">6. Hiperparámetros y Regularización</a>
  <ul class="collapse">
  <li><a href="#sec-learning-rate" id="toc-sec-learning-rate" class="nav-link" data-scroll-target="#sec-learning-rate">6.1 Learning Rate (Tasa de Aprendizaje)</a></li>
  <li><a href="#sec-n-estimators" id="toc-sec-n-estimators" class="nav-link" data-scroll-target="#sec-n-estimators">6.2 Número de Estimadores y Early Stopping</a></li>
  <li><a href="#sec-tree-structure" id="toc-sec-tree-structure" class="nav-link" data-scroll-target="#sec-tree-structure">6.3 Parámetros de Estructura del Árbol</a></li>
  <li><a href="#sec-subsampling" id="toc-sec-subsampling" class="nav-link" data-scroll-target="#sec-subsampling">6.4 Subsampling (Stochastic Gradient Boosting)</a></li>
  <li><a href="#sec-regularization" id="toc-sec-regularization" class="nav-link" data-scroll-target="#sec-regularization">6.5 Regularización</a></li>
  <li><a href="#sec-hyperparameters-summary" id="toc-sec-hyperparameters-summary" class="nav-link" data-scroll-target="#sec-hyperparameters-summary">6.6 Resumen de Hiperparámetros</a></li>
  </ul></li>
  <li><a href="#sec-boosting-conclusions" id="toc-sec-boosting-conclusions" class="nav-link" data-scroll-target="#sec-boosting-conclusions">7. Conclusiones</a>
  <ul class="collapse">
  <li><a href="#puntos-clave" id="toc-puntos-clave" class="nav-link" data-scroll-target="#puntos-clave">Puntos clave</a></li>
  <li><a href="#guía-de-decisión-rápida" id="toc-guía-de-decisión-rápida" class="nav-link" data-scroll-target="#guía-de-decisión-rápida">Guía de decisión rápida</a></li>
  <li><a href="#relación-con-otros-temas-del-curso" id="toc-relación-con-otros-temas-del-curso" class="nav-link" data-scroll-target="#relación-con-otros-temas-del-curso">Relación con otros temas del curso</a></li>
  <li><a href="#recomendaciones-prácticas-finales" id="toc-recomendaciones-prácticas-finales" class="nav-link" data-scroll-target="#recomendaciones-prácticas-finales">Recomendaciones prácticas finales</a></li>
  <li><a href="#recursos-adicionales" id="toc-recursos-adicionales" class="nav-link" data-scroll-target="#recursos-adicionales">Recursos adicionales</a></li>
  <li><a href="#próximos-pasos" id="toc-próximos-pasos" class="nav-link" data-scroll-target="#próximos-pasos">Próximos pasos</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-boosting" class="quarto-section-identifier"><span class="chapter-title">Métodos de Boosting</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introducción-al-boosting" class="level2">
<h2 class="anchored" data-anchor-id="introducción-al-boosting">Introducción al Boosting</h2>
<p>En el capítulo anterior estudiamos Random Forest, un método de ensamble que construye múltiples árboles de decisión de manera <strong>paralela e independiente</strong>, promediando sus predicciones para reducir la varianza. Random Forest es robusto, fácil de usar y funciona bien en una amplia variedad de problemas. Sin embargo, existe otra familia de métodos de ensamble que adopta una filosofía radicalmente diferente: los <strong>métodos de boosting</strong>.</p>
<p>A diferencia de los métodos de bagging, que construyen modelos independientes en paralelo, el boosting construye una secuencia de modelos <strong>de forma iterativa y adaptativa</strong>. Cada nuevo modelo se enfoca específicamente en corregir los errores cometidos por los modelos anteriores. Esta idea es intuitiva: si un estudiante está aprendiendo un tema difícil, no repite el mismo ejercicio una y otra vez esperando mejorar (como haría bagging). En su lugar, identifica sus errores, presta atención especial a los conceptos que no comprende bien, y practica específicamente en esas áreas débiles. Exactamente así funciona el boosting: es un proceso de aprendizaje adaptativo que se enfoca iterativamente en los casos más difíciles.</p>
<p>Esta estrategia ha demostrado ser extraordinariamente exitosa en la práctica. Los algoritmos de boosting, particularmente sus implementaciones modernas como XGBoost, LightGBM y CatBoost, dominan las competencias de machine learning como Kaggle, son ampliamente utilizados en la industria para problemas de predicción con datos estructurados (tablas), y han ganado reputación como los algoritmos de aprendizaje supervisado más efectivos para este tipo de datos. En este capítulo exploraremos por qué el boosting es tan poderoso, cómo funcionan sus principales variantes, y cómo aplicarlo efectivamente en problemas reales.</p>
<section id="el-concepto-central-del-boosting" class="level3">
<h3 class="anchored" data-anchor-id="el-concepto-central-del-boosting">El Concepto Central del Boosting</h3>
<p>La idea fundamental del boosting se puede resumir en una fórmula simple pero poderosa:</p>
<p><span class="math display">\[
\text{Aprendices débiles} + \text{Aprendizaje adaptativo} = \text{Aprendiz fuerte}
\]</span></p>
<p>Un <strong>aprendiz débil</strong> (<em>weak learner</em>) es un modelo que tiene un desempeño apenas mejor que el azar. Por ejemplo, en clasificación binaria, un modelo que acierta el 51% de las veces es un aprendiz débil (comparado con 50% de adivinar al azar). En la práctica, los árboles de decisión muy simples, llamados <strong>decision stumps</strong> (árboles de profundidad 1, con una sola división), son los aprendices débiles más comunes en boosting.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Aprendiz Débil (Weak Learner)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Un <strong>aprendiz débil</strong> es un modelo de predicción cuyo desempeño es ligeramente mejor que adivinar al azar, pero no necesariamente muy preciso. La teoría matemática del boosting garantiza que combinando múltiples aprendices débiles de forma adecuada, es posible construir un modelo arbitrariamente preciso, asumiendo que cada aprendiz débil es mejor que el azar.</p>
<p>Los árboles de decisión poco profundos (típicamente profundidad 1-3) son los aprendices débiles más utilizados en boosting porque:</p>
<ul>
<li>Son rápidos de entrenar</li>
<li>Tienen alto sesgo pero baja varianza</li>
<li>Pueden capturar interacciones entre variables</li>
<li>Son diferenciables (importante para gradient boosting)</li>
</ul>
</div>
</div>
<p>El boosting construye un modelo final como una <strong>combinación ponderada</strong> de estos aprendices débiles:</p>
<p><span class="math display">\[
F(x) = \sum_{m=1}^{M} \alpha_m h_m(x)
\]</span></p>
<p>donde: - <span class="math inline">\(F(x)\)</span> es la predicción final del modelo de boosting - <span class="math inline">\(M\)</span> es el número total de iteraciones (modelos débiles) - <span class="math inline">\(h_m(x)\)</span> es el <span class="math inline">\(m\)</span>-ésimo aprendiz débil - <span class="math inline">\(\alpha_m\)</span> es el peso asignado al modelo <span class="math inline">\(h_m(x)\)</span></p>
<p>La magia del boosting está en <strong>cómo</strong> construimos esta secuencia. Cada nuevo modelo <span class="math inline">\(h_m\)</span> no se entrena de manera independiente (como en bagging), sino que se enfoca específicamente en los ejemplos donde el modelo acumulado <span class="math inline">\(F_{m-1}(x) = \sum_{i=1}^{m-1} \alpha_i h_i(x)\)</span> tiene mayor error. En otras palabras:</p>
<ol type="1">
<li><strong>Iteración 1</strong>: Entrenamos un modelo simple en todos los datos</li>
<li><strong>Iteración 2</strong>: Identificamos dónde falló el primer modelo y entrenamos un segundo modelo que se enfoca en esos errores</li>
<li><strong>Iteración 3</strong>: Identificamos dónde falló la combinación de los dos primeros modelos y entrenamos un tercer modelo para corregir</li>
<li><strong>…y así sucesivamente</strong></li>
</ol>
<p>Este proceso adaptativo y secuencial es lo que distingue fundamentalmente al boosting de otros métodos de ensamble.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Diferencia Clave: Boosting vs Bagging
</div>
</div>
<div class="callout-body-container callout-body">
<p>La diferencia fundamental entre boosting y bagging se resume en dos dimensiones:</p>
<p><strong>Construcción:</strong> - <strong>Bagging (Random Forest)</strong>: Construye árboles <strong>en paralelo e independientemente</strong>. Cada árbol se entrena en una muestra bootstrap diferente sin comunicación entre ellos. - <strong>Boosting</strong>: Construye modelos <strong>secuencialmente y adaptativamente</strong>. Cada nuevo modelo depende explícitamente de los errores de los modelos anteriores.</p>
<p><strong>Objetivo:</strong> - <strong>Bagging</strong>: Reduce <strong>varianza</strong> promediando modelos complejos (árboles profundos) - <strong>Boosting</strong>: Reduce <strong>sesgo</strong> combinando modelos simples (árboles superficiales) que corrigen iterativamente los errores</p>
<p>Esta diferencia tiene consecuencias importantes: - Bagging es fácilmente paralelizable (todos los árboles pueden entrenarse simultáneamente) - Boosting debe entrenarse secuencialmente (cada modelo necesita los resultados del anterior) - Bagging es muy robusto al ruido y outliers - Boosting puede sobreajustar si no se regula cuidadosamente, especialmente en datos ruidosos</p>
</div>
</div>
</section>
<section id="intuición-visual-boosting-en-acción" class="level3">
<h3 class="anchored" data-anchor-id="intuición-visual-boosting-en-acción">Intuición Visual: Boosting en Acción</h3>
<p>Para entender cómo funciona el boosting en la práctica, consideremos un problema de regresión simple en una dimensión. Generaremos datos sintéticos con una función no lineal y veremos cómo el boosting construye iterativamente un modelo cada vez más preciso.</p>
<div id="cell-fig-boosting-intuition" class="cell" data-fig-height="8" data-fig-width="12" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Configurar el estilo de las gráficas</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'default'</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos sintéticos 1D</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> true_function(x):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Función verdadera: combinación de seno y tendencia lineal"""</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sin(<span class="dv">2</span> <span class="op">*</span> x) <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> x <span class="op">+</span> np.cos(x)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">150</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n_samples)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> true_function(X_train) <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.2</span>, n_samples)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Puntos para visualización</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>X_plot <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">300</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> true_function(X_plot.ravel())</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar modelos con diferente número de iteraciones</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>single_tree <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>single_tree.fit(X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), y_train)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>boosting_5 <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>boosting_5.fit(X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), y_train)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>boosting_20 <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>boosting_20.fit(X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), y_train)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicciones</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>y_single <span class="op">=</span> single_tree.predict(X_plot)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>y_boost_5 <span class="op">=</span> boosting_5.predict(X_plot)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>y_boost_20 <span class="op">=</span> boosting_20.predict(X_plot)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear figura con 4 subgráficas</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="co"># (a) Datos originales</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>ax.scatter(X_train, y_train, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">30</span>, edgecolors<span class="op">=</span><span class="st">'k'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Datos de entrenamiento'</span>)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>ax.plot(X_plot, y_true, <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Función verdadera'</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(a) Datos originales'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="co"># (b) Árbol único (aprendiz débil)</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>ax.scatter(X_train, y_train, alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">30</span>, edgecolors<span class="op">=</span><span class="st">'k'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Datos'</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>ax.plot(X_plot, y_true, <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Función verdadera'</span>)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>ax.plot(X_plot, y_single, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>, label<span class="op">=</span><span class="st">'Árbol único (débil)'</span>)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(b) Un solo aprendiz débil (árbol profundidad=2)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular y mostrar MSE</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>mse_single <span class="op">=</span> np.mean((y_train <span class="op">-</span> single_tree.predict(X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)))<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>ax.text(<span class="fl">0.05</span>, <span class="fl">0.95</span>, <span class="ss">f'MSE = </span><span class="sc">{</span>mse_single<span class="sc">:.3f}</span><span class="ss">'</span>, transform<span class="op">=</span>ax.transAxes,</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>        verticalalignment<span class="op">=</span><span class="st">'top'</span>, bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'wheat'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="co"># (c) Boosting con 5 iteraciones</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>ax.scatter(X_train, y_train, alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">30</span>, edgecolors<span class="op">=</span><span class="st">'k'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Datos'</span>)</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>ax.plot(X_plot, y_true, <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Función verdadera'</span>)</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>ax.plot(X_plot, y_boost_5, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>, label<span class="op">=</span><span class="st">'Boosting (5 iteraciones)'</span>)</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(c) Después de 5 iteraciones de boosting'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular y mostrar MSE</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>mse_boost5 <span class="op">=</span> np.mean((y_train <span class="op">-</span> boosting_5.predict(X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)))<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>ax.text(<span class="fl">0.05</span>, <span class="fl">0.95</span>, <span class="ss">f'MSE = </span><span class="sc">{</span>mse_boost5<span class="sc">:.3f}</span><span class="ss">'</span>, transform<span class="op">=</span>ax.transAxes,</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>        verticalalignment<span class="op">=</span><span class="st">'top'</span>, bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'lightblue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a><span class="co"># (d) Boosting con 20 iteraciones</span></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>ax.scatter(X_train, y_train, alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">30</span>, edgecolors<span class="op">=</span><span class="st">'k'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Datos'</span>)</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>ax.plot(X_plot, y_true, <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Función verdadera'</span>)</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>ax.plot(X_plot, y_boost_20, <span class="st">'purple'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>, label<span class="op">=</span><span class="st">'Boosting (20 iteraciones)'</span>)</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(d) Después de 20 iteraciones de boosting'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular y mostrar MSE</span></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>mse_boost20 <span class="op">=</span> np.mean((y_train <span class="op">-</span> boosting_20.predict(X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)))<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>ax.text(<span class="fl">0.05</span>, <span class="fl">0.95</span>, <span class="ss">f'MSE = </span><span class="sc">{</span>mse_boost20<span class="sc">:.3f}</span><span class="ss">'</span>, transform<span class="op">=</span>ax.transAxes,</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>        verticalalignment<span class="op">=</span><span class="st">'top'</span>, bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'plum'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-boosting-intuition" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-boosting-intuition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-boosting_files/figure-html/fig-boosting-intuition-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-boosting-intuition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.1: Demostración visual del proceso de boosting en un problema de regresión 1D. (a) Los datos originales con una función no lineal más ruido. (b) Un único árbol de decisión poco profundo (aprendiz débil) proporciona una aproximación muy burda. (c) Después de 5 iteraciones de boosting, el modelo comienza a capturar la forma general de los datos. (d) Después de 20 iteraciones, el modelo se ajusta bien a la función subyacente, corrigiendo progresivamente los errores de las iteraciones anteriores.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Esta visualización ilustra el proceso fundamental del boosting:</p>
<ol type="1">
<li><p><strong>Panel (a)</strong>: Los datos originales muestran una relación no lineal con ruido. Un modelo lineal simple tendría alto sesgo en este problema.</p></li>
<li><p><strong>Panel (b)</strong>: Un único árbol de profundidad 2 (nuestro aprendiz débil) proporciona una aproximación muy burda con forma de escalera. Este modelo tiene alto sesgo (MSE alto) - claramente no captura bien la complejidad de los datos.</p></li>
<li><p><strong>Panel (c)</strong>: Después de 5 iteraciones, cada una agregando un nuevo árbol que corrige los errores de la combinación anterior, el modelo comienza a capturar la tendencia general. El MSE ha disminuido significativamente.</p></li>
<li><p><strong>Panel (d)</strong>: Con 20 iteraciones, el modelo final se ajusta muy bien a la función verdadera. Cada iteración agregó correcciones incrementales, construyendo colaborativamente una función compleja a partir de piezas simples.</p></li>
</ol>
<p>Veamos ahora cómo evolucionan los <strong>residuales</strong> (errores) a través de las iteraciones, que es donde realmente se aprecia la naturaleza adaptativa del boosting:</p>
<div id="cell-fig-boosting-residuals" class="cell" data-fig-height="4" data-fig-width="12" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar modelos intermedios para ver la evolución de residuales</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>boosting_1 <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>boosting_1.fit(X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), y_train)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular residuales</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>residuals_1 <span class="op">=</span> y_train <span class="op">-</span> boosting_1.predict(X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>residuals_5 <span class="op">=</span> y_train <span class="op">-</span> boosting_5.predict(X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>residuals_20 <span class="op">=</span> y_train <span class="op">-</span> boosting_20.predict(X_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear figura</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co"># (a) Residuales después de 1 iteración</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>ax.scatter(X_train, residuals_1, alpha<span class="op">=</span><span class="fl">0.6</span>, s<span class="op">=</span><span class="dv">40</span>, c<span class="op">=</span>np.<span class="bu">abs</span>(residuals_1),</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>           cmap<span class="op">=</span><span class="st">'Reds'</span>, edgecolors<span class="op">=</span><span class="st">'k'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>ax.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Residual (y - ŷ)'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(a) Residuales después de 1 iteración'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>std_1 <span class="op">=</span> np.std(residuals_1)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>ax.text(<span class="fl">0.05</span>, <span class="fl">0.95</span>, <span class="ss">f'Std(residuales) = </span><span class="sc">{</span>std_1<span class="sc">:.3f}</span><span class="ss">'</span>, transform<span class="op">=</span>ax.transAxes,</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        verticalalignment<span class="op">=</span><span class="st">'top'</span>, bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'wheat'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="co"># (b) Residuales después de 5 iteraciones</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>ax.scatter(X_train, residuals_5, alpha<span class="op">=</span><span class="fl">0.6</span>, s<span class="op">=</span><span class="dv">40</span>, c<span class="op">=</span>np.<span class="bu">abs</span>(residuals_5),</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>           cmap<span class="op">=</span><span class="st">'Reds'</span>, edgecolors<span class="op">=</span><span class="st">'k'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>ax.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Residual (y - ŷ)'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(b) Residuales después de 5 iteraciones'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>std_5 <span class="op">=</span> np.std(residuals_5)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>ax.text(<span class="fl">0.05</span>, <span class="fl">0.95</span>, <span class="ss">f'Std(residuales) = </span><span class="sc">{</span>std_5<span class="sc">:.3f}</span><span class="ss">'</span>, transform<span class="op">=</span>ax.transAxes,</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        verticalalignment<span class="op">=</span><span class="st">'top'</span>, bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'lightblue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="co"># (c) Residuales después de 20 iteraciones</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">2</span>]</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>ax.scatter(X_train, residuals_20, alpha<span class="op">=</span><span class="fl">0.6</span>, s<span class="op">=</span><span class="dv">40</span>, c<span class="op">=</span>np.<span class="bu">abs</span>(residuals_20),</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>           cmap<span class="op">=</span><span class="st">'Reds'</span>, edgecolors<span class="op">=</span><span class="st">'k'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>ax.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Residual (y - ŷ)'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(c) Residuales después de 20 iteraciones'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>std_20 <span class="op">=</span> np.std(residuals_20)</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>ax.text(<span class="fl">0.05</span>, <span class="fl">0.95</span>, <span class="ss">f'Std(residuales) = </span><span class="sc">{</span>std_20<span class="sc">:.3f}</span><span class="ss">'</span>, transform<span class="op">=</span>ax.transAxes,</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>        verticalalignment<span class="op">=</span><span class="st">'top'</span>, bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'plum'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-boosting-residuals" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-boosting-residuals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-boosting_files/figure-html/fig-boosting-residuals-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-boosting-residuals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.2: Evolución de los residuales durante el proceso de boosting. Los residuales son las diferencias entre los valores verdaderos y las predicciones del modelo acumulado. (a) Después de la primera iteración, los residuales son grandes y estructurados. (b) Después de 5 iteraciones, los residuales se han reducido considerablemente. (c) Después de 20 iteraciones, los residuales son pequeños y cercanos a cero, indicando que el modelo ha aprendido la función subyacente. Cada nueva iteración se enfoca en reducir estos residuales.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Los residuales nos muestran la historia completa del boosting:</p>
<ul>
<li><p><strong>Después de 1 iteración</strong>: Los residuales son grandes (desviación estándar alta) y muestran patrones claros. Hay regiones donde el modelo consistentemente subestima o sobreestima.</p></li>
<li><p><strong>Después de 5 iteraciones</strong>: Los residuales se han reducido considerablemente. Los patrones sistemáticos han disminuido, pero aún hay estructura que el modelo no ha capturado completamente.</p></li>
<li><p><strong>Después de 20 iteraciones</strong>: Los residuales son pequeños y se distribuyen aleatoriamente alrededor de cero. Esto indica que el modelo ha aprendido la señal subyacente y lo que queda es principalmente ruido irreducible.</p></li>
</ul>
<p><strong>La lección clave</strong>: Cada nueva iteración de boosting entrena un modelo que intenta predecir estos residuales, y luego lo suma al modelo acumulado. Este proceso de “corrección iterativa de errores” es la esencia del boosting, y es lo que le permite construir modelos complejos y precisos a partir de componentes simples.</p>
<p>En las siguientes secciones, exploraremos los algoritmos específicos que implementan esta idea general: desde AdaBoost, el primer método práctico de boosting, hasta gradient boosting y sus implementaciones modernas que dominan el campo del machine learning para datos estructurados.</p>
</section>
</section>
<section id="boosting-vs-bagging-vs-random-forest" class="level2">
<h2 class="anchored" data-anchor-id="boosting-vs-bagging-vs-random-forest">Boosting vs Bagging vs Random Forest</h2>
<p>Ahora que comprendemos la intuición básica del boosting, es importante posicionarlo claramente frente a otros métodos de ensamble que ya conocemos: bagging y Random Forest. Aunque todos estos métodos combinan múltiples modelos base para mejorar el rendimiento, difieren fundamentalmente en <strong>cómo</strong> construyen y combinan estos modelos, y en <strong>qué tipo de error</strong> están diseñados para reducir.</p>
<section id="tabla-comparativa" class="level3">
<h3 class="anchored" data-anchor-id="tabla-comparativa">Tabla Comparativa</h3>
<p>La siguiente tabla resume las diferencias clave entre estos tres métodos de ensamble:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 32%">
<col style="width: 18%">
<col style="width: 30%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Característica</th>
<th>Bagging</th>
<th>Random Forest</th>
<th>Boosting</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Construcción</strong></td>
<td>Paralela</td>
<td>Paralela</td>
<td><strong>Secuencial</strong></td>
</tr>
<tr class="even">
<td><strong>Dependencia</strong></td>
<td>Independiente</td>
<td>Independiente</td>
<td><strong>Adaptativa</strong></td>
</tr>
<tr class="odd">
<td><strong>Objetivo principal</strong></td>
<td>Reducir varianza</td>
<td>Reducir varianza</td>
<td><strong>Reducir sesgo</strong></td>
</tr>
<tr class="even">
<td><strong>Aprendices base</strong></td>
<td>Fuertes (árboles profundos)</td>
<td>Fuertes (árboles profundos)</td>
<td><strong>Débiles (árboles superficiales)</strong></td>
</tr>
<tr class="odd">
<td><strong>Muestreo de datos</strong></td>
<td>Bootstrap de filas</td>
<td>Bootstrap de filas</td>
<td>Pesos adaptativos o full data</td>
</tr>
<tr class="even">
<td><strong>Muestreo de features</strong></td>
<td>Todas las features</td>
<td>Subconjunto aleatorio</td>
<td>Todas las features</td>
</tr>
<tr class="odd">
<td><strong>Riesgo de sobreajuste</strong></td>
<td>Bajo</td>
<td>Muy bajo</td>
<td><strong>Medio-Alto</strong></td>
</tr>
<tr class="even">
<td><strong>Sensibilidad al ruido</strong></td>
<td>Baja</td>
<td>Muy baja</td>
<td><strong>Alta</strong></td>
</tr>
<tr class="odd">
<td><strong>Velocidad de entrenamiento</strong></td>
<td>Rápida (paralelizable)</td>
<td>Rápida (paralelizable)</td>
<td><strong>Más lenta (secuencial)</strong></td>
</tr>
<tr class="even">
<td><strong>Velocidad de predicción</strong></td>
<td>Media</td>
<td>Media</td>
<td>Rápida-Media</td>
</tr>
<tr class="odd">
<td><strong>Interpretabilidad</strong></td>
<td>Baja</td>
<td>Baja</td>
<td>Media</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Implicaciones de la Construcción Paralela vs Secuencial
</div>
</div>
<div class="callout-body-container callout-body">
<p>La diferencia entre construcción <strong>paralela</strong> (bagging/RF) y <strong>secuencial</strong> (boosting) tiene consecuencias prácticas importantes:</p>
<p><strong>Paralelización:</strong> - Bagging y Random Forest pueden entrenar todos los árboles simultáneamente en múltiples núcleos/máquinas - Boosting debe entrenar cada modelo después del anterior, limitando la paralelización - En sistemas distribuidos modernos, esto puede significar diferencias de velocidad de 10-100x</p>
<p><strong>Adaptación:</strong> - En bagging/RF, si un árbol comete errores, los otros árboles no lo “saben” - En boosting, cada modelo nuevo se construye específicamente para corregir los errores de los anteriores - Esto hace al boosting más “inteligente” pero también más susceptible a sobreajustar datos ruidosos</p>
</div>
</div>
</section>
<section id="perspectiva-de-sesgo-varianza" class="level3">
<h3 class="anchored" data-anchor-id="perspectiva-de-sesgo-varianza">Perspectiva de Sesgo-Varianza</h3>
<p>Para entender profundamente cuándo usar cada método, debemos revisar la descomposición del error en términos de sesgo y varianza (visto en el <span class="quarto-unresolved-ref">?sec-principios</span>).</p>
<p>Recordemos que el error esperado de predicción se puede descomponer como:</p>
<p><span class="math display">\[
\text{Error esperado} = \text{Sesgo}^2 + \text{Varianza} + \text{Ruido irreducible}
\]</span></p>
<ul>
<li><strong>Sesgo</strong>: Error por supuestos simplificadores en el modelo. Modelos simples (ej: regresión lineal) tienen alto sesgo.</li>
<li><strong>Varianza</strong>: Error por sensibilidad a fluctuaciones en los datos de entrenamiento. Modelos complejos (ej: árboles profundos) tienen alta varianza.</li>
</ul>
<p>Los tres métodos atacan diferentes partes de esta ecuación:</p>
<p><strong>Bagging y Random Forest</strong>: Reducen <strong>varianza</strong> - Comienzan con aprendices base que tienen <strong>baja sesgo</strong> pero <strong>alta varianza</strong> (árboles profundos sin poda) - Un solo árbol profundo sobreajusta y varía mucho entre muestras de entrenamiento - Promediando muchos árboles, la varianza se reduce: <span class="math inline">\(\text{Var}(\bar{X}) = \frac{\sigma^2}{n}\)</span> - El sesgo se mantiene aproximadamente igual (promedio de modelos insesgados es insesgado) - Random Forest agrega decorrelación entre árboles para mejorar aún más la reducción de varianza</p>
<p><strong>Boosting</strong>: Reduce <strong>sesgo</strong> - Comienza con aprendices base que tienen <strong>alto sesgo</strong> pero <strong>baja varianza</strong> (árboles superficiales, stumps) - Un solo árbol superficial es muy simple y subajusta (alto sesgo) - Combinando adaptativamente muchos modelos simples, cada uno corrigiendo los errores del anterior - La suma de muchos modelos simples crea un modelo complejo: el sesgo disminuye - La varianza aumenta un poco, pero se controla mediante regularización (learning rate, early stopping)</p>
<p>Visualicemos esto con un problema de clasificación concreto:</p>
<div id="cell-fig-bias-variance-comparison" class="cell" data-fig-height="8" data-fig-width="14" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier, GradientBoostingClassifier</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos sintéticos</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(n_samples<span class="op">=</span><span class="dv">300</span>, noise<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar modelos</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>single_deep_tree <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>single_deep_tree.fit(X_train, y_train)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>random_forest <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, max_depth<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>random_forest.fit(X_train, y_train)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>single_shallow_tree <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>single_shallow_tree.fit(X_train, y_train)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>boosting <span class="op">=</span> GradientBoostingClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, max_depth<span class="op">=</span><span class="dv">1</span>, learning_rate<span class="op">=</span><span class="fl">0.5</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>boosting.fit(X_train, y_train)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear malla para visualización</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> <span class="fl">0.02</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">0.5</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">0.5</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">0.5</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">0.5</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Función auxiliar para plotear fronteras de decisión</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_decision_boundary(ax, model, X, y, title, X_test<span class="op">=</span><span class="va">None</span>, y_test<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    ax.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.3</span>, cmap<span class="op">=</span><span class="st">'RdYlBu'</span>, levels<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">'RdYlBu'</span>, edgecolors<span class="op">=</span><span class="st">'k'</span>,</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>               s<span class="op">=</span><span class="dv">50</span>, linewidths<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Train'</span>)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> X_test <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        ax.scatter(X_test[:, <span class="dv">0</span>], X_test[:, <span class="dv">1</span>], c<span class="op">=</span>y_test, cmap<span class="op">=</span><span class="st">'RdYlBu'</span>,</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>                   edgecolors<span class="op">=</span><span class="st">'k'</span>, s<span class="op">=</span><span class="dv">50</span>, linewidths<span class="op">=</span><span class="fl">1.5</span>, alpha<span class="op">=</span><span class="fl">0.4</span>,</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>                   marker<span class="op">=</span><span class="st">'^'</span>, label<span class="op">=</span><span class="st">'Test'</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>    train_acc <span class="op">=</span> model.score(X, y)</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> X_test <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>        test_acc <span class="op">=</span> model.score(X_test, y_test)</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>        ax.text(<span class="fl">0.02</span>, <span class="fl">0.98</span>, <span class="ss">f'Train: </span><span class="sc">{</span>train_acc<span class="sc">:.3f}</span><span class="ch">\n</span><span class="ss">Test: </span><span class="sc">{</span>test_acc<span class="sc">:.3f}</span><span class="ss">'</span>,</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>                transform<span class="op">=</span>ax.transAxes, verticalalignment<span class="op">=</span><span class="st">'top'</span>,</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>                bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'white'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>),</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>                fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(xx.<span class="bu">min</span>(), xx.<span class="bu">max</span>())</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(yy.<span class="bu">min</span>(), yy.<span class="bu">max</span>())</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Feature 1'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Feature 2'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title, fontsize<span class="op">=</span><span class="dv">11</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>    ax.legend(fontsize<span class="op">=</span><span class="dv">8</span>, loc<span class="op">=</span><span class="st">'lower right'</span>)</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear figura</span></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">8</span>))</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>gs <span class="op">=</span> fig.add_gridspec(<span class="dv">2</span>, <span class="dv">3</span>, hspace<span class="op">=</span><span class="fl">0.3</span>, wspace<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a><span class="co"># (a) Datos originales</span></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(gs[<span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>ax.scatter(X_train[:, <span class="dv">0</span>], X_train[:, <span class="dv">1</span>], c<span class="op">=</span>y_train, cmap<span class="op">=</span><span class="st">'RdYlBu'</span>,</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>           edgecolors<span class="op">=</span><span class="st">'k'</span>, s<span class="op">=</span><span class="dv">50</span>, linewidths<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Train'</span>)</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>ax.scatter(X_test[:, <span class="dv">0</span>], X_test[:, <span class="dv">1</span>], c<span class="op">=</span>y_test, cmap<span class="op">=</span><span class="st">'RdYlBu'</span>,</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>           edgecolors<span class="op">=</span><span class="st">'k'</span>, s<span class="op">=</span><span class="dv">50</span>, linewidths<span class="op">=</span><span class="fl">1.5</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, marker<span class="op">=</span><span class="st">'^'</span>, label<span class="op">=</span><span class="st">'Test'</span>)</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Feature 1'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Feature 2'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(a) Datos originales (make_moons)'</span>, fontsize<span class="op">=</span><span class="dv">11</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a><span class="co"># (b) Árbol profundo único (alta varianza, bajo sesgo)</span></span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(gs[<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(ax, single_deep_tree, X_train, y_train,</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>                       <span class="st">'(b) Árbol único profundo</span><span class="ch">\n</span><span class="st">(Bajo sesgo, Alta varianza)'</span>,</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>                       X_test, y_test)</span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a><span class="co"># (c) Random Forest (baja varianza, bajo sesgo)</span></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(gs[<span class="dv">0</span>, <span class="dv">2</span>])</span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(ax, random_forest, X_train, y_train,</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>                       <span class="st">'(c) Random Forest</span><span class="ch">\n</span><span class="st">(Bajo sesgo, Baja varianza)'</span>,</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>                       X_test, y_test)</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a><span class="co"># (d) Árbol superficial único (alto sesgo, baja varianza)</span></span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(gs[<span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(ax, single_shallow_tree, X_train, y_train,</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>                       <span class="st">'(d) Árbol único superficial</span><span class="ch">\n</span><span class="st">(Alto sesgo, Baja varianza)'</span>,</span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>                       X_test, y_test)</span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a><span class="co"># (e) Boosting (reducción de sesgo)</span></span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(gs[<span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(ax, boosting, X_train, y_train,</span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>                       <span class="st">'(e) Gradient Boosting</span><span class="ch">\n</span><span class="st">(Bajo sesgo, Varianza controlada)'</span>,</span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a>                       X_test, y_test)</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a><span class="co"># (f) Diagrama conceptual sesgo-varianza</span></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(gs[<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>ax.text(<span class="fl">0.5</span>, <span class="fl">0.95</span>, <span class="st">'Trade-off Sesgo-Varianza'</span>, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'top'</span>,</span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>, transform<span class="op">=</span>ax.transAxes)</span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a><span class="co"># Dibujar ejes</span></span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a>ax.arrow(<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="dv">0</span>, head_width<span class="op">=</span><span class="fl">0.03</span>, head_length<span class="op">=</span><span class="fl">0.03</span>, fc<span class="op">=</span><span class="st">'black'</span>, ec<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a>ax.arrow(<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="fl">0.7</span>, head_width<span class="op">=</span><span class="fl">0.03</span>, head_length<span class="op">=</span><span class="fl">0.03</span>, fc<span class="op">=</span><span class="st">'black'</span>, ec<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>ax.text(<span class="fl">0.95</span>, <span class="fl">0.05</span>, <span class="st">'Sesgo →'</span>, ha<span class="op">=</span><span class="st">'right'</span>, va<span class="op">=</span><span class="st">'bottom'</span>, fontsize<span class="op">=</span><span class="dv">10</span>, transform<span class="op">=</span>ax.transAxes)</span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>ax.text(<span class="fl">0.05</span>, <span class="fl">0.85</span>, <span class="st">'Varianza</span><span class="ch">\n</span><span class="st">↑'</span>, ha<span class="op">=</span><span class="st">'left'</span>, va<span class="op">=</span><span class="st">'top'</span>, fontsize<span class="op">=</span><span class="dv">10</span>, transform<span class="op">=</span>ax.transAxes)</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a><span class="co"># Posicionar métodos</span></span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>methods <span class="op">=</span> {</span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Árbol profundo</span><span class="ch">\n</span><span class="st">(sin poda)'</span>: (<span class="fl">0.25</span>, <span class="fl">0.7</span>, <span class="st">'red'</span>),</span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Random Forest'</span>: (<span class="fl">0.25</span>, <span class="fl">0.35</span>, <span class="st">'green'</span>),</span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Árbol superficial</span><span class="ch">\n</span><span class="st">(stump)'</span>: (<span class="fl">0.75</span>, <span class="fl">0.25</span>, <span class="st">'orange'</span>),</span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Boosting'</span>: (<span class="fl">0.35</span>, <span class="fl">0.35</span>, <span class="st">'blue'</span>),</span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> method, (x, y, color) <span class="kw">in</span> methods.items():</span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, y, <span class="st">'o'</span>, markersize<span class="op">=</span><span class="dv">15</span>, color<span class="op">=</span>color, alpha<span class="op">=</span><span class="fl">0.6</span>, transform<span class="op">=</span>ax.transAxes)</span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a>    ax.text(x, y<span class="op">-</span><span class="fl">0.08</span>, method, ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'top'</span>, fontsize<span class="op">=</span><span class="fl">8.5</span>,</span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a>            fontweight<span class="op">=</span><span class="st">'bold'</span>, transform<span class="op">=</span>ax.transAxes)</span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a><span class="co"># Línea de error óptimo</span></span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a>x_line <span class="op">=</span> np.linspace(<span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="dv">100</span>)</span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a>y_line <span class="op">=</span> <span class="fl">0.15</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> (x_line <span class="op">-</span> <span class="fl">0.3</span>)<span class="op">**</span><span class="dv">2</span>  <span class="co"># Parábola</span></span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a>ax.plot(x_line, y_line, <span class="st">'k--'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, linewidth<span class="op">=</span><span class="dv">2</span>, transform<span class="op">=</span>ax.transAxes, label<span class="op">=</span><span class="st">'Error total'</span>)</span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a>ax.axis(<span class="st">'off'</span>)</span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-bias-variance-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bias-variance-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-boosting_files/figure-html/fig-bias-variance-comparison-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bias-variance-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.3: Comparación de métodos de ensamble desde la perspectiva sesgo-varianza en un problema de clasificación no lineal (make_moons). (a) Los datos tienen una estructura en forma de lunas entrelazadas con ruido. (b) Un árbol único profundo tiene bajo sesgo pero alta varianza (sobreajusta). (c) Random Forest mantiene bajo sesgo y reduce varianza significativamente. (d) Un árbol único superficial tiene alto sesgo pero baja varianza (subajusta). (e) Boosting reduce el sesgo progresivamente manteniendo la varianza controlada. Las fronteras de decisión ilustran cómo cada método equilibra este trade-off.
</figcaption>
</figure>
</div>
</div>
</div>
<p>La visualización anterior ilustra claramente las diferencias:</p>
<ul>
<li><p><strong>Panel (b) - Árbol profundo único</strong>: La frontera de decisión es extremadamente irregular, ajustándose a cada peculiaridad de los datos de entrenamiento. Alta precisión en train (casi 1.0) pero menor en test. Esto es <strong>alta varianza</strong> y <strong>bajo sesgo</strong>.</p></li>
<li><p><strong>Panel (c) - Random Forest</strong>: La frontera es suave pero captura bien la estructura en forma de luna. Precisión similar en train y test. Random Forest promedió 100 árboles profundos, reduciendo la varianza mientras mantiene bajo sesgo.</p></li>
<li><p><strong>Panel (d) - Árbol superficial único</strong>: La frontera es extremadamente simple (una línea recta), incapaz de capturar la complejidad de los datos. Esto es <strong>alto sesgo</strong> y <strong>baja varianza</strong>.</p></li>
<li><p><strong>Panel (e) - Gradient Boosting</strong>: La frontera captura bien la estructura no lineal sin sobreajustar excesivamente. Boosting combinó 100 árboles superficiales, cada uno corrigiendo errores del anterior, reduciendo el sesgo progresivamente.</p></li>
<li><p><strong>Panel (f)</strong>: El diagrama conceptual posiciona cada método en el espacio sesgo-varianza, mostrando que Random Forest y Boosting convergen a la zona de bajo error total desde direcciones opuestas.</p></li>
</ul>
</section>
<section id="comparación-de-curvas-de-aprendizaje" class="level3">
<h3 class="anchored" data-anchor-id="comparación-de-curvas-de-aprendizaje">Comparación de Curvas de Aprendizaje</h3>
<p>Otra forma de entender las diferencias es observar cómo evoluciona el error en entrenamiento y validación a medida que agregamos más modelos al ensamble:</p>
<div id="cell-fig-learning-curves-comparison" class="cell" data-fig-height="5" data-fig-width="12" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> log_loss</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar modelos con staged_predict para obtener predicciones en cada iteración</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">200</span>, max_depth<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                             warm_start<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>gb <span class="op">=</span> GradientBoostingClassifier(n_estimators<span class="op">=</span><span class="dv">200</span>, max_depth<span class="op">=</span><span class="dv">2</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                                 random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>rf.fit(X_train, y_train)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>gb.fit(X_train, y_train)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Para Random Forest, necesitamos entrenar incrementalmente</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>rf_train_errors <span class="op">=</span> []</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>rf_test_errors <span class="op">=</span> []</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_trees <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">5</span>):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    rf_temp <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span>n_trees, max_depth<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    rf_temp.fit(X_train, y_train)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    rf_train_errors.append(<span class="dv">1</span> <span class="op">-</span> rf_temp.score(X_train, y_train))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    rf_test_errors.append(<span class="dv">1</span> <span class="op">-</span> rf_temp.score(X_test, y_test))</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>rf_n_estimators <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">5</span>))</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Para Gradient Boosting, usamos staged_predict</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>gb_train_errors <span class="op">=</span> []</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>gb_test_errors <span class="op">=</span> []</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_pred, test_pred <span class="kw">in</span> <span class="bu">zip</span>(gb.staged_predict(X_train), gb.staged_predict(X_test)):</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    gb_train_errors.append(<span class="dv">1</span> <span class="op">-</span> np.mean(train_pred <span class="op">==</span> y_train))</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    gb_test_errors.append(<span class="dv">1</span> <span class="op">-</span> np.mean(test_pred <span class="op">==</span> y_test))</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>gb_n_estimators <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">201</span>))</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear figura</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="co"># (a) Random Forest</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>ax.plot(rf_n_estimators, rf_train_errors, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Error Train'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>ax.plot(rf_n_estimators, rf_test_errors, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Error Test'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Número de árboles'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Error de clasificación'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(a) Random Forest: Curvas de aprendizaje'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="dv">0</span>, <span class="fl">0.4</span>])</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Marcar punto de rendimiento estable</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>stable_point <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=</span>stable_point, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>ax.text(stable_point <span class="op">+</span> <span class="dv">5</span>, <span class="fl">0.35</span>, <span class="ss">f'Estable en ~</span><span class="sc">{</span>stable_point<span class="sc">}</span><span class="ss"> árboles'</span>,</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span>, color<span class="op">=</span><span class="st">'green'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a><span class="co"># (b) Gradient Boosting</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>ax.plot(gb_n_estimators, gb_train_errors, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Error Train'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>ax.plot(gb_n_estimators, gb_test_errors, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Error Test'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Número de iteraciones'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Error de clasificación'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(b) Gradient Boosting: Curvas de aprendizaje'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="dv">0</span>, <span class="fl">0.4</span>])</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Marcar punto óptimo (antes de que test error aumente)</span></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>best_n <span class="op">=</span> np.argmin(gb_test_errors)</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=</span>best_n, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>ax.plot(best_n, gb_test_errors[best_n], <span class="st">'go'</span>, markersize<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span><span class="ss">f'Óptimo (</span><span class="sc">{</span>best_n<span class="sc">}</span><span class="ss"> iter.)'</span>)</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>ax.text(best_n <span class="op">+</span> <span class="dv">5</span>, <span class="fl">0.35</span>, <span class="ss">f'Óptimo: </span><span class="sc">{</span>best_n<span class="sc">}</span><span class="ss"> iteraciones</span><span class="ch">\n</span><span class="ss">(early stopping)'</span>,</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span>, color<span class="op">=</span><span class="st">'green'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Marcar zona de sobreajuste</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> best_n <span class="op">&lt;</span> <span class="dv">180</span>:</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>    ax.axvspan(best_n <span class="op">+</span> <span class="dv">20</span>, <span class="dv">200</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Zona de sobreajuste'</span>)</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>    ax.text(best_n <span class="op">+</span> <span class="dv">30</span>, <span class="fl">0.05</span>, <span class="st">'Sobreajuste'</span>, fontsize<span class="op">=</span><span class="dv">9</span>, color<span class="op">=</span><span class="st">'darkred'</span>,</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>            fontweight<span class="op">=</span><span class="st">'bold'</span>, rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-learning-curves-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-learning-curves-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-boosting_files/figure-html/fig-learning-curves-comparison-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-learning-curves-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.4: Curvas de aprendizaje comparando Random Forest y Gradient Boosting. Para ambos métodos, se muestra el error de entrenamiento y validación a medida que se agregan más árboles al ensamble. (a) Random Forest: el error de entrenamiento y validación convergen rápidamente y se estabilizan. (b) Gradient Boosting: el error de entrenamiento continúa disminuyendo, pero el error de validación eventualmente comienza a aumentar si no se detiene a tiempo, indicando sobreajuste. Esto ilustra que boosting requiere más cuidado en la regularización.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Las curvas de aprendizaje revelan comportamientos distintivos:</p>
<p><strong>Random Forest (panel a)</strong>: - El error de entrenamiento y validación convergen rápidamente (en ~50 árboles) - Agregar más árboles mejora marginalmente o no cambia el rendimiento - <strong>No hay sobreajuste</strong>: ambas curvas se estabilizan juntas - Es seguro usar muchos árboles (100-500) sin preocuparse por sobreajuste</p>
<p><strong>Gradient Boosting (panel b)</strong>: - El error de entrenamiento continúa disminuyendo monotónicamente - El error de validación disminuye inicialmente pero puede aumentar después - <strong>Riesgo de sobreajuste</strong>: si entrenamos demasiadas iteraciones - Es crucial usar <strong>early stopping</strong>: detener cuando el error de validación deja de mejorar - En este ejemplo, el óptimo está alrededor de 70-100 iteraciones</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cuándo usar Bagging vs Boosting
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Usa Bagging (o Random Forest) cuando:</strong> - Los datos tienen <strong>mucho ruido</strong> o outliers - Prefieres un modelo robusto que “no se rompa” fácilmente - Necesitas paralelización para datasets muy grandes - Quieres un modelo “plug-and-play” con pocos hiperparámetros - No te importa un tiempo de predicción ligeramente mayor</p>
<p><strong>Usa Boosting cuando:</strong> - Los datos son <strong>relativamente limpios</strong> con etiquetas confiables - Tienes un modelo con <strong>alto sesgo</strong> que necesitas mejorar - Estás dispuesto a invertir tiempo en ajustar hiperparámetros - Necesitas extraer el máximo rendimiento del modelo - Puedes monitorear y usar validación cruzada o early stopping</p>
<p><strong>Regla general</strong>: Si tienes dudas, empieza con <strong>Random Forest</strong>. Es más robusto y perdona errores. Si Random Forest funciona bien pero quieres apretar hasta la última gota de performance, prueba <strong>boosting</strong> cuidadosamente.</p>
</div>
</div>
</section>
<section id="cuándo-usar-cada-método" class="level3">
<h3 class="anchored" data-anchor-id="cuándo-usar-cada-método">¿Cuándo Usar Cada Método?</h3>
<p>Para ayudar en la decisión, aquí hay una guía práctica:</p>
<p><strong>Situaciones donde Random Forest es superior:</strong> 1. <strong>Datos muy ruidosos</strong>: Con muchos outliers o errores de etiquetado 2. <strong>Datasets desbalanceados</strong>: Donde ciertas clases son raras 3. <strong>Features de alta cardinalidad</strong>: Variables categóricas con muchos niveles 4. <strong>Tiempo limitado</strong>: Necesitas resultados rápidos sin mucho tuning 5. <strong>Entrenamiento distribuido</strong>: Tienes muchas máquinas disponibles</p>
<p><strong>Situaciones donde Boosting es superior:</strong> 1. <strong>Datos limpios y bien curados</strong>: Con etiquetas confiables 2. <strong>Modelos simples fracasan</strong>: Alto sesgo que necesitas reducir 3. <strong>Competencias de ML</strong>: Donde cada 0.1% de accuracy importa 4. <strong>Features informativas</strong>: Pocas features realmente útiles que boosting puede aprovechar 5. <strong>Interpretabilidad relativa</strong>: Necesitas feature importance y explicaciones</p>
<p><strong>Casos ambiguos - prueba ambos:</strong> - Datasets de tamaño medio (~1K-100K filas) - Problemas de regresión con métricas cuadráticas - Datos tabulares estándar sin características extremas - Cuando tienes tiempo para experimentación</p>
<p>En la práctica, muchos científicos de datos <strong>entrenan ambos</strong> y usan validación cruzada para decidir. Los mejores modelos a menudo son <strong>ensambles de ensambles</strong>: combinaciones de Random Forest y Boosting que capturan lo mejor de ambos mundos.</p>
<p>En las siguientes secciones, profundizaremos en los algoritmos específicos de boosting, comenzando con AdaBoost, el primero en demostrar que esta idea funcionaba en la práctica.</p>
</section>
</section>
<section id="adaboost-adaptive-boosting" class="level2">
<h2 class="anchored" data-anchor-id="adaboost-adaptive-boosting">AdaBoost: Adaptive Boosting</h2>
<section id="contexto-histórico-e-importancia" class="level3">
<h3 class="anchored" data-anchor-id="contexto-histórico-e-importancia">Contexto Histórico e Importancia</h3>
<p>AdaBoost (Adaptive Boosting) fue desarrollado por Yoav Freund y Robert Schapire en 1996, convirtiéndose en el primer algoritmo práctico de boosting ampliamente exitoso. Su trabajo les valió el prestigioso Premio Gödel en 2003, uno de los reconocimientos más importantes en ciencias de la computación teórica.</p>
<p>Antes de AdaBoost, existían resultados teóricos que sugerían que era posible combinar aprendices débiles para crear un aprendiz fuerte, pero faltaba un algoritmo práctico y eficiente. AdaBoost resolvió este problema de manera elegante, proporcionando:</p>
<ol type="1">
<li><strong>Un algoritmo simple y práctico</strong>: Fácil de implementar y aplicar a diversos problemas</li>
<li><strong>Garantías teóricas fuertes</strong>: Pruebas matemáticas de convergencia y capacidad de generalización</li>
<li><strong>Excelente rendimiento empírico</strong>: Mejoras dramáticas en precisión comparado con métodos anteriores</li>
<li><strong>Interpretabilidad</strong>: Identificación clara de ejemplos difíciles mediante pesos</li>
</ol>
<p>AdaBoost fue revolucionario en su momento y sigue siendo relevante hoy en día, tanto como método práctico como fundamento teórico para algoritmos más modernos de boosting.</p>
</section>
<section id="el-algoritmo-adaboost" class="level3">
<h3 class="anchored" data-anchor-id="el-algoritmo-adaboost">El Algoritmo AdaBoost</h3>
<p>AdaBoost funciona manteniendo un <strong>vector de pesos</strong> sobre los ejemplos de entrenamiento. En cada iteración, entrena un clasificador débil en los datos ponderados, evalúa su rendimiento, y aumenta los pesos de los ejemplos mal clasificados para que el siguiente clasificador se enfoque en ellos.</p>
<p><strong>Algoritmo AdaBoost (para clasificación binaria):</strong></p>
<ol type="1">
<li><p><strong>Inicialización</strong>: Asignar pesos uniformes a todos los ejemplos <span class="math display">\[w_i^{(1)} = \frac{1}{n}, \quad i = 1, \ldots, n\]</span></p></li>
<li><p><strong>Para cada iteración</strong> <span class="math inline">\(m = 1, 2, \ldots, M\)</span>:</p>
<ol type="a">
<li><p><strong>Entrenar clasificador débil</strong> <span class="math inline">\(h_m(x)\)</span> en datos con pesos <span class="math inline">\(w^{(m)}\)</span></p></li>
<li><p><strong>Calcular tasa de error ponderada</strong>: <span class="math display">\[\epsilon_m = \frac{\sum_{i=1}^n w_i^{(m)} \mathbb{1}(h_m(x_i) \neq y_i)}{\sum_{i=1}^n w_i^{(m)}}\]</span></p></li>
<li><p><strong>Calcular peso del clasificador</strong> (importancia): <span class="math display">\[\alpha_m = \frac{1}{2} \ln\left(\frac{1 - \epsilon_m}{\epsilon_m}\right)\]</span></p></li>
<li><p><strong>Actualizar pesos de los ejemplos</strong>: <span class="math display">\[w_i^{(m+1)} = w_i^{(m)} \exp(-\alpha_m y_i h_m(x_i))\]</span></p></li>
<li><p><strong>Normalizar pesos</strong>: <span class="math inline">\(w^{(m+1)} \leftarrow w^{(m+1)} / \sum_i w_i^{(m+1)}\)</span></p></li>
</ol></li>
<li><p><strong>Predicción final</strong>: Combinación ponderada por votación <span class="math display">\[H(x) = \text{sign}\left(\sum_{m=1}^M \alpha_m h_m(x)\right)\]</span></p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
¿Por qué funciona la fórmula de <span class="math inline">\(\alpha_m\)</span>?
</div>
</div>
<div class="callout-body-container callout-body">
<p>La fórmula <span class="math inline">\(\alpha_m = \frac{1}{2}\ln\frac{1-\epsilon_m}{\epsilon_m}\)</span> no es arbitraria; surge naturalmente de la teoría de optimización.</p>
<p><strong>Interpretación intuitiva:</strong> - Si <span class="math inline">\(\epsilon_m \approx 0\)</span> (clasificador casi perfecto): <span class="math inline">\(\alpha_m \to +\infty\)</span> (peso muy alto) - Si <span class="math inline">\(\epsilon_m = 0.5\)</span> (clasificador aleatorio): <span class="math inline">\(\alpha_m = 0\)</span> (sin peso, se ignora) - Si <span class="math inline">\(\epsilon_m &gt; 0.5\)</span> (peor que azar): <span class="math inline">\(\alpha_m &lt; 0\)</span> (se invierte la predicción)</p>
<p><strong>Justificación matemática:</strong> La fórmula minimiza exponencialmente una cota superior del error de entrenamiento. Específicamente, AdaBoost puede verse como un algoritmo de descenso por coordenadas que minimiza la función de pérdida exponencial:</p>
<p><span class="math display">\[L = \sum_{i=1}^n \exp\left(-y_i \sum_{m=1}^M \alpha_m h_m(x_i)\right)\]</span></p>
<p>Esta conexión con la pérdida exponencial explica tanto el éxito como las limitaciones de AdaBoost (sensibilidad a outliers).</p>
</div>
</div>
</section>
<section id="ejemplo-paso-a-paso" class="level3">
<h3 class="anchored" data-anchor-id="ejemplo-paso-a-paso">Ejemplo Paso a Paso</h3>
<p>Para entender cómo AdaBoost adapta los pesos, consideremos un ejemplo simple con 10 puntos en 1D:</p>
<p><strong>Datos</strong>: <span class="math inline">\(x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\)</span> con etiquetas <span class="math inline">\(y = [-1, -1, -1, -1, -1, +1, +1, +1, +1, +1]\)</span></p>
<p>Los puntos están perfectamente separados en <span class="math inline">\(x = 5.5\)</span>, excepto que agregamos <strong>ruido</strong>: cambiamos la etiqueta del punto <span class="math inline">\(x=3\)</span> a <span class="math inline">\(+1\)</span> (outlier).</p>
<p><strong>Iteración 1:</strong> - Pesos iniciales: todos <span class="math inline">\(w_i = 0.1\)</span> (uniforme) - Clasificador débil: encuentra división óptima en <span class="math inline">\(x = 5.5\)</span> - Error: solo el outlier (<span class="math inline">\(x=3\)</span>) se clasifica mal, <span class="math inline">\(\epsilon_1 = 0.1\)</span> - Peso del clasificador: <span class="math inline">\(\alpha_1 = \frac{1}{2}\ln\frac{0.9}{0.1} \approx 1.10\)</span> - <strong>Actualización de pesos</strong>: el peso del outlier aumenta significativamente</p>
<p><strong>Iteración 2:</strong> - Ahora el outlier tiene peso ~0.3, mientras otros puntos tienen peso ~0.078 - El siguiente clasificador se enfoca más en el outlier - Puede encontrar una división que lo clasifique correctamente, pero comete errores en otros puntos</p>
<p>Este proceso continúa, con AdaBoost tratando cada vez más agresivamente de clasificar correctamente cada ejemplo, incluyendo outliers. Esto explica tanto su poder (no abandona ejemplos difíciles) como su debilidad (sensibilidad al ruido).</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Sensibilidad de AdaBoost a Outliers y Ruido
</div>
</div>
<div class="callout-body-container callout-body">
<p>AdaBoost tiene una vulnerabilidad importante: es <strong>muy sensible a outliers</strong> y datos con etiquetas erróneas.</p>
<p><strong>El problema:</strong> - Los pesos crecen exponencialmente: <span class="math inline">\(w_i^{(m+1)} = w_i^{(m)} \exp(\alpha_m)\)</span> para ejemplos mal clasificados - Si un ejemplo es imposible de clasificar correctamente (outlier o etiqueta errónea), su peso explotará - El algoritmo desperdicia iteraciones tratando de ajustarse a ruido irreducible</p>
<p><strong>Consecuencias prácticas:</strong> - En datasets limpios: AdaBoost funciona excelentemente - En datasets ruidosos: puede sobreajustar dramáticamente - Comparado con Random Forest: mucho menos robusto al ruido</p>
<p><strong>Soluciones:</strong> 1. <strong>Limpieza de datos</strong>: Identificar y corregir/remover outliers antes del entrenamiento 2. <strong>Variantes robustas</strong>: AdaBoost.R2 para regresión, LogitBoost, BrownBoost 3. <strong>Gradient Boosting</strong>: Más robusto con funciones de pérdida apropiadas (Huber, MAE) 4. <strong>Regularización</strong>: Limitar pesos máximos o usar learning rate &lt; 1</p>
<p><strong>Regla práctica</strong>: Si sospechas que tus datos tienen &gt;5-10% de etiquetas erróneas, considera Random Forest o Gradient Boosting en lugar de AdaBoost.</p>
</div>
</div>
</section>
<section id="adaboost-en-acción-visualización-completa" class="level3">
<h3 class="anchored" data-anchor-id="adaboost-en-acción-visualización-completa">AdaBoost en Acción: Visualización Completa</h3>
<p>Veamos cómo AdaBoost construye progresivamente su clasificador y cómo evolucionan los pesos de las muestras:</p>
<div id="cell-fig-adaboost-decision-boundaries" class="cell" data-fig-height="10" data-fig-width="14" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> AdaBoostClassifier</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generar datos sintéticos 2D</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    flip_y<span class="op">=</span><span class="fl">0.1</span>,  <span class="co"># 10% de ruido para hacer el problema interesante</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir datos</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar AdaBoost con diferentes números de estimadores</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>adaboost_models <span class="op">=</span> {}</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>n_estimators_list <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">50</span>]</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_est <span class="kw">in</span> n_estimators_list:</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    ada <span class="op">=</span> AdaBoostClassifier(</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        estimator<span class="op">=</span>DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span>n_est,</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        algorithm<span class="op">=</span><span class="st">'SAMME'</span>,</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    ada.fit(X_train, y_train)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    adaboost_models[n_est] <span class="op">=</span> ada</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear malla para visualización</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> <span class="fl">0.02</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Función para obtener pesos de las muestras después de entrenar</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_sample_weights(model, X, y):</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Aproximar los pesos finales de las muestras"""</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Para AdaBoost, los pesos no son directamente accesibles después del entrenamiento</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pero podemos aproximarlos viendo qué tan bien se clasifica cada muestra</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>    n_samples <span class="op">=</span> <span class="bu">len</span>(X)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> np.ones(n_samples)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simular el proceso de AdaBoost</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> estimator, alpha <span class="kw">in</span> <span class="bu">zip</span>(model.estimators_, model.estimator_weights_):</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> estimator.predict(X)</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>        incorrect <span class="op">=</span> (predictions <span class="op">!=</span> y)</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        weights[incorrect] <span class="op">*=</span> np.exp(alpha)</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normalizar</span></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> weights <span class="op">/</span> weights.<span class="bu">sum</span>() <span class="op">*</span> n_samples</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> weights</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear figura</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.ravel()</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>titles <span class="op">=</span> [<span class="st">'(a) Después de 1 iteración'</span>, <span class="st">'(b) Después de 5 iteraciones'</span>,</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>          <span class="st">'(c) Después de 10 iteraciones'</span>, <span class="st">'(d) Después de 50 iteraciones'</span>]</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, n_est <span class="kw">in</span> <span class="bu">enumerate</span>(n_estimators_list):</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[idx]</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> adaboost_models[n_est]</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predecir en la malla</span></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plotear frontera de decisión</span></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>    ax.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.3</span>, cmap<span class="op">=</span><span class="st">'RdYlBu'</span>, levels<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calcular pesos de las muestras</span></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>    sample_weights <span class="op">=</span> get_sample_weights(model, X_train, y_train)</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plotear puntos de entrenamiento con tamaño proporcional a los pesos</span></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>    scatter <span class="op">=</span> ax.scatter(X_train[:, <span class="dv">0</span>], X_train[:, <span class="dv">1</span>],</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>                        c<span class="op">=</span>y_train,</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>                        s<span class="op">=</span>sample_weights <span class="op">*</span> <span class="dv">100</span>,  <span class="co"># Escalar para visualización</span></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>                        cmap<span class="op">=</span><span class="st">'RdYlBu'</span>,</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>                        edgecolors<span class="op">=</span><span class="st">'k'</span>,</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>                        linewidths<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>                        alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plotear puntos de test</span></span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X_test[:, <span class="dv">0</span>], X_test[:, <span class="dv">1</span>],</span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>              c<span class="op">=</span>y_test,</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>              s<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>              cmap<span class="op">=</span><span class="st">'RdYlBu'</span>,</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>              edgecolors<span class="op">=</span><span class="st">'k'</span>,</span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>              linewidths<span class="op">=</span><span class="fl">1.5</span>,</span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>              alpha<span class="op">=</span><span class="fl">0.4</span>,</span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a>              marker<span class="op">=</span><span class="st">'^'</span>,</span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a>              label<span class="op">=</span><span class="st">'Test'</span>)</span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Métricas</span></span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>    train_acc <span class="op">=</span> model.score(X_train, y_train)</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>    test_acc <span class="op">=</span> model.score(X_test, y_test)</span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a>    ax.text(<span class="fl">0.02</span>, <span class="fl">0.98</span>, <span class="ss">f'Train: </span><span class="sc">{</span>train_acc<span class="sc">:.3f}</span><span class="ch">\n</span><span class="ss">Test: </span><span class="sc">{</span>test_acc<span class="sc">:.3f}</span><span class="ch">\n</span><span class="ss">Estimators: </span><span class="sc">{</span>n_est<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a>            transform<span class="op">=</span>ax.transAxes,</span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a>            verticalalignment<span class="op">=</span><span class="st">'top'</span>,</span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>            bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'white'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>),</span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a>            fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Feature 1'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Feature 2'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a>    ax.set_title(titles[idx], fontsize<span class="op">=</span><span class="dv">11</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(xx.<span class="bu">min</span>(), xx.<span class="bu">max</span>())</span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(yy.<span class="bu">min</span>(), yy.<span class="bu">max</span>())</span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a><span class="co"># Agregar leyenda sobre tamaño de puntos</span></span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a>fig.text(<span class="fl">0.5</span>, <span class="fl">0.02</span>, <span class="st">'Nota: El tamaño de los puntos de entrenamiento es proporcional a sus pesos en AdaBoost</span><span class="ch">\n</span><span class="st">'</span> <span class="op">+</span></span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a>         <span class="st">'(puntos más grandes = mayor peso = ejemplos más "difíciles")'</span>,</span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a>         ha<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">10</span>, style<span class="op">=</span><span class="st">'italic'</span>, bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'wheat'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>))</span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a>plt.tight_layout(rect<span class="op">=</span>[<span class="dv">0</span>, <span class="fl">0.03</span>, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/xwing/miniforge3/envs/mineria_datos/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:

The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.

/Users/xwing/miniforge3/envs/mineria_datos/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:

The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.

/Users/xwing/miniforge3/envs/mineria_datos/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:

The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.

/Users/xwing/miniforge3/envs/mineria_datos/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:

The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="fig-adaboost-decision-boundaries" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adaboost-decision-boundaries-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-boosting_files/figure-html/fig-adaboost-decision-boundaries-output-2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adaboost-decision-boundaries-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.5: Evolución de las fronteras de decisión de AdaBoost a través de las iteraciones. Se muestra un problema de clasificación binaria con datos sintéticos. (a) Después de 1 iteración: una frontera muy simple (decision stump). (b) Después de 5 iteraciones: la frontera comienza a capturar la estructura no lineal. (c) Después de 10 iteraciones: mejor ajuste a los datos. (d) Después de 50 iteraciones: frontera refinada que captura detalles finos. El tamaño de los puntos representa los pesos de las muestras, mostrando en qué ejemplos se enfoca el algoritmo.
</figcaption>
</figure>
</div>
</div>
</div>
<p>La visualización muestra cómo AdaBoost construye progresivamente su clasificador:</p>
<ul>
<li><p><strong>Panel (a) - 1 iteración</strong>: Un solo decision stump crea una frontera de decisión muy simple (una línea recta). Los puntos mal clasificados por este primer clasificador recibirán mayor peso.</p></li>
<li><p><strong>Panel (b) - 5 iteraciones</strong>: La frontera comienza a tomar forma no lineal, adaptándose a los patrones en los datos. Algunos puntos han crecido de tamaño (mayor peso) porque son consistentemente difíciles de clasificar.</p></li>
<li><p><strong>Panel (c) - 10 iteraciones</strong>: La frontera captura mejor la separación entre clases. Los puntos con mayor peso (más grandes) son típicamente aquellos cerca de la frontera de decisión o outliers.</p></li>
<li><p><strong>Panel (d) - 50 iteraciones</strong>: La frontera es muy refinada y captura detalles finos. Nótese que algunos puntos se han vuelto muy grandes (pesos muy altos), lo que podría indicar el inicio de sobreajuste, especialmente en datos ruidosos.</p></li>
</ul>
</section>
<section id="análisis-de-rendimiento-y-comparación" class="level3">
<h3 class="anchored" data-anchor-id="análisis-de-rendimiento-y-comparación">Análisis de Rendimiento y Comparación</h3>
<p>Veamos cómo evoluciona el error a medida que agregamos más estimadores, y comparemos con un árbol de decisión único:</p>
<div id="cell-fig-adaboost-learning-curves" class="cell" data-fig-height="5" data-fig-width="12" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar AdaBoost con muchos estimadores para ver curva completa</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>ada_full <span class="op">=</span> AdaBoostClassifier(</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    algorithm<span class="op">=</span><span class="st">'SAMME'</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>ada_full.fit(X_train, y_train)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular errores en cada etapa usando staged_predict</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>train_errors <span class="op">=</span> []</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>test_errors <span class="op">=</span> []</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_pred, test_pred <span class="kw">in</span> <span class="bu">zip</span>(ada_full.staged_predict(X_train),</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>                                   ada_full.staged_predict(X_test)):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    train_errors.append(<span class="dv">1</span> <span class="op">-</span> np.mean(train_pred <span class="op">==</span> y_train))</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    test_errors.append(<span class="dv">1</span> <span class="op">-</span> np.mean(test_pred <span class="op">==</span> y_test))</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar un árbol único para comparación</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>single_tree <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>single_tree.fit(X_train, y_train)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear figura</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="co"># (a) Curvas de aprendizaje</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>ax.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">201</span>), train_errors, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Error Train'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>ax.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">201</span>), test_errors, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Error Test'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Marcar error del árbol único</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>single_tree_error <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> single_tree.score(X_test, y_test)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>ax.axhline(y<span class="op">=</span>single_tree_error, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>           label<span class="op">=</span><span class="ss">f'Árbol único (test)'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Marcar punto óptimo</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>best_n <span class="op">=</span> np.argmin(test_errors) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=</span>best_n, color<span class="op">=</span><span class="st">'purple'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>ax.plot(best_n, test_errors[best_n<span class="op">-</span><span class="dv">1</span>], <span class="st">'mo'</span>, markersize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>ax.text(best_n <span class="op">+</span> <span class="dv">5</span>, test_errors[best_n<span class="op">-</span><span class="dv">1</span>], <span class="ss">f'Óptimo: </span><span class="sc">{</span>best_n<span class="sc">}</span><span class="ss"> iter.</span><span class="ch">\n</span><span class="ss">Error: </span><span class="sc">{</span>test_errors[best_n<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">'</span>,</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span>, color<span class="op">=</span><span class="st">'purple'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Número de estimadores'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Error de clasificación'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(a) AdaBoost: Curvas de aprendizaje'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">10</span>, loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="dv">0</span>, <span class="dv">200</span>])</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="dv">0</span>, <span class="fl">0.5</span>])</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a><span class="co"># (b) Comparación de importancia de features</span></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature importance de AdaBoost (usando el modelo óptimo)</span></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>ada_optimal <span class="op">=</span> AdaBoostClassifier(</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span>best_n,</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>    algorithm<span class="op">=</span><span class="st">'SAMME'</span>,</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>ada_optimal.fit(X_train, y_train)</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>ada_importance <span class="op">=</span> ada_optimal.feature_importances_</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>tree_importance <span class="op">=</span> single_tree.feature_importances_</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>x_pos <span class="op">=</span> np.arange(<span class="bu">len</span>(ada_importance))</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="fl">0.35</span></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>ax.bar(x_pos <span class="op">-</span> width<span class="op">/</span><span class="dv">2</span>, ada_importance, width, label<span class="op">=</span><span class="st">'AdaBoost'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'steelblue'</span>)</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>ax.bar(x_pos <span class="op">+</span> width<span class="op">/</span><span class="dv">2</span>, tree_importance, width, label<span class="op">=</span><span class="st">'Árbol único'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'coral'</span>)</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Feature'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Importancia'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(b) Importancia de Features'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x_pos)</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([<span class="st">'Feature 1'</span>, <span class="st">'Feature 2'</span>])</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a><span class="co"># Imprimir resumen</span></span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">60</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RESUMEN DE RENDIMIENTO"</span>)</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">60</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"AdaBoost (n_estimators=</span><span class="sc">{</span>best_n<span class="sc">}</span><span class="ss">):"</span>)</span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Accuracy Train: </span><span class="sc">{</span><span class="dv">1</span> <span class="op">-</span> train_errors[best_n<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Accuracy Test:  </span><span class="sc">{</span><span class="dv">1</span> <span class="op">-</span> test_errors[best_n<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Árbol Único (max_depth=5):"</span>)</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Accuracy Train: </span><span class="sc">{</span>single_tree<span class="sc">.</span>score(X_train, y_train)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Accuracy Test:  </span><span class="sc">{</span>single_tree<span class="sc">.</span>score(X_test, y_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Mejora de AdaBoost sobre árbol único: </span><span class="sc">{</span>(<span class="dv">1</span><span class="op">-</span>test_errors[best_n<span class="op">-</span><span class="dv">1</span>]) <span class="op">-</span> single_tree<span class="sc">.</span>score(X_test, y_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">60</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/xwing/miniforge3/envs/mineria_datos/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:

The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.

/Users/xwing/miniforge3/envs/mineria_datos/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning:

The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="fig-adaboost-learning-curves" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adaboost-learning-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-boosting_files/figure-html/fig-adaboost-learning-curves-output-2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adaboost-learning-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.6: Análisis de rendimiento de AdaBoost. (a) Curvas de aprendizaje mostrando la evolución del error de clasificación en train y test a medida que se agregan más estimadores. El error de test disminuye rápidamente al inicio y luego se estabiliza. (b) Comparación de importancia de features entre AdaBoost y un árbol de decisión único, mostrando cómo AdaBoost identifica las features más relevantes a través de múltiples iteraciones.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
============================================================
RESUMEN DE RENDIMIENTO
============================================================
AdaBoost (n_estimators=14):
  - Accuracy Train: 0.8143
  - Accuracy Test:  0.8000

Árbol Único (max_depth=5):
  - Accuracy Train: 0.8786
  - Accuracy Test:  0.7667

Mejora de AdaBoost sobre árbol único: 0.0333
============================================================
</code></pre>
</div>
</div>
<p><strong>Observaciones clave:</strong></p>
<ol type="1">
<li><p><strong>Convergencia rápida</strong>: AdaBoost alcanza buen rendimiento con relativamente pocas iteraciones (~10-30), luego mejora marginalmente.</p></li>
<li><p><strong>Comparación con árbol único</strong>: AdaBoost típicamente supera significativamente a un solo árbol de decisión, incluso uno más profundo.</p></li>
<li><p><strong>Riesgo de sobreajuste</strong>: Aunque en este ejemplo el sobreajuste es moderado, en datasets muy ruidosos se observaría una divergencia mayor entre train y test error.</p></li>
<li><p><strong>Feature importance</strong>: AdaBoost identifica features importantes promediando sobre múltiples clasificadores débiles, lo que puede ser más estable que un solo árbol.</p></li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cuándo usar AdaBoost
</div>
</div>
<div class="callout-body-container callout-body">
<p>AdaBoost funciona mejor en las siguientes situaciones:</p>
<p><strong>✅ Usar AdaBoost cuando:</strong> - Datos <strong>limpios</strong> con pocas etiquetas erróneas (&lt; 5%) - Clasificación binaria o multiclase bien balanceada - Necesitas interpretabilidad (pesos de ejemplos + feature importance) - Los aprendices débiles simples (stumps) son suficientes - Quieres un algoritmo teóricamente fundamentado - Dataset de tamaño pequeño a mediano (&lt; 100K ejemplos)</p>
<p><strong>❌ Evitar AdaBoost cuando:</strong> - Datos con <strong>mucho ruido</strong> o outliers significativos - Etiquetas poco confiables o errores de anotación - Clases muy desbalanceadas sin balanceo previo - Problemas de regresión (usar Gradient Boosting) - Dataset muy grande donde necesitas velocidad (considerar XGBoost/LightGBM)</p>
<p><strong>Alternativas:</strong> - Datos ruidosos → <strong>Random Forest</strong> o <strong>Gradient Boosting con loss robusta</strong> - Regresión → <strong>Gradient Boosting</strong> o <strong>XGBoost</strong> - Necesitas velocidad → <strong>LightGBM</strong> o <strong>XGBoost</strong> - Muchas features categóricas → <strong>CatBoost</strong></p>
</div>
</div>
</section>
<section id="implementación-y-detalles-prácticos" class="level3">
<h3 class="anchored" data-anchor-id="implementación-y-detalles-prácticos">Implementación y Detalles Prácticos</h3>
<p>En <code>scikit-learn</code>, AdaBoost es muy fácil de usar:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> AdaBoostClassifier</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># AdaBoost con decision stumps (configuración clásica)</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>ada <span class="op">=</span> AdaBoostClassifier(</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">1</span>),  <span class="co"># Aprendiz débil</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">50</span>,                                 <span class="co"># Número de iteraciones</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">1.0</span>,                               <span class="co"># Factor de shrinkage</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    algorithm<span class="op">=</span><span class="st">'SAMME.R'</span>,                             <span class="co"># SAMME.R usa probabilidades</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>ada.fit(X_train, y_train)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> ada.predict(X_test)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Hiperparámetros clave:</strong></p>
<ol type="1">
<li><strong><code>estimator</code></strong>: Clasificador base (usualmente <code>DecisionTreeClassifier(max_depth=1)</code>)
<ul>
<li>Stumps (profundidad 1) son más robustos</li>
<li>Árboles más profundos (2-3) pueden capturar interacciones</li>
</ul></li>
<li><strong><code>n_estimators</code></strong>: Número de clasificadores débiles (50-500)
<ul>
<li>Más estimadores = modelo más complejo</li>
<li>Usar validación cruzada para encontrar el óptimo</li>
</ul></li>
<li><strong><code>learning_rate</code></strong>: Factor de shrinkage (0.1-1.0)
<ul>
<li>Valores &lt; 1.0 reducen la contribución de cada clasificador</li>
<li>Ayuda a prevenir sobreajuste</li>
<li>Requiere más <code>n_estimators</code> si es pequeño</li>
</ul></li>
<li><strong><code>algorithm</code></strong>:
<ul>
<li><code>'SAMME'</code>: Versión discreta (clasificaciones duras)</li>
<li><code>'SAMME.R'</code>: Versión basada en probabilidades (usualmente mejor)</li>
</ul></li>
</ol>
<p><strong>Consideraciones de preprocesamiento:</strong> - AdaBoost funciona mejor con <strong>features normalizadas</strong>, aunque no es estrictamente necesario - <strong>Identificar y remover outliers</strong> mejora significativamente el rendimiento - Para clases desbalanceadas, considerar <strong>balanceo previo</strong> o <code>class_weight</code> en el clasificador base</p>
<p>En la siguiente sección, exploraremos Gradient Boosting, una generalización más flexible y poderosa de AdaBoost que funciona con cualquier función de pérdida diferenciable.</p>
</section>
</section>
<section id="gradient-boosting" class="level2">
<h2 class="anchored" data-anchor-id="gradient-boosting">Gradient Boosting</h2>
<section id="más-allá-de-adaboost-una-generalización-poderosa" class="level3">
<h3 class="anchored" data-anchor-id="más-allá-de-adaboost-una-generalización-poderosa">Más Allá de AdaBoost: Una Generalización Poderosa</h3>
<p>AdaBoost demostró que el boosting funciona brillantemente en la práctica. Sin embargo, tiene limitaciones importantes:</p>
<ol type="1">
<li><strong>Diseñado principalmente para clasificación</strong>: Adaptarlo a regresión no es trivial</li>
<li><strong>Función de pérdida fija</strong>: Usa pérdida exponencial implícitamente, que es sensible a outliers</li>
<li><strong>Marco específico</strong>: El algoritmo está diseñado para su caso particular, sin generalización obvia</li>
</ol>
<p>En 1999-2001, Jerome Friedman desarrolló <strong>Gradient Boosting</strong>, una reformulación revolucionaria que resuelve estas limitaciones. Su insight clave fue reconocer que boosting puede verse como un <strong>algoritmo de optimización</strong> que minimiza una función de pérdida en el espacio de funciones.</p>
<p><strong>Las ventajas de Gradient Boosting:</strong></p>
<ul>
<li><strong>Flexibilidad total</strong>: Funciona con <strong>cualquier función de pérdida diferenciable</strong></li>
<li><strong>Unificación</strong>: Un solo framework para clasificación, regresión, y otros problemas</li>
<li><strong>Robustez</strong>: Podemos elegir pérdidas robustas (Huber, MAE) para datos con outliers</li>
<li><strong>Control fino</strong>: Regularización mediante learning rate, subsampling, y otros hiperparámetros</li>
<li><strong>Estado del arte</strong>: Base de algoritmos modernos (XGBoost, LightGBM, CatBoost)</li>
</ul>
</section>
<section id="la-perspectiva-del-descenso-por-gradiente" class="level3">
<h3 class="anchored" data-anchor-id="la-perspectiva-del-descenso-por-gradiente">La Perspectiva del Descenso por Gradiente</h3>
<p>Para entender Gradient Boosting, necesitamos una analogía con el descenso por gradiente clásico, pero en el <strong>espacio de funciones</strong> en lugar del espacio de parámetros.</p>
<p><strong>Descenso por gradiente clásico</strong> (minimizar <span class="math inline">\(L(\theta)\)</span> respecto a parámetros <span class="math inline">\(\theta\)</span>):</p>
<ol type="1">
<li>Empezar con <span class="math inline">\(\theta_0\)</span> inicial</li>
<li>Calcular gradiente: <span class="math inline">\(g = \frac{\partial L}{\partial \theta}\)</span></li>
<li>Actualizar: <span class="math inline">\(\theta_{t+1} = \theta_t - \eta \cdot g\)</span> (donde <span class="math inline">\(\eta\)</span> es learning rate)</li>
<li>Repetir hasta convergencia</li>
</ol>
<p><strong>Gradient Boosting</strong> (minimizar <span class="math inline">\(L(F)\)</span> respecto a la función de predicción <span class="math inline">\(F\)</span>):</p>
<ol type="1">
<li>Empezar con predicción constante <span class="math inline">\(F_0(x)\)</span></li>
<li>Calcular “gradiente funcional”: <span class="math inline">\(-\frac{\partial L(y, F(x))}{\partial F(x)}\)</span> para cada ejemplo</li>
<li>Ajustar un modelo <span class="math inline">\(h(x)\)</span> que aproxime este gradiente negativo</li>
<li>Actualizar: <span class="math inline">\(F_{t+1}(x) = F_t(x) + \eta \cdot h(x)\)</span></li>
<li>Repetir <span class="math inline">\(M\)</span> iteraciones</li>
</ol>
<p><strong>La analogía</strong>: Imagine que está parado en una montaña (superficie de error) y quiere bajar al valle (mínimo). En cada paso: - <strong>Descenso clásico</strong>: Mide la pendiente donde está parado y da un paso en la dirección opuesta - <strong>Gradient Boosting</strong>: Mide cuánto error tiene en cada punto de datos, entrena un modelo que predice esos errores, y resta ese modelo de sus predicciones actuales</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Conexión entre AdaBoost y Gradient Boosting
</div>
</div>
<div class="callout-body-container callout-body">
<p>¿Cómo se relacionan AdaBoost y Gradient Boosting? La respuesta es elegante: <strong>AdaBoost es un caso especial de Gradient Boosting</strong>.</p>
<p>Específicamente, AdaBoost equivale a Gradient Boosting con <strong>pérdida exponencial</strong>:</p>
<p><span class="math display">\[L(y, F(x)) = \exp(-y \cdot F(x))\]</span></p>
<p>Si derivamos el gradiente de esta pérdida y construimos el algoritmo de Gradient Boosting correspondiente, recuperamos exactamente las actualizaciones de pesos de AdaBoost.</p>
<p><strong>Implicaciones:</strong> - AdaBoost optimiza una función objetivo específica (pérdida exponencial) - Gradient Boosting nos permite optimizar <strong>cualquier</strong> función objetivo - Para clasificación robusta, podemos usar <strong>log-loss</strong> en lugar de pérdida exponencial - Para regresión, podemos usar MSE, MAE, Huber, Quantile loss, etc.</p>
<p>Esta unificación es profunda: muestra que el boosting no es solo un “truco” heurístico, sino que tiene fundamentos sólidos en optimización matemática.</p>
</div>
</div>
</section>
<section id="el-algoritmo-de-gradient-boosting" class="level3">
<h3 class="anchored" data-anchor-id="el-algoritmo-de-gradient-boosting">El Algoritmo de Gradient Boosting</h3>
<p>Presentamos el algoritmo completo de Gradient Boosting para una función de pérdida general <span class="math inline">\(L(y, F(x))\)</span>:</p>
<p><strong>Algoritmo: Gradient Boosting</strong></p>
<ol type="1">
<li><p><strong>Inicializar</strong> con predicción constante óptima: <span class="math display">\[F_0(x) = \arg\min_{\gamma} \sum_{i=1}^n L(y_i, \gamma)\]</span></p>
<ul>
<li>Para regresión MSE: <span class="math inline">\(F_0(x) = \bar{y}\)</span> (media)</li>
<li>Para clasificación log-loss: <span class="math inline">\(F_0(x) = \log\frac{p}{1-p}\)</span> (log-odds de las clases)</li>
</ul></li>
<li><p><strong>Para</strong> <span class="math inline">\(m = 1, 2, \ldots, M\)</span> <strong>iteraciones</strong>:</p>
<ol type="a">
<li><p><strong>Calcular pseudo-residuales</strong> (gradiente negativo): <span class="math display">\[r_{im} = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F(x)=F_{m-1}(x)}\]</span></p></li>
<li><p><strong>Entrenar aprendiz débil</strong> <span class="math inline">\(h_m(x)\)</span> para predecir los residuales <span class="math inline">\(r_{im}\)</span>: <span class="math display">\[h_m = \arg\min_h \sum_{i=1}^n (r_{im} - h(x_i))^2\]</span></p></li>
<li><p><strong>Calcular paso óptimo</strong> (line search): <span class="math display">\[\gamma_m = \arg\min_{\gamma} \sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \gamma h_m(x_i))\]</span></p></li>
<li><p><strong>Actualizar modelo</strong>: <span class="math display">\[F_m(x) = F_{m-1}(x) + \nu \cdot \gamma_m \cdot h_m(x)\]</span></p>
<p>donde <span class="math inline">\(\nu \in (0, 1]\)</span> es el <strong>learning rate</strong> (shrinkage parameter)</p></li>
</ol></li>
<li><p><strong>Predicción final</strong>: <span class="math inline">\(F(x) = F_M(x)\)</span></p></li>
</ol>
<p><strong>Interpretación de los pasos:</strong></p>
<ul>
<li><p><strong>Paso 2a</strong>: Los “pseudo-residuales” son la dirección en la que deberíamos cambiar nuestras predicciones para minimizar la pérdida. Para MSE, estos son simplemente los residuales usuales: <span class="math inline">\(r_{im} = y_i - F_{m-1}(x_i)\)</span>.</p></li>
<li><p><strong>Paso 2b</strong>: Entrenamos un árbol (u otro modelo) para predecir estos pseudo-residuales. Es decir, tratamos de modelar “en qué dirección estamos equivocados”.</p></li>
<li><p><strong>Paso 2c</strong>: En lugar de simplemente sumar el nuevo modelo, buscamos el mejor peso para multiplicarlo. Esto es análogo a line search en optimización.</p></li>
<li><p><strong>Paso 2d</strong>: Actualizamos con un learning rate <span class="math inline">\(\nu &lt; 1\)</span> para regularización. Valores típicos son <span class="math inline">\(\nu = 0.1\)</span> o <span class="math inline">\(0.05\)</span>.</p></li>
</ul>
</section>
<section id="funciones-de-pérdida" class="level3">
<h3 class="anchored" data-anchor-id="funciones-de-pérdida">Funciones de Pérdida</h3>
<p>Una de las grandes fortalezas de Gradient Boosting es la flexibilidad para elegir la función de pérdida según el problema:</p>
<p><strong>Para Regresión:</strong></p>
<ol type="1">
<li><strong>MSE (Mean Squared Error)</strong>: <span class="math display">\[L(y, F(x)) = \frac{1}{2}(y - F(x))^2\]</span> <span class="math display">\[\text{Gradiente: } r = y - F(x)\]</span>
<ul>
<li>Uso: Regresión estándar cuando queremos penalizar cuadráticamente los errores</li>
<li>Sensible a outliers (errores grandes tienen penalización cuadrática)</li>
</ul></li>
<li><strong>MAE (Mean Absolute Error)</strong>: <span class="math display">\[L(y, F(x)) = |y - F(x)|\]</span> <span class="math display">\[\text{Gradiente: } r = \text{sign}(y - F(x))\]</span>
<ul>
<li>Uso: Regresión robusta a outliers</li>
<li>Menos sensible a valores extremos (penalización lineal)</li>
</ul></li>
<li><strong>Huber Loss</strong> (compromiso entre MSE y MAE): <span class="math display">\[L(y, F(x)) = \begin{cases}
\frac{1}{2}(y - F(x))^2 &amp; \text{si } |y - F(x)| \leq \delta \\
\delta |y - F(x)| - \frac{1}{2}\delta^2 &amp; \text{si } |y - F(x)| &gt; \delta
\end{cases}\]</span>
<ul>
<li>Uso: Mejor de ambos mundos - cuadrática cerca de cero, lineal para errores grandes</li>
<li>Parámetro <span class="math inline">\(\delta\)</span> controla el punto de transición</li>
</ul></li>
</ol>
<p><strong>Para Clasificación:</strong></p>
<ol type="1">
<li><strong>Log-loss (Binomial Deviance)</strong>: <span class="math display">\[L(y, F(x)) = \log(1 + \exp(-2yF(x))), \quad y \in \{-1, +1\}\]</span>
<ul>
<li>Uso: Clasificación binaria estándar (más robusta que pérdida exponencial)</li>
<li>Conexión directa con regresión logística</li>
</ul></li>
<li><strong>Exponential Loss</strong>: <span class="math display">\[L(y, F(x)) = \exp(-yF(x))\]</span>
<ul>
<li>Equivalente a AdaBoost</li>
<li>Muy sensible a outliers y etiquetas erróneas</li>
</ul></li>
</ol>
<p>Veamos Gradient Boosting en acción con ejemplos de regresión y clasificación:</p>
</section>
<section id="ejemplo-de-regresión-ajuste-iterativo-de-residuales" class="level3">
<h3 class="anchored" data-anchor-id="ejemplo-de-regresión-ajuste-iterativo-de-residuales">Ejemplo de Regresión: Ajuste Iterativo de Residuales</h3>
<p>Usaremos el dataset de California Housing para mostrar cómo Gradient Boosting ajusta iterativamente los residuales:</p>
<div id="cell-fig-gradient-boosting-regression" class="cell" data-fig-height="10" data-fig-width="14" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_california_housing</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Cargar datos</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>california <span class="op">=</span> fetch_california_housing()</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> california.data, california.target</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Usar un subconjunto para velocidad</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.random.choice(<span class="bu">len</span>(X), size<span class="op">=</span><span class="dv">5000</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>X_sample, y_sample <span class="op">=</span> X[indices], y[indices]</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir datos</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    X_sample, y_sample, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar Gradient Boosting con diferentes números de iteraciones</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>gb_models <span class="op">=</span> {}</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>n_estimators_list <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>]</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_est <span class="kw">in</span> n_estimators_list:</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    gb <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span>n_est,</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    gb.fit(X_train, y_train)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    gb_models[n_est] <span class="op">=</span> gb</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear figura</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>gs <span class="op">=</span> fig.add_gridspec(<span class="dv">3</span>, <span class="dv">2</span>, hspace<span class="op">=</span><span class="fl">0.3</span>, wspace<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Fila 1: Predicciones vs Valores reales para diferentes números de iteraciones</span></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, n_est <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="dv">1</span>, <span class="dv">10</span>]):</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(gs[<span class="dv">0</span>, idx])</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> gb_models[n_est]</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>    y_pred_train <span class="op">=</span> model.predict(X_train)</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>    y_pred_test <span class="op">=</span> model.predict(X_test)</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot test predictions</span></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>    ax.scatter(y_test, y_pred_test, alpha<span class="op">=</span><span class="fl">0.4</span>, s<span class="op">=</span><span class="dv">20</span>, edgecolors<span class="op">=</span><span class="st">'k'</span>,</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>               linewidths<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Test'</span>, c<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Línea diagonal (predicciones perfectas)</span></span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>    min_val <span class="op">=</span> <span class="bu">min</span>(y_test.<span class="bu">min</span>(), y_pred_test.<span class="bu">min</span>())</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>    max_val <span class="op">=</span> <span class="bu">max</span>(y_test.<span class="bu">max</span>(), y_pred_test.<span class="bu">max</span>())</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>    ax.plot([min_val, max_val], [min_val, max_val], <span class="st">'k--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Métricas</span></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>    rmse_test <span class="op">=</span> np.sqrt(mean_squared_error(y_test, y_pred_test))</span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>    r2_test <span class="op">=</span> r2_score(y_test, y_pred_test)</span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>    ax.text(<span class="fl">0.05</span>, <span class="fl">0.95</span>, <span class="ss">f'n_estimators = </span><span class="sc">{</span>n_est<span class="sc">}</span><span class="ch">\n</span><span class="ss">RMSE = </span><span class="sc">{</span>rmse_test<span class="sc">:.3f}</span><span class="ch">\n</span><span class="ss">R² = </span><span class="sc">{</span>r2_test<span class="sc">:.3f}</span><span class="ss">'</span>,</span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a>            transform<span class="op">=</span>ax.transAxes, verticalalignment<span class="op">=</span><span class="st">'top'</span>,</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a>            bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'wheat'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>),</span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a>            fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Valor Real'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Predicción'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f'(</span><span class="sc">{</span><span class="bu">chr</span>(<span class="dv">97</span><span class="op">+</span>idx)<span class="sc">}</span><span class="ss">) Después de </span><span class="sc">{</span>n_est<span class="sc">}</span><span class="ss"> iteración(es)'</span>,</span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>                 fontsize<span class="op">=</span><span class="dv">11</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a>    ax.legend(fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Fila 2: Predicciones para 50 y 100 iteraciones</span></span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, n_est <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="dv">50</span>, <span class="dv">100</span>]):</span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(gs[<span class="dv">1</span>, idx])</span>
<span id="cb11-76"><a href="#cb11-76" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> gb_models[n_est]</span>
<span id="cb11-77"><a href="#cb11-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-78"><a href="#cb11-78" aria-hidden="true" tabindex="-1"></a>    y_pred_train <span class="op">=</span> model.predict(X_train)</span>
<span id="cb11-79"><a href="#cb11-79" aria-hidden="true" tabindex="-1"></a>    y_pred_test <span class="op">=</span> model.predict(X_test)</span>
<span id="cb11-80"><a href="#cb11-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-81"><a href="#cb11-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot test predictions</span></span>
<span id="cb11-82"><a href="#cb11-82" aria-hidden="true" tabindex="-1"></a>    ax.scatter(y_test, y_pred_test, alpha<span class="op">=</span><span class="fl">0.4</span>, s<span class="op">=</span><span class="dv">20</span>, edgecolors<span class="op">=</span><span class="st">'k'</span>,</span>
<span id="cb11-83"><a href="#cb11-83" aria-hidden="true" tabindex="-1"></a>               linewidths<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Test'</span>, c<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb11-84"><a href="#cb11-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-85"><a href="#cb11-85" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Línea diagonal</span></span>
<span id="cb11-86"><a href="#cb11-86" aria-hidden="true" tabindex="-1"></a>    min_val <span class="op">=</span> <span class="bu">min</span>(y_test.<span class="bu">min</span>(), y_pred_test.<span class="bu">min</span>())</span>
<span id="cb11-87"><a href="#cb11-87" aria-hidden="true" tabindex="-1"></a>    max_val <span class="op">=</span> <span class="bu">max</span>(y_test.<span class="bu">max</span>(), y_pred_test.<span class="bu">max</span>())</span>
<span id="cb11-88"><a href="#cb11-88" aria-hidden="true" tabindex="-1"></a>    ax.plot([min_val, max_val], [min_val, max_val], <span class="st">'k--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-89"><a href="#cb11-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-90"><a href="#cb11-90" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Métricas</span></span>
<span id="cb11-91"><a href="#cb11-91" aria-hidden="true" tabindex="-1"></a>    rmse_test <span class="op">=</span> np.sqrt(mean_squared_error(y_test, y_pred_test))</span>
<span id="cb11-92"><a href="#cb11-92" aria-hidden="true" tabindex="-1"></a>    r2_test <span class="op">=</span> r2_score(y_test, y_pred_test)</span>
<span id="cb11-93"><a href="#cb11-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-94"><a href="#cb11-94" aria-hidden="true" tabindex="-1"></a>    ax.text(<span class="fl">0.05</span>, <span class="fl">0.95</span>, <span class="ss">f'n_estimators = </span><span class="sc">{</span>n_est<span class="sc">}</span><span class="ch">\n</span><span class="ss">RMSE = </span><span class="sc">{</span>rmse_test<span class="sc">:.3f}</span><span class="ch">\n</span><span class="ss">R² = </span><span class="sc">{</span>r2_test<span class="sc">:.3f}</span><span class="ss">'</span>,</span>
<span id="cb11-95"><a href="#cb11-95" aria-hidden="true" tabindex="-1"></a>            transform<span class="op">=</span>ax.transAxes, verticalalignment<span class="op">=</span><span class="st">'top'</span>,</span>
<span id="cb11-96"><a href="#cb11-96" aria-hidden="true" tabindex="-1"></a>            bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'lightblue'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>),</span>
<span id="cb11-97"><a href="#cb11-97" aria-hidden="true" tabindex="-1"></a>            fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb11-98"><a href="#cb11-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-99"><a href="#cb11-99" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Valor Real'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb11-100"><a href="#cb11-100" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Predicción'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb11-101"><a href="#cb11-101" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f'(</span><span class="sc">{</span><span class="bu">chr</span>(<span class="dv">99</span><span class="op">+</span>idx)<span class="sc">}</span><span class="ss">) Después de </span><span class="sc">{</span>n_est<span class="sc">}</span><span class="ss"> iteraciones'</span>,</span>
<span id="cb11-102"><a href="#cb11-102" aria-hidden="true" tabindex="-1"></a>                 fontsize<span class="op">=</span><span class="dv">11</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb11-103"><a href="#cb11-103" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb11-104"><a href="#cb11-104" aria-hidden="true" tabindex="-1"></a>    ax.legend(fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb11-105"><a href="#cb11-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-106"><a href="#cb11-106" aria-hidden="true" tabindex="-1"></a><span class="co"># Fila 3: Curvas de aprendizaje completas</span></span>
<span id="cb11-107"><a href="#cb11-107" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(gs[<span class="dv">2</span>, :])</span>
<span id="cb11-108"><a href="#cb11-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-109"><a href="#cb11-109" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar modelo con muchas iteraciones para ver evolución</span></span>
<span id="cb11-110"><a href="#cb11-110" aria-hidden="true" tabindex="-1"></a>gb_full <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb11-111"><a href="#cb11-111" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb11-112"><a href="#cb11-112" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb11-113"><a href="#cb11-113" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb11-114"><a href="#cb11-114" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb11-115"><a href="#cb11-115" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-116"><a href="#cb11-116" aria-hidden="true" tabindex="-1"></a>gb_full.fit(X_train, y_train)</span>
<span id="cb11-117"><a href="#cb11-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-118"><a href="#cb11-118" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular RMSE en cada etapa</span></span>
<span id="cb11-119"><a href="#cb11-119" aria-hidden="true" tabindex="-1"></a>train_rmse <span class="op">=</span> []</span>
<span id="cb11-120"><a href="#cb11-120" aria-hidden="true" tabindex="-1"></a>test_rmse <span class="op">=</span> []</span>
<span id="cb11-121"><a href="#cb11-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-122"><a href="#cb11-122" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> y_pred_train, y_pred_test <span class="kw">in</span> <span class="bu">zip</span>(gb_full.staged_predict(X_train),</span>
<span id="cb11-123"><a href="#cb11-123" aria-hidden="true" tabindex="-1"></a>                                       gb_full.staged_predict(X_test)):</span>
<span id="cb11-124"><a href="#cb11-124" aria-hidden="true" tabindex="-1"></a>    train_rmse.append(np.sqrt(mean_squared_error(y_train, y_pred_train)))</span>
<span id="cb11-125"><a href="#cb11-125" aria-hidden="true" tabindex="-1"></a>    test_rmse.append(np.sqrt(mean_squared_error(y_test, y_pred_test)))</span>
<span id="cb11-126"><a href="#cb11-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-127"><a href="#cb11-127" aria-hidden="true" tabindex="-1"></a>ax.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">201</span>), train_rmse, <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'RMSE Train'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-128"><a href="#cb11-128" aria-hidden="true" tabindex="-1"></a>ax.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">201</span>), test_rmse, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'RMSE Test'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-129"><a href="#cb11-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-130"><a href="#cb11-130" aria-hidden="true" tabindex="-1"></a><span class="co"># Marcar punto óptimo</span></span>
<span id="cb11-131"><a href="#cb11-131" aria-hidden="true" tabindex="-1"></a>best_n <span class="op">=</span> np.argmin(test_rmse) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb11-132"><a href="#cb11-132" aria-hidden="true" tabindex="-1"></a>ax.axvline(x<span class="op">=</span>best_n, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-133"><a href="#cb11-133" aria-hidden="true" tabindex="-1"></a>ax.plot(best_n, test_rmse[best_n<span class="op">-</span><span class="dv">1</span>], <span class="st">'go'</span>, markersize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb11-134"><a href="#cb11-134" aria-hidden="true" tabindex="-1"></a>ax.text(best_n <span class="op">+</span> <span class="dv">5</span>, test_rmse[best_n<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.02</span>,</span>
<span id="cb11-135"><a href="#cb11-135" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f'Óptimo: </span><span class="sc">{</span>best_n<span class="sc">}</span><span class="ss"> iter.</span><span class="ch">\n</span><span class="ss">RMSE = </span><span class="sc">{</span>test_rmse[best_n<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">'</span>,</span>
<span id="cb11-136"><a href="#cb11-136" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span>, color<span class="op">=</span><span class="st">'green'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb11-137"><a href="#cb11-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-138"><a href="#cb11-138" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Número de iteraciones (árboles)'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb11-139"><a href="#cb11-139" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'RMSE'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb11-140"><a href="#cb11-140" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(e) Curvas de aprendizaje: Evolución del error'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb11-141"><a href="#cb11-141" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb11-142"><a href="#cb11-142" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb11-143"><a href="#cb11-143" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="dv">0</span>, <span class="dv">200</span>])</span>
<span id="cb11-144"><a href="#cb11-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-145"><a href="#cb11-145" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb11-146"><a href="#cb11-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-147"><a href="#cb11-147" aria-hidden="true" tabindex="-1"></a><span class="co"># Imprimir resumen</span></span>
<span id="cb11-148"><a href="#cb11-148" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-149"><a href="#cb11-149" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GRADIENT BOOSTING - REGRESIÓN (California Housing)"</span>)</span>
<span id="cb11-150"><a href="#cb11-150" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-151"><a href="#cb11-151" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Configuración: max_depth=3, learning_rate=0.1"</span>)</span>
<span id="cb11-152"><a href="#cb11-152" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Evolución del rendimiento:"</span>)</span>
<span id="cb11-153"><a href="#cb11-153" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_est <span class="kw">in</span> n_estimators_list:</span>
<span id="cb11-154"><a href="#cb11-154" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> gb_models[n_est]</span>
<span id="cb11-155"><a href="#cb11-155" aria-hidden="true" tabindex="-1"></a>    rmse <span class="op">=</span> np.sqrt(mean_squared_error(y_test, model.predict(X_test)))</span>
<span id="cb11-156"><a href="#cb11-156" aria-hidden="true" tabindex="-1"></a>    r2 <span class="op">=</span> r2_score(y_test, model.predict(X_test))</span>
<span id="cb11-157"><a href="#cb11-157" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>n_est<span class="sc">:3d}</span><span class="ss"> iteraciones: RMSE = </span><span class="sc">{</span>rmse<span class="sc">:.4f}</span><span class="ss">, R² = </span><span class="sc">{</span>r2<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb11-158"><a href="#cb11-158" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Punto óptimo: </span><span class="sc">{</span>best_n<span class="sc">}</span><span class="ss"> iteraciones, RMSE = </span><span class="sc">{</span>test_rmse[best_n<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb11-159"><a href="#cb11-159" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-gradient-boosting-regression" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-boosting-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-boosting_files/figure-html/fig-gradient-boosting-regression-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-boosting-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.7: Gradient Boosting en regresión: ajuste iterativo de residuales usando el dataset California Housing. (a) Predicciones del modelo acumulado vs valores reales después de 1, 10, 50 y 100 iteraciones. La línea diagonal representa predicciones perfectas. (b) Distribución de residuales en cada etapa, mostrando cómo se reduce progresivamente el error. (c) Evolución del RMSE en train y test, demostrando la convergencia del algoritmo y el punto óptimo de early stopping.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
======================================================================
GRADIENT BOOSTING - REGRESIÓN (California Housing)
======================================================================
Configuración: max_depth=3, learning_rate=0.1

Evolución del rendimiento:
    1 iteraciones: RMSE = 1.1006, R² = 0.0999
   10 iteraciones: RMSE = 0.8092, R² = 0.5135
   50 iteraciones: RMSE = 0.5872, R² = 0.7438
  100 iteraciones: RMSE = 0.5550, R² = 0.7711

Punto óptimo: 199 iteraciones, RMSE = 0.5330
======================================================================
</code></pre>
</div>
</div>
<p>La visualización muestra el proceso iterativo de Gradient Boosting:</p>
<ul>
<li><p><strong>Paneles (a)-(d)</strong>: Conforme aumentan las iteraciones, las predicciones se acercan cada vez más a la línea diagonal (predicciones perfectas), y las métricas RMSE y R² mejoran consistentemente.</p></li>
<li><p><strong>Panel (e)</strong>: Las curvas de aprendizaje muestran que el RMSE en train continúa disminuyendo monotónicamente, mientras que el RMSE en test disminuye inicialmente pero eventualmente se estabiliza. El punto óptimo (~50-80 iteraciones) marca donde deberíamos usar early stopping.</p></li>
</ul>
<p><strong>Observación clave</strong>: A diferencia de Random Forest, donde agregar más árboles casi nunca daña, en Gradient Boosting debemos ser cuidadosos con el número de iteraciones para evitar sobreajuste.</p>
</section>
<section id="hiperparámetros-un-análisis-profundo" class="level3">
<h3 class="anchored" data-anchor-id="hiperparámetros-un-análisis-profundo">Hiperparámetros: Un Análisis Profundo</h3>
<p>Gradient Boosting tiene varios hiperparámetros críticos que controlan el balance entre sesgo, varianza, y tiempo de entrenamiento. Entender sus efectos es esencial para obtener buen rendimiento.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Trade-off entre Learning Rate y Número de Estimadores
</div>
</div>
<div class="callout-body-container callout-body">
<p>Existe una relación inversa fundamental entre <strong>learning rate</strong> (<span class="math inline">\(\nu\)</span>) y <strong>número de estimadores</strong> (<span class="math inline">\(M\)</span>):</p>
<p><strong>Learning rate bajo + Muchos estimadores:</strong> - <span class="math inline">\(\nu = 0.01\)</span> con <span class="math inline">\(M = 1000\)</span> - Aprendizaje muy gradual, cada árbol hace contribuciones pequeñas - <strong>Ventajas</strong>: Mejor generalización, menor sobreajuste, modelo más robusto - <strong>Desventajas</strong>: Entrenamiento muy lento, necesita más memoria</p>
<p><strong>Learning rate alto + Pocos estimadores:</strong> - <span class="math inline">\(\nu = 0.5\)</span> con <span class="math inline">\(M = 50\)</span> - Aprendizaje agresivo, cada árbol hace grandes correcciones - <strong>Ventajas</strong>: Entrenamiento rápido, converge en pocas iteraciones - <strong>Desventajas</strong>: Mayor riesgo de sobreajuste, menos robusto</p>
<p><strong>Regla práctica:</strong> <span class="math display">\[\nu \times M \approx \text{constante}\]</span></p>
<p>Si reduces el learning rate a la mitad, necesitarás aproximadamente el doble de iteraciones para alcanzar el mismo rendimiento. En producción, valores típicos son: - <span class="math inline">\(\nu = 0.1\)</span> con <span class="math inline">\(M = 100-500\)</span> - <span class="math inline">\(\nu = 0.05\)</span> con <span class="math inline">\(M = 200-1000\)</span> - <span class="math inline">\(\nu = 0.01\)</span> con <span class="math inline">\(M = 1000-5000\)</span> (para competencias donde cada 0.001% importa)</p>
</div>
</div>
<p>Veamos el efecto de diferentes hiperparámetros:</p>
<div id="cell-fig-gradient-boosting-hyperparameters" class="cell" data-fig-height="4" data-fig-width="14" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Usar el mismo conjunto de datos de California Housing</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Ya tenemos X_train, X_test, y_train, y_test</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">4</span>))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># (a) Efecto del learning rate</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">0</span>]</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>learning_rates <span class="op">=</span> [<span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.5</span>]</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'orange'</span>, <span class="st">'red'</span>]</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lr, color <span class="kw">in</span> <span class="bu">zip</span>(learning_rates, colors):</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    gb <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span>lr,</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    gb.fit(X_train, y_train)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    test_errors <span class="op">=</span> []</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> y_pred <span class="kw">in</span> gb.staged_predict(X_test):</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        test_errors.append(np.sqrt(mean_squared_error(y_test, y_pred)))</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    ax.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">201</span>), test_errors, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'LR = </span><span class="sc">{</span>lr<span class="sc">}</span><span class="ss">'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Número de iteraciones'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'RMSE (Test)'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(a) Efecto del Learning Rate'</span>, fontsize<span class="op">=</span><span class="dv">11</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="fl">0.4</span>, <span class="fl">1.2</span>])</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a><span class="co"># (b) Efecto de max_depth</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">1</span>]</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>max_depths <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>]</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'orange'</span>, <span class="st">'red'</span>]</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> depth, color <span class="kw">in</span> <span class="bu">zip</span>(max_depths, colors):</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>    gb <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span>depth,</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>    gb.fit(X_train, y_train)</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>    test_errors <span class="op">=</span> []</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> y_pred <span class="kw">in</span> gb.staged_predict(X_test):</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>        test_errors.append(np.sqrt(mean_squared_error(y_test, y_pred)))</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>    ax.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">201</span>), test_errors, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'Depth = </span><span class="sc">{</span>depth<span class="sc">}</span><span class="ss">'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Número de iteraciones'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'RMSE (Test)'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(b) Efecto de Max Depth'</span>, fontsize<span class="op">=</span><span class="dv">11</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="fl">0.4</span>, <span class="fl">1.2</span>])</span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a><span class="co"># (c) Efecto de subsample</span></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axes[<span class="dv">2</span>]</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>subsamples <span class="op">=</span> [<span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>, <span class="fl">1.0</span>]</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'blue'</span>, <span class="st">'green'</span>, <span class="st">'orange'</span>, <span class="st">'red'</span>]</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ss, color <span class="kw">in</span> <span class="bu">zip</span>(subsamples, colors):</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a>    gb <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>        subsample<span class="op">=</span>ss,</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>    gb.fit(X_train, y_train)</span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>    test_errors <span class="op">=</span> []</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> y_pred <span class="kw">in</span> gb.staged_predict(X_test):</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>        test_errors.append(np.sqrt(mean_squared_error(y_test, y_pred)))</span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>    ax.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">201</span>), test_errors, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>            label<span class="op">=</span><span class="ss">f'Subsample = </span><span class="sc">{</span>ss<span class="sc">}</span><span class="ss">'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Número de iteraciones'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'RMSE (Test)'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'(c) Efecto de Subsample'</span>, fontsize<span class="op">=</span><span class="dv">11</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a>ax.legend(fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="fl">0.4</span>, <span class="fl">1.2</span>])</span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-gradient-boosting-hyperparameters" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-boosting-hyperparameters-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-boosting_files/figure-html/fig-gradient-boosting-hyperparameters-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-boosting-hyperparameters-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.8: Efecto de los hiperparámetros principales en Gradient Boosting. (a) Learning rate: valores más bajos producen convergencia más suave pero requieren más iteraciones. (b) Max depth: árboles más profundos capturan interacciones complejas pero aumentan el riesgo de sobreajuste. (c) Subsample: muestreo estocástico agrega regularización reduciendo la varianza. Las curvas muestran error de test vs número de iteraciones para diferentes valores de cada hiperparámetro.
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Análisis de los efectos:</strong></p>
<p><strong>Panel (a) - Learning Rate:</strong> - <strong>LR = 0.01</strong>: Converge muy lentamente pero de forma muy suave. Necesita 150+ iteraciones para alcanzar buen rendimiento. - <strong>LR = 0.1</strong>: Balance óptimo en este caso - converge en ~50-100 iteraciones con buen rendimiento. - <strong>LR = 0.5</strong>: Converge rápidamente pero puede oscilar o sobreajustar en las últimas iteraciones.</p>
<p><strong>Panel (b) - Max Depth:</strong> - <strong>Depth = 1</strong> (stumps): Converge lentamente, modelo simple que puede tener alto sesgo residual. - <strong>Depth = 3</strong>: Excelente balance - captura interacciones importantes sin sobreajustar. - <strong>Depth = 5</strong>: Mejor rendimiento inicial, pero riesgo de sobreajuste en iteraciones posteriores.</p>
<p><strong>Panel (c) - Subsample:</strong> - <strong>Subsample &lt; 1.0</strong>: Introduce estocasticidad (muestreo sin reemplazo) que actúa como regularización. - <strong>Subsample = 0.7-0.8</strong>: A menudo produce mejores resultados que 1.0, especialmente en datasets grandes. - Trade-off: Reduce varianza pero puede aumentar ligeramente el sesgo.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hiperparámetros Recomendados para Empezar
</div>
</div>
<div class="callout-body-container callout-body">
<p>Si estás comenzando con Gradient Boosting y no sabes qué valores usar, estas son configuraciones sólidas como punto de partida:</p>
<p><strong>Configuración conservadora (recomendada para empezar):</strong></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>GradientBoostingRegressor(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">100</span>,       <span class="co"># Suficiente para la mayoría de problemas</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,      <span class="co"># Balance entre velocidad y calidad</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">3</span>,            <span class="co"># Captura interacciones de 2º orden</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    min_samples_split<span class="op">=</span><span class="dv">20</span>,   <span class="co"># Previene sobreajuste en hojas</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    min_samples_leaf<span class="op">=</span><span class="dv">10</span>,    <span class="co"># Regularización adicional</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    subsample<span class="op">=</span><span class="fl">0.8</span>,          <span class="co"># Estocástico para mejor generalización</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Después de validación cruzada, ajusta en este orden:</strong></p>
<ol type="1">
<li><strong>Primero</strong>: <code>max_depth</code> y <code>min_samples_split</code> (estructura del árbol)</li>
<li><strong>Segundo</strong>: <code>learning_rate</code> y <code>n_estimators</code> (compromiso velocidad-calidad)</li>
<li><strong>Tercero</strong>: <code>subsample</code> y <code>max_features</code> (regularización estocástica)</li>
</ol>
<p><strong>Para datasets grandes (&gt;100K filas):</strong> - Reduce <code>learning_rate</code> a 0.05 - Aumenta <code>n_estimators</code> a 200-500 - Usa <code>subsample=0.5-0.7</code> para velocidad</p>
<p><strong>Para datasets pequeños (&lt;1K filas):</strong> - Usa <code>max_depth=2</code> (árboles más simples) - Aumenta <code>min_samples_leaf=20</code> (más regularización) - Considera Random Forest como alternativa más robusta</p>
</div>
</div>
</section>
<section id="early-stopping-detención-automática" class="level3">
<h3 class="anchored" data-anchor-id="early-stopping-detención-automática">Early Stopping: Detención Automática</h3>
<p>Una técnica crucial para Gradient Boosting es <strong>early stopping</strong>: detener el entrenamiento cuando el error de validación deja de mejorar, en lugar de especificar un número fijo de iteraciones.</p>
<div id="cell-fig-gradient-boosting-early-stopping" class="cell" data-fig-height="5" data-fig-width="10" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir train en train/validation para early stopping</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>X_train_sub, X_val, y_train_sub, y_val <span class="op">=</span> train_test_split(</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    X_train, y_train, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar con muchas iteraciones para ver el efecto</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>gb_early <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">300</span>,</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>gb_early.fit(X_train_sub, y_train_sub)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular errores en cada etapa</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>train_errors_es <span class="op">=</span> []</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>val_errors_es <span class="op">=</span> []</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> y_pred_train, y_pred_val <span class="kw">in</span> <span class="bu">zip</span>(gb_early.staged_predict(X_train_sub),</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>                                      gb_early.staged_predict(X_val)):</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    train_errors_es.append(np.sqrt(mean_squared_error(y_train_sub, y_pred_train)))</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    val_errors_es.append(np.sqrt(mean_squared_error(y_val, y_pred_val)))</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Simular early stopping: encontrar cuando val error deja de mejorar</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>patience <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>best_val_error <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>best_iteration <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>no_improvement_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, val_error <span class="kw">in</span> <span class="bu">enumerate</span>(val_errors_es):</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val_error <span class="op">&lt;</span> best_val_error:</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>        best_val_error <span class="op">=</span> val_error</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>        best_iteration <span class="op">=</span> i <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>        no_improvement_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>        no_improvement_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> no_improvement_count <span class="op">&gt;=</span> patience:</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>            early_stop_iteration <span class="op">=</span> i <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>    early_stop_iteration <span class="op">=</span> <span class="bu">len</span>(val_errors_es)</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar</span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(train_errors_es) <span class="op">+</span> <span class="dv">1</span>), train_errors_es,</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>         <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Error Train'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(val_errors_es) <span class="op">+</span> <span class="dv">1</span>), val_errors_es,</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>         <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Error Validación'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Marcar mejor punto</span></span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>best_iteration, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>plt.plot(best_iteration, val_errors_es[best_iteration<span class="op">-</span><span class="dv">1</span>], <span class="st">'go'</span>, markersize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Marcar donde early stopping detendría</span></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>early_stop_iteration, color<span class="op">=</span><span class="st">'orange'</span>, linestyle<span class="op">=</span><span class="st">':'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Área de sobreajuste</span></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> early_stop_iteration <span class="op">&lt;</span> <span class="dv">280</span>:</span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a>    plt.axvspan(early_stop_iteration, <span class="dv">300</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>    plt.text(early_stop_iteration <span class="op">+</span> <span class="dv">10</span>, <span class="fl">0.45</span>, <span class="st">'Zona de sobreajuste</span><span class="ch">\n</span><span class="st">(sin beneficio)'</span>,</span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>             fontsize<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">'darkred'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a>plt.text(best_iteration <span class="op">+</span> <span class="dv">5</span>, val_errors_es[best_iteration<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.02</span>,</span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a>         <span class="ss">f'Mejor: iter. </span><span class="sc">{</span>best_iteration<span class="sc">}</span><span class="ch">\n</span><span class="ss">RMSE = </span><span class="sc">{</span>val_errors_es[best_iteration<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">'</span> <span class="op">+</span></span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a>         <span class="ss">f'Early stop: iter. </span><span class="sc">{</span>early_stop_iteration<span class="sc">}</span><span class="ch">\n</span><span class="ss">(patience=</span><span class="sc">{</span>patience<span class="sc">}</span><span class="ss">)'</span>,</span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a>         fontsize<span class="op">=</span><span class="dv">9</span>, color<span class="op">=</span><span class="st">'green'</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a>         bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'lightgreen'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>))</span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Número de iteraciones'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'RMSE'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Early Stopping en Gradient Boosting'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="dv">0</span>, <span class="dv">300</span>])</span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">60</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"EARLY STOPPING ANALYSIS"</span>)</span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">60</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mejor iteración (mínimo error val): </span><span class="sc">{</span>best_iteration<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Early stopping (patience=</span><span class="sc">{</span>patience<span class="sc">}</span><span class="ss">): iter. </span><span class="sc">{</span>early_stop_iteration<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Iteraciones ahorradas: </span><span class="sc">{</span><span class="dv">300</span> <span class="op">-</span> early_stop_iteration<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE en mejor punto: </span><span class="sc">{</span>val_errors_es[best_iteration<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE si usáramos todas (300): </span><span class="sc">{</span>val_errors_es[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Diferencia: </span><span class="sc">{</span>val_errors_es[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> val_errors_es[best_iteration<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss"> (peor)"</span>)</span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">60</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-gradient-boosting-early-stopping" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-boosting-early-stopping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-boosting_files/figure-html/fig-gradient-boosting-early-stopping-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-boosting-early-stopping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.9: Demostración de early stopping en Gradient Boosting. La línea azul muestra el error de entrenamiento (que continúa disminuyendo), mientras que la línea roja muestra el error de validación. El punto verde marca donde early stopping detendría el entrenamiento (cuando el error de validación no mejora por 20 iteraciones consecutivas). Continuar más allá de este punto lleva a sobreajuste sin beneficio en generalización.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
============================================================
EARLY STOPPING ANALYSIS
============================================================
Mejor iteración (mínimo error val): 290
Early stopping (patience=20): iter. 300
Iteraciones ahorradas: 0
RMSE en mejor punto: 0.5074
RMSE si usáramos todas (300): 0.5079
Diferencia: 0.0005 (peor)
============================================================
</code></pre>
</div>
</div>
<p><strong>Cómo implementar early stopping en scikit-learn:</strong></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividir datos en train/validation/test</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>X_train_sub, X_val, y_train_sub, y_val <span class="op">=</span> train_test_split(</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    X_train, y_train, test_size<span class="op">=</span><span class="fl">0.2</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Opción 1: Monitoreo manual con staged_predict</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>gb <span class="op">=</span> GradientBoostingRegressor(n_estimators<span class="op">=</span><span class="dv">1000</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>gb.fit(X_train_sub, y_train_sub)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>val_errors <span class="op">=</span> [mean_squared_error(y_val, y_pred)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>              <span class="cf">for</span> y_pred <span class="kw">in</span> gb.staged_predict(X_val)]</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>best_n <span class="op">=</span> np.argmin(val_errors) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Reentrenar con número óptimo</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>gb_final <span class="op">=</span> GradientBoostingRegressor(n_estimators<span class="op">=</span>best_n, learning_rate<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>gb_final.fit(X_train, y_train)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Opción 2: Usar n_iter_no_change (sklearn &gt;= 0.20)</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>gb_auto <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    validation_fraction<span class="op">=</span><span class="fl">0.2</span>,  <span class="co"># Separa automáticamente validación</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    n_iter_no_change<span class="op">=</span><span class="dv">20</span>,      <span class="co"># Patience</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    tol<span class="op">=</span><span class="fl">0.0001</span>                <span class="co"># Mejora mínima considerada significativa</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>gb_auto.fit(X_train, y_train)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Iteraciones usadas: </span><span class="sc">{</span>gb_auto<span class="sc">.</span>n_estimators_<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cuidado con el Sobreajuste en Gradient Boosting
</div>
</div>
<div class="callout-body-container callout-body">
<p>Gradient Boosting puede sobreajustar fácilmente si no se regula adecuadamente. Las señales de advertencia incluyen:</p>
<p><strong>Síntomas de sobreajuste:</strong> - Gap grande y creciente entre error de train y test - Rendimiento en test empeora mientras train mejora - Modelo muy sensible a pequeños cambios en hiperparámetros - Predicciones extrañas en regiones con pocos datos</p>
<p><strong>Causas comunes:</strong> 1. <strong>Demasiadas iteraciones</strong> sin early stopping 2. <strong>Árboles muy profundos</strong> (max_depth &gt; 5-7) 3. <strong>Learning rate muy alto</strong> (&gt;0.3) con muchas iteraciones 4. <strong>min_samples_leaf muy bajo</strong> (&lt;5) 5. <strong>Datos con ruido</strong> o outliers (usar funciones de pérdida robustas)</p>
<p><strong>Soluciones:</strong> - ✅ <strong>Siempre usar early stopping</strong> con conjunto de validación - ✅ Reducir <code>max_depth</code> (empezar con 3) - ✅ Reducir <code>learning_rate</code> y compensar con más iteraciones - ✅ Aumentar <code>min_samples_split</code> y <code>min_samples_leaf</code> - ✅ Usar <code>subsample &lt; 1.0</code> (típicamente 0.5-0.8) - ✅ Usar regularización: <code>max_features &lt; n_features</code> - ✅ Si el sobreajuste persiste, considerar Random Forest</p>
<p><strong>Regla de oro</strong>: Si dudas entre sobreajustar o subajustar, es mejor subajustar ligeramente. Un modelo simple que generaliza es preferible a uno complejo que memoriza.</p>
</div>
</div>
</section>
<section id="feature-importance-e-interpretabilidad" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance-e-interpretabilidad">Feature Importance e Interpretabilidad</h3>
<p>Una ventaja de Gradient Boosting es que permite analizar la importancia relativa de las features:</p>
<div id="cell-fig-gradient-boosting-feature-importance" class="cell" data-fig-height="5" data-fig-width="10" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar modelo óptimo</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>gb_final <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>gb_final.fit(X_train, y_train)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener nombres de features</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> california.feature_names</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature importance</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> gb_final.feature_importances_</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.argsort(importances)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>plt.bar(<span class="bu">range</span>(<span class="bu">len</span>(importances)), importances[indices], alpha<span class="op">=</span><span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'steelblue'</span>)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(<span class="bu">len</span>(importances)), [feature_names[i] <span class="cf">for</span> i <span class="kw">in</span> indices], rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Feature'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Importancia (reducción de MSE)'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Feature Importance en Gradient Boosting (California Housing)'</span>,</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>          fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Imprimir ranking</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">50</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RANKING DE FEATURE IMPORTANCE"</span>)</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">50</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, idx <span class="kw">in</span> <span class="bu">enumerate</span>(indices, <span class="dv">1</span>):</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>feature_names[idx]<span class="sc">:15s}</span><span class="ss">: </span><span class="sc">{</span>importances[idx]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'='</span><span class="op">*</span><span class="dv">50</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-gradient-boosting-feature-importance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-boosting-feature-importance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="06-boosting_files/figure-html/fig-gradient-boosting-feature-importance-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-boosting-feature-importance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;10.10: Análisis de importancia de features en Gradient Boosting usando el dataset California Housing. Las features más importantes son MedInc (ingreso mediano) y AveOccup (ocupación promedio), seguidas por la ubicación geográfica. La importancia se calcula como la reducción total de error atribuida a cada feature sumada sobre todos los árboles del ensamble.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
==================================================
RANKING DE FEATURE IMPORTANCE
==================================================
1. MedInc         : 0.5760
2. AveOccup       : 0.1377
3. Longitude      : 0.1079
4. Latitude       : 0.0955
5. HouseAge       : 0.0359
6. AveRooms       : 0.0270
7. Population     : 0.0124
8. AveBedrms      : 0.0075
==================================================
</code></pre>
</div>
</div>
<p><strong>Interpretación de feature importance en Gradient Boosting:</strong></p>
<ul>
<li><strong>Valores altos</strong>: Features que contribuyen significativamente a reducir el error en múltiples splits</li>
<li><strong>Cálculo</strong>: Suma ponderada de la reducción de error en cada split que usa esa feature, a través de todos los árboles</li>
<li><strong>Uso práctico</strong>:
<ul>
<li>Identificar features más predictivas</li>
<li>Ingeniería de features (crear interacciones de features importantes)</li>
<li>Selección de features (eliminar features con importancia cercana a cero)</li>
<li>Comunicación con stakeholders (explicar qué factores impulsan las predicciones)</li>
</ul></li>
</ul>
<p>En resumen, Gradient Boosting es un framework extremadamente flexible y poderoso que generaliza AdaBoost, permitiendo optimizar cualquier función de pérdida diferenciable. Sus principales fortalezas son la flexibilidad en la elección de pérdida y la capacidad de control fino mediante hiperparámetros. Sin embargo, requiere más cuidado que Random Forest para evitar sobreajuste, y el tuning de hiperparámetros es más crítico para obtener buen rendimiento.</p>
<p>En las siguientes secciones exploraremos implementaciones modernas de gradient boosting (XGBoost, LightGBM, CatBoost) que optimizan velocidad, uso de memoria, y añaden características adicionales para facilitar su uso en producción.</p>
</section>
</section>
<section id="sec-modern-boosting" class="level2">
<h2 class="anchored" data-anchor-id="sec-modern-boosting">5. Implementaciones Modernas de Boosting</h2>
<p>Aunque las implementaciones clásicas de AdaBoost y Gradient Boosting en scikit-learn son excelentes para entender los conceptos fundamentales, en la práctica moderna se utilizan implementaciones optimizadas que ofrecen mejoras significativas en velocidad, uso de memoria, capacidad de regularización y facilidad de uso. En esta sección exploraremos tres de las bibliotecas más populares y poderosas: XGBoost, LightGBM y CatBoost.</p>
<p>Estas implementaciones han dominado competencias de machine learning como Kaggle y se utilizan ampliamente en producción debido a su rendimiento superior. Cada una introduce innovaciones algorítmicas y de ingeniería que las hacen más eficientes que las implementaciones base.</p>
<section id="sec-xgboost" class="level3">
<h3 class="anchored" data-anchor-id="sec-xgboost">5.1 XGBoost (eXtreme Gradient Boosting)</h3>
<p>XGBoost, desarrollado por Tianqi Chen en 2014, es probablemente la implementación de boosting más popular y ampliamente utilizada en la industria. Su éxito se debe a una combinación de innovaciones algorítmicas, optimizaciones de ingeniería, y una API bien diseñada que facilita su uso en producción.</p>
<section id="qué-hace-especial-a-xgboost" class="level4">
<h4 class="anchored" data-anchor-id="qué-hace-especial-a-xgboost">¿Qué hace especial a XGBoost?</h4>
<p>XGBoost introduce varias mejoras clave sobre el gradient boosting tradicional:</p>
<p><strong>1. Regularización en la función objetivo</strong></p>
<p>XGBoost añade términos de regularización explícitos a la función objetivo que se optimiza en cada iteración:</p>
<p><span class="math display">\[
\text{Obj}^{(t)} = \sum_{i=1}^n L(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t) + \text{constant}
\]</span></p>
<p>donde el término de regularización es:</p>
<p><span class="math display">\[
\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2 + \alpha \sum_{j=1}^T |w_j|
\]</span></p>
<p>Aquí: - <span class="math inline">\(T\)</span> = número de hojas en el árbol - <span class="math inline">\(w_j\)</span> = peso (predicción) en la hoja <span class="math inline">\(j\)</span> - <span class="math inline">\(\gamma\)</span> = penalización por número de hojas (complejidad del árbol) - <span class="math inline">\(\lambda\)</span> = regularización L2 sobre los pesos de las hojas - <span class="math inline">\(\alpha\)</span> = regularización L1 sobre los pesos de las hojas</p>
<p>Esta regularización ayuda a prevenir sobreajuste de manera más efectiva que simplemente limitar la profundidad del árbol.</p>
<p><strong>2. Optimización de segundo orden (Newton Boosting)</strong></p>
<p>Mientras que el gradient boosting clásico solo usa la primera derivada (gradiente) de la función de pérdida, XGBoost usa también la segunda derivada (Hessian). Esto proporciona información sobre la curvatura de la función de pérdida y permite una optimización más precisa y rápida, similar a cómo el método de Newton es más eficiente que el descenso por gradiente.</p>
<p><strong>3. Construcción de árbol eficiente</strong></p>
<p>XGBoost utiliza algoritmos optimizados para encontrar los mejores splits: - <strong>Exact greedy algorithm</strong>: Enumera todos los posibles split points (para datasets pequeños) - <strong>Approximate algorithm</strong>: Usa histogramas y quantiles para proponer candidatos de split (para datasets grandes) - <strong>Sparsity-aware algorithm</strong>: Maneja eficientemente missing values y features sparse</p>
<p><strong>4. Poda de árbol con max_delta_step</strong></p>
<p>En lugar de limitar la profundidad durante la construcción, XGBoost puede construir árboles profundos y luego podarlos hacia atrás, eliminando splits que no aportan ganancia suficiente después de considerar la regularización.</p>
<p><strong>5. Parallel y Distributed Computing</strong></p>
<p>XGBoost paraleliza la construcción de árboles a nivel de features (no de árboles, ya que boosting es secuencial). También soporta entrenamiento distribuido y puede usar GPUs.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
XGBoost en competencias
</div>
</div>
<div class="callout-body-container callout-body">
<p>XGBoost ha sido el algoritmo ganador o parte de la solución ganadora en la mayoría de competencias de Kaggle que involucran datos tabulares desde 2015. Su combinación de precisión, velocidad y flexibilidad lo ha convertido en el punto de partida estándar para estos problemas.</p>
</div>
</div>
</section>
<section id="hiperparámetros-importantes-en-xgboost" class="level4">
<h4 class="anchored" data-anchor-id="hiperparámetros-importantes-en-xgboost">Hiperparámetros importantes en XGBoost</h4>
<p>XGBoost tiene muchos hiperparámetros, pero los más importantes son:</p>
<p><strong>Estructura del árbol:</strong> - <code>max_depth</code>: Profundidad máxima de cada árbol (típicamente 3-10) - <code>min_child_weight</code>: Suma mínima de weights (Hessian) en una hoja (análogo a min_samples_leaf) - <code>gamma</code>: Reducción mínima de loss necesaria para hacer un split (mayor = más conservador)</p>
<p><strong>Regularización:</strong> - <code>lambda</code> (reg_lambda): Regularización L2 en pesos de hojas (default = 1) - <code>alpha</code> (reg_alpha): Regularización L1 en pesos de hojas (default = 0)</p>
<p><strong>Muestreo:</strong> - <code>subsample</code>: Fracción de samples a usar por árbol (0.5-1.0) - <code>colsample_bytree</code>: Fracción de features a usar por árbol (0.5-1.0) - <code>colsample_bylevel</code>: Fracción de features a usar por nivel del árbol</p>
<p><strong>Proceso de boosting:</strong> - <code>learning_rate</code> (eta): Shrinkage de cada árbol (típicamente 0.01-0.3) - <code>n_estimators</code>: Número de árboles a construir - <code>objective</code>: Función de pérdida a optimizar</p>
</section>
<section id="ejemplos-prácticos-con-xgboost" class="level4">
<h4 class="anchored" data-anchor-id="ejemplos-prácticos-con-xgboost">Ejemplos prácticos con XGBoost</h4>
<p>Comencemos con un ejemplo de clasificación comparando XGBoost con Gradient Boosting de scikit-learn:</p>
<div id="cell-xgboost-vs-sklearn" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingClassifier</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, log_loss</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> xgboost <span class="im">as</span> xgb</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear dataset sintético</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">10000</span>,</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">15</span>,</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Parámetros similares para comparación justa</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>common_params <span class="op">=</span> {</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: <span class="dv">100</span>,</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'learning_rate'</span>: <span class="fl">0.1</span>,</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: <span class="dv">3</span>,</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">'random_state'</span>: <span class="dv">42</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar sklearn Gradient Boosting</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Entrenando sklearn GradientBoosting..."</span>)</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>gb_sklearn <span class="op">=</span> GradientBoostingClassifier(<span class="op">**</span>common_params)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>gb_sklearn.fit(X_train, y_train)</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>time_sklearn <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar XGBoost</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Entrenando XGBoost..."</span>)</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>xgb_model <span class="op">=</span> xgb.XGBClassifier(</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    <span class="op">**</span>common_params,</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    eval_metric<span class="op">=</span><span class="st">'logloss'</span>,</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>    use_label_encoder<span class="op">=</span><span class="va">False</span></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>xgb_model.fit(X_train, y_train, verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>time_xgb <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluar</span></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>y_pred_sklearn <span class="op">=</span> gb_sklearn.predict(X_test)</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>y_pred_xgb <span class="op">=</span> xgb_model.predict(X_test)</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>y_proba_sklearn <span class="op">=</span> gb_sklearn.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>y_proba_xgb <span class="op">=</span> xgb_model.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar resultados</span></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Comparación de accuracy y tiempo</span></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> [<span class="st">'sklearn GB'</span>, <span class="st">'XGBoost'</span>]</span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>accuracies <span class="op">=</span> [</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>    accuracy_score(y_test, y_pred_sklearn),</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>    accuracy_score(y_test, y_pred_xgb)</span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>times <span class="op">=</span> [time_sklearn, time_xgb]</span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].bar(models, accuracies, color<span class="op">=</span>[<span class="st">'#3498db'</span>, <span class="st">'#e74c3c'</span>])</span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Precisión del Modelo'</span>)</span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylim([<span class="fl">0.85</span>, <span class="fl">0.95</span>])</span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (acc, t) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(accuracies, times)):</span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].text(i, acc <span class="op">+</span> <span class="fl">0.002</span>, <span class="ss">f'</span><span class="sc">{</span>acc<span class="sc">:.4f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].bar(models, times, color<span class="op">=</span>[<span class="st">'#3498db'</span>, <span class="st">'#e74c3c'</span>])</span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Tiempo (segundos)'</span>)</span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Tiempo de Entrenamiento'</span>)</span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(times):</span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].text(i, t <span class="op">+</span> <span class="fl">0.05</span>, <span class="ss">f'</span><span class="sc">{</span>t<span class="sc">:.2f}</span><span class="ss">s'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Resultados:"</span>)</span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"sklearn GB - Accuracy: </span><span class="sc">{</span>accuracies[<span class="dv">0</span>]<span class="sc">:.4f}</span><span class="ss">, Tiempo: </span><span class="sc">{</span>time_sklearn<span class="sc">:.2f}</span><span class="ss">s"</span>)</span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"XGBoost    - Accuracy: </span><span class="sc">{</span>accuracies[<span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">, Tiempo: </span><span class="sc">{</span>time_xgb<span class="sc">:.2f}</span><span class="ss">s"</span>)</span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Speedup: </span><span class="sc">{</span>time_sklearn<span class="op">/</span>time_xgb<span class="sc">:.2f}</span><span class="ss">x"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Entrenando sklearn GradientBoosting...
Entrenando XGBoost...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="xgboost-vs-sklearn" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/xgboost-vs-sklearn-output-2.png" class="img-fluid figure-img"></p>
<figcaption>Comparación de XGBoost vs Gradient Boosting de sklearn en clasificación</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Resultados:
sklearn GB - Accuracy: 0.9193, Tiempo: 3.53s
XGBoost    - Accuracy: 0.9153, Tiempo: 0.08s
Speedup: 44.37x</code></pre>
</div>
</div>
<p>Ahora veamos un ejemplo de regresión con California Housing, mostrando diferentes tipos de feature importance en XGBoost:</p>
<div id="cell-xgboost-feature-importance" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_california_housing</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Cargar datos</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>housing <span class="op">=</span> fetch_california_housing()</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.DataFrame(housing.data, columns<span class="op">=</span>housing.feature_names)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> housing.target</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar modelo XGBoost</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>xgb_reg <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>xgb_reg.fit(X_train, y_train)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener diferentes tipos de importance</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>importance_types <span class="op">=</span> [<span class="st">'weight'</span>, <span class="st">'gain'</span>, <span class="st">'cover'</span>]</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> {}</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> imp_type <span class="kw">in</span> importance_types:</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>    importances[imp_type] <span class="op">=</span> xgb_reg.get_booster().get_score(</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>        importance_type<span class="op">=</span>imp_type</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar</span></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, imp_type <span class="kw">in</span> <span class="bu">enumerate</span>(importance_types):</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>    imp_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>        <span class="st">'feature'</span>: <span class="bu">list</span>(importances[imp_type].keys()),</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>        <span class="st">'importance'</span>: <span class="bu">list</span>(importances[imp_type].values())</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>    }).sort_values(<span class="st">'importance'</span>, ascending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>    axes[idx].barh(imp_df[<span class="st">'feature'</span>], imp_df[<span class="st">'importance'</span>], color<span class="op">=</span><span class="st">'#2ecc71'</span>)</span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_xlabel(<span class="st">'Importance'</span>)</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_title(<span class="ss">f'Importance Type: </span><span class="sc">{</span>imp_type<span class="sc">.</span>upper()<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>    axes[idx].grid(axis<span class="op">=</span><span class="st">'x'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Tipos de Feature Importance en XGBoost:"</span>)</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- WEIGHT: Número de veces que la feature aparece en un split"</span>)</span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- GAIN: Ganancia promedio (reducción de loss) cuando se usa la feature"</span>)</span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- COVER: Cobertura promedio (número de observaciones afectadas)"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="xgboost-feature-importance" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/xgboost-feature-importance-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Diferentes tipos de feature importance en XGBoost</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Tipos de Feature Importance en XGBoost:
- WEIGHT: Número de veces que la feature aparece en un split
- GAIN: Ganancia promedio (reducción de loss) cuando se usa la feature
- COVER: Cobertura promedio (número de observaciones afectadas)</code></pre>
</div>
</div>
<p>Ahora demostremos early stopping y validación cruzada integrada:</p>
<div id="cell-xgboost-early-stopping" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear validation set</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>X_train_sub, X_val, y_train_sub, y_val <span class="op">=</span> train_test_split(</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    X_train, y_train, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Modelo con early stopping</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>xgb_early <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">1000</span>,  <span class="co"># Número grande, early stopping decidirá cuándo parar</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    early_stopping_rounds<span class="op">=</span><span class="dv">20</span>  <span class="co"># Parar si no mejora en 20 rondas</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar con validation set</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>eval_set <span class="op">=</span> [(X_train_sub, y_train_sub), (X_val, y_val)]</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>xgb_early.fit(</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    X_train_sub,</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    y_train_sub,</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    eval_set<span class="op">=</span>eval_set,</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener resultados de evaluación</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> xgb_early.evals_result()</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>train_rmse <span class="op">=</span> np.sqrt(results[<span class="st">'validation_0'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>val_rmse <span class="op">=</span> np.sqrt(results[<span class="st">'validation_1'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar learning curves</span></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>plt.plot(train_rmse, label<span class="op">=</span><span class="st">'Training RMSE'</span>, color<span class="op">=</span><span class="st">'#3498db'</span>)</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>plt.plot(val_rmse, label<span class="op">=</span><span class="st">'Validation RMSE'</span>, color<span class="op">=</span><span class="st">'#e74c3c'</span>)</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>plt.axvline(</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>xgb_early.best_iteration,</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">'green'</span>,</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>    linestyle<span class="op">=</span><span class="st">'--'</span>,</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="ss">f'Best iteration (</span><span class="sc">{</span>xgb_early<span class="sc">.</span>best_iteration<span class="sc">}</span><span class="ss">)'</span></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Número de árboles'</span>)</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'RMSE'</span>)</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Learning Curves con Early Stopping'</span>)</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>plt.grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mejor iteración: </span><span class="sc">{</span>xgb_early<span class="sc">.</span>best_iteration<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mejor score de validación: </span><span class="sc">{</span>xgb_early<span class="sc">.</span>best_score<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Árboles ahorrados: </span><span class="sc">{</span><span class="dv">1000</span> <span class="op">-</span> xgb_early<span class="sc">.</span>best_iteration<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="xgboost-early-stopping" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/xgboost-early-stopping-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Early stopping en XGBoost con conjunto de validación</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Mejor iteración: 961
Mejor score de validación: 0.4705
Árboles ahorrados: 39</code></pre>
</div>
</div>
<p>Finalmente, veamos un ejemplo de hyperparameter tuning con Grid Search:</p>
<div id="cell-xgboost-tuning" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Definir grid de hiperparámetros (versión simplificada)</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>],</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'learning_rate'</span>: [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>],</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'subsample'</span>: [<span class="fl">0.8</span>, <span class="fl">1.0</span>],</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'colsample_bytree'</span>: [<span class="fl">0.8</span>, <span class="fl">1.0</span>]</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Modelo base</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>xgb_base <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Grid Search con CV</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    xgb_base,</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    param_grid,</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_root_mean_squared_error'</span>,</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Usar subset de datos para velocidad</span></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>X_subset <span class="op">=</span> X_train[:<span class="dv">5000</span>]</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>y_subset <span class="op">=</span> y_train[:<span class="dv">5000</span>]</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Ejecutando Grid Search..."</span>)</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_subset, y_subset)</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Resultados</span></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Mejores hiperparámetros:"</span>)</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param, value <span class="kw">in</span> grid_search.best_params_.items():</span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>param<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Mejor score (RMSE): </span><span class="sc">{</span><span class="op">-</span>grid_search<span class="sc">.</span>best_score_<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar top 10 combinaciones</span></span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> pd.DataFrame(grid_search.cv_results_)</span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>top_10 <span class="op">=</span> results_df.nsmallest(<span class="dv">10</span>, <span class="st">'rank_test_score'</span>)[</span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'param_max_depth'</span>, <span class="st">'param_learning_rate'</span>,</span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>     <span class="st">'param_subsample'</span>, <span class="st">'param_colsample_bytree'</span>,</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>     <span class="st">'mean_test_score'</span>, <span class="st">'std_test_score'</span>, <span class="st">'rank_test_score'</span>]</span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>x_pos <span class="op">=</span> np.arange(<span class="bu">len</span>(top_10))</span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> <span class="op">-</span>top_10[<span class="st">'mean_test_score'</span>]</span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a>stds <span class="op">=</span> top_10[<span class="st">'std_test_score'</span>]</span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a>ax.bar(x_pos, scores, yerr<span class="op">=</span>stds, capsize<span class="op">=</span><span class="dv">5</span>, color<span class="op">=</span><span class="st">'#9b59b6'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x_pos)</span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels(</span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a>    [<span class="ss">f"Config </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(top_10))],</span>
<span id="cb27-58"><a href="#cb27-58" aria-hidden="true" tabindex="-1"></a>    rotation<span class="op">=</span><span class="dv">45</span></span>
<span id="cb27-59"><a href="#cb27-59" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-60"><a href="#cb27-60" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'RMSE'</span>)</span>
<span id="cb27-61"><a href="#cb27-61" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Top 10 Configuraciones de Hiperparámetros'</span>)</span>
<span id="cb27-62"><a href="#cb27-62" aria-hidden="true" tabindex="-1"></a>ax.grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb27-63"><a href="#cb27-63" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb27-64"><a href="#cb27-64" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb27-65"><a href="#cb27-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-66"><a href="#cb27-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostrar configuraciones</span></span>
<span id="cb27-67"><a href="#cb27-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top 3 configuraciones:"</span>)</span>
<span id="cb27-68"><a href="#cb27-68" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, row <span class="kw">in</span> top_10.head(<span class="dv">3</span>).iterrows():</span>
<span id="cb27-69"><a href="#cb27-69" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Rank </span><span class="sc">{</span><span class="bu">int</span>(row[<span class="st">'rank_test_score'</span>])<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb27-70"><a href="#cb27-70" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  max_depth: </span><span class="sc">{</span>row[<span class="st">'param_max_depth'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-71"><a href="#cb27-71" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  learning_rate: </span><span class="sc">{</span>row[<span class="st">'param_learning_rate'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-72"><a href="#cb27-72" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  subsample: </span><span class="sc">{</span>row[<span class="st">'param_subsample'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-73"><a href="#cb27-73" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  colsample_bytree: </span><span class="sc">{</span>row[<span class="st">'param_colsample_bytree'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-74"><a href="#cb27-74" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  RMSE: </span><span class="sc">{</span><span class="op">-</span>row[<span class="st">'mean_test_score'</span>]<span class="sc">:.4f}</span><span class="ss"> ± </span><span class="sc">{</span>row[<span class="st">'std_test_score'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ejecutando Grid Search...

Mejores hiperparámetros:
  colsample_bytree: 0.8
  learning_rate: 0.1
  max_depth: 7
  subsample: 0.8

Mejor score (RMSE): 0.5048</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="xgboost-tuning" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/xgboost-tuning-output-2.png" class="img-fluid figure-img"></p>
<figcaption>Grid Search para hiperparámetros de XGBoost</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top 3 configuraciones:

Rank 1:
  max_depth: 7.0
  learning_rate: 0.1
  subsample: 0.8
  colsample_bytree: 0.8
  RMSE: 0.5048 ± 0.0117

Rank 2:
  max_depth: 7.0
  learning_rate: 0.1
  subsample: 1.0
  colsample_bytree: 0.8
  RMSE: 0.5071 ± 0.0068

Rank 3:
  max_depth: 5.0
  learning_rate: 0.1
  subsample: 1.0
  colsample_bytree: 0.8
  RMSE: 0.5088 ± 0.0194</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cuándo usar XGBoost
</div>
</div>
<div class="callout-body-container callout-body">
<p>Usa XGBoost cuando: - Trabajas con datos tabulares/estructurados - Necesitas el mejor balance entre precisión y velocidad - Quieres una implementación madura y bien documentada - Participas en competencias de ML (es el estándar de facto) - Necesitas flexibilidad en funciones objetivo y métricas de evaluación</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Instalación de XGBoost
</div>
</div>
<div class="callout-body-container callout-body">
<p>Para usar XGBoost, instálalo con:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install xgboost</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Para soporte de GPU (opcional):</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install xgboost<span class="pp">[</span><span class="ss">gpu</span><span class="pp">]</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section>
</section>
<section id="sec-lightgbm" class="level3">
<h3 class="anchored" data-anchor-id="sec-lightgbm">5.2 LightGBM (Light Gradient Boosting Machine)</h3>
<p>LightGBM, desarrollado por Microsoft Research en 2017, es una implementación de gradient boosting diseñada específicamente para ser extremadamente rápida y eficiente en memoria. Está optimizada para datasets grandes (más de 10,000 muestras y cientos o miles de features) y es particularmente efectiva cuando la velocidad de entrenamiento es crítica.</p>
<p>El nombre “Light” no se refiere a que sea una versión simplificada, sino a que es “ligera” en términos de uso de recursos computacionales mientras mantiene (o incluso supera) la precisión de otros métodos de boosting.</p>
<section id="innovaciones-clave-de-lightgbm" class="level4">
<h4 class="anchored" data-anchor-id="innovaciones-clave-de-lightgbm">Innovaciones clave de LightGBM</h4>
<p><strong>1. GOSS (Gradient-based One-Side Sampling)</strong></p>
<p>Una de las innovaciones más importantes de LightGBM es su estrategia de muestreo inteligente:</p>
<ul>
<li><strong>Problema</strong>: En un dataset grande, calcular el mejor split considerando todas las muestras es costoso</li>
<li><strong>Solución</strong>: No todas las muestras son igualmente importantes para encontrar el mejor split</li>
<li><strong>GOSS funciona así</strong>:
<ol type="1">
<li>Ordena las muestras por el valor absoluto de sus gradientes</li>
<li>Mantiene todas las muestras con gradientes grandes (errores grandes)</li>
<li>Muestrea aleatoriamente una fracción de las muestras con gradientes pequeños</li>
<li>Cuando calcula el gain, compensa las muestras pequeñas con un factor multiplicador</li>
</ol></li>
</ul>
<p>La intuición es que las muestras mal predichas (gradientes grandes) son más informativas para encontrar buenos splits, mientras que las muestras bien predichas contribuyen menos a la decisión del split.</p>
<p><strong>2. EFB (Exclusive Feature Bundling)</strong></p>
<p>Otra innovación para reducir el número de features efectivas:</p>
<ul>
<li><strong>Observación</strong>: En datasets con muchas features sparse (muchos ceros), varias features nunca toman valores no-cero simultáneamente</li>
<li><strong>Solución</strong>: Agrupar features mutuamente exclusivas en un solo “bundle”</li>
<li><strong>Resultado</strong>: Reducir el número de features sin pérdida de información</li>
</ul>
<p>Por ejemplo, en datos de one-hot encoding, múltiples columnas pueden agruparse porque solo una puede ser 1 a la vez.</p>
<p><strong>3. Leaf-wise Tree Growth (vs Level-wise)</strong></p>
<p>Esta es probablemente la diferencia más visible con XGBoost:</p>
<ul>
<li><strong>Level-wise (XGBoost, sklearn)</strong>: Crece el árbol nivel por nivel, dividiendo todos los nodos del mismo nivel</li>
<li><strong>Leaf-wise (LightGBM)</strong>: Crece el árbol dividiendo la hoja que maximiza la reducción de pérdida, independientemente del nivel</li>
</ul>
<pre><code>Level-wise:           Leaf-wise:
    [Root]               [Root]
    /    \               /    \
   []    []            [A]    []
  /  \  /  \              \
 []  [][]  []             [B]</code></pre>
<p><strong>Ventajas del leaf-wise</strong>: - Generalmente alcanza menor loss con el mismo número de splits - Más eficiente computacionalmente</p>
<p><strong>Desventaja</strong>: - Puede sobreajustar más fácilmente creando árboles muy profundos y desbalanceados - Se controla con <code>max_depth</code> y <code>num_leaves</code></p>
<p><strong>4. Histogram-based Learning</strong></p>
<p>En lugar de buscar el mejor split considerando todos los valores posibles:</p>
<ul>
<li>Discretiza features continuas en bins (histogramas de valores)</li>
<li>Solo considera los límites de bins como candidatos para splits</li>
<li>Reduce complejidad de <span class="math inline">\(O(\#data \times \#features)\)</span> a <span class="math inline">\(O(\#bins \times \#features)\)</span></li>
<li>También reduce uso de memoria considerablemente</li>
</ul>
</section>
<section id="hiperparámetros-importantes-en-lightgbm" class="level4">
<h4 class="anchored" data-anchor-id="hiperparámetros-importantes-en-lightgbm">Hiperparámetros importantes en LightGBM</h4>
<p>LightGBM tiene algunos hiperparámetros únicos además de los estándar:</p>
<p><strong>Específicos de LightGBM:</strong> - <code>num_leaves</code>: Número máximo de hojas por árbol (más importante que <code>max_depth</code>) - <code>min_data_in_leaf</code>: Mínimo de samples en una hoja (previene overfitting) - <code>bagging_fraction</code> / <code>subsample</code>: Fracción de datos para cada árbol - <code>feature_fraction</code> / <code>colsample_bytree</code>: Fracción de features para cada árbol - <code>max_bin</code>: Número de bins para histogram-based learning</p>
<p><strong>Regularización:</strong> - <code>lambda_l1</code>: Regularización L1 - <code>lambda_l2</code>: Regularización L2 - <code>min_gain_to_split</code>: Ganancia mínima para hacer un split</p>
<p><strong>Control de velocidad:</strong> - <code>num_threads</code>: Número de threads paralelos - <code>device_type</code>: ‘cpu’ o ‘gpu’</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
num_leaves vs max_depth
</div>
</div>
<div class="callout-body-container callout-body">
<p>En LightGBM, <code>num_leaves</code> es más importante que <code>max_depth</code> debido al crecimiento leaf-wise. Una regla general es: <span class="math display">\[\text{num\_leaves} \leq 2^{\text{max\_depth}}\]</span></p>
<p>Un árbol con <code>max_depth=5</code> podría tener hasta 32 hojas, pero típicamente queremos menos (e.g., <code>num_leaves=31</code>) para mejor generalización.</p>
</div>
</div>
</section>
<section id="ejemplos-prácticos-con-lightgbm" class="level4">
<h4 class="anchored" data-anchor-id="ejemplos-prácticos-con-lightgbm">Ejemplos prácticos con LightGBM</h4>
<p>Comencemos comparando la velocidad de LightGBM con XGBoost en un dataset grande:</p>
<div id="cell-lightgbm-speed-comparison" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> lightgbm <span class="im">as</span> lgb</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_regression</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear dataset más grande para ver diferencias de velocidad</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>X_large, y_large <span class="op">=</span> make_regression(</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">50000</span>,</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">80</span>,</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>X_train_l, X_test_l, y_train_l, y_test_l <span class="op">=</span> train_test_split(</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    X_large, y_large, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Parámetros comparables</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>n_estimators <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a><span class="co"># XGBoost</span></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Entrenando XGBoost..."</span>)</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>xgb_large <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span>n_estimators,</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>    verbosity<span class="op">=</span><span class="dv">0</span></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>xgb_large.fit(X_train_l, y_train_l)</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>time_xgb <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>score_xgb <span class="op">=</span> xgb_large.score(X_test_l, y_test_l)</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a><span class="co"># LightGBM</span></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Entrenando LightGBM..."</span>)</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>lgb_model <span class="op">=</span> lgb.LGBMRegressor(</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span>n_estimators,</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>    num_leaves<span class="op">=</span><span class="dv">31</span>,  <span class="co"># Aproximadamente 2^5</span></span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>lgb_model.fit(X_train_l, y_train_l)</span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>time_lgb <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a>score_lgb <span class="op">=</span> lgb_model.score(X_test_l, y_test_l)</span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar comparación</span></span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> [<span class="st">'XGBoost'</span>, <span class="st">'LightGBM'</span>]</span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a>times <span class="op">=</span> [time_xgb, time_lgb]</span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> [score_xgb, score_lgb]</span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Tiempo</span></span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].bar(models, times, color<span class="op">=</span>[<span class="st">'#e74c3c'</span>, <span class="st">'#16a085'</span>])</span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Tiempo (segundos)'</span>)</span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Tiempo de Entrenamiento'</span>)</span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(times):</span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].text(i, t <span class="op">+</span> <span class="fl">0.1</span>, <span class="ss">f'</span><span class="sc">{</span>t<span class="sc">:.2f}</span><span class="ss">s'</span>, ha<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb33-61"><a href="#cb33-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-62"><a href="#cb33-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Score (R²)</span></span>
<span id="cb33-63"><a href="#cb33-63" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].bar(models, scores, color<span class="op">=</span>[<span class="st">'#e74c3c'</span>, <span class="st">'#16a085'</span>])</span>
<span id="cb33-64"><a href="#cb33-64" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'R² Score'</span>)</span>
<span id="cb33-65"><a href="#cb33-65" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Precisión del Modelo'</span>)</span>
<span id="cb33-66"><a href="#cb33-66" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylim([<span class="fl">0.9</span>, <span class="fl">1.0</span>])</span>
<span id="cb33-67"><a href="#cb33-67" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(scores):</span>
<span id="cb33-68"><a href="#cb33-68" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].text(i, s <span class="op">+</span> <span class="fl">0.002</span>, <span class="ss">f'</span><span class="sc">{</span>s<span class="sc">:.4f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb33-69"><a href="#cb33-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-70"><a href="#cb33-70" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb33-71"><a href="#cb33-71" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb33-72"><a href="#cb33-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-73"><a href="#cb33-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Dataset: </span><span class="sc">{</span>X_large<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">:,}</span><span class="ss"> samples, </span><span class="sc">{</span>X_large<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> features"</span>)</span>
<span id="cb33-74"><a href="#cb33-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"XGBoost  : </span><span class="sc">{</span>time_xgb<span class="sc">:.2f}</span><span class="ss">s, R² = </span><span class="sc">{</span>score_xgb<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-75"><a href="#cb33-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LightGBM : </span><span class="sc">{</span>time_lgb<span class="sc">:.2f}</span><span class="ss">s, R² = </span><span class="sc">{</span>score_lgb<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-76"><a href="#cb33-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Speedup  : </span><span class="sc">{</span>time_xgb<span class="op">/</span>time_lgb<span class="sc">:.2f}</span><span class="ss">x más rápido"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Entrenando XGBoost...
Entrenando LightGBM...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="lightgbm-speed-comparison" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/lightgbm-speed-comparison-output-2.png" class="img-fluid figure-img"></p>
<figcaption>Comparación de velocidad entre LightGBM y XGBoost</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Dataset: 50,000 samples, 100 features
XGBoost  : 0.62s, R² = 0.6097
LightGBM : 0.93s, R² = 0.6476
Speedup  : 0.67x más rápido</code></pre>
</div>
</div>
<p>Ahora visualicemos la diferencia entre leaf-wise y level-wise tree growth:</p>
<div id="cell-lightgbm-leaf-vs-level" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear dataset simple para visualización</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>X_moons, y_moons <span class="op">=</span> make_moons(n_samples<span class="op">=</span><span class="dv">300</span>, noise<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar modelos con pocos árboles para visualizar</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>lgb_leaf <span class="op">=</span> lgb.LGBMClassifier(</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    num_leaves<span class="op">=</span><span class="dv">7</span>,</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>xgb_level <span class="op">=</span> xgb.XGBClassifier(</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">3</span>,  <span class="co"># Aproximadamente 7 hojas</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>    verbosity<span class="op">=</span><span class="dv">0</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>lgb_leaf.fit(X_moons, y_moons)</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>xgb_level.fit(X_moons, y_moons)</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear grid para decision boundary</span></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>x_min, x_max <span class="op">=</span> X_moons[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">0.5</span>, X_moons[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">0.5</span></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>y_min, y_max <span class="op">=</span> X_moons[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">0.5</span>, X_moons[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">0.5</span></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.linspace(x_min, x_max, <span class="dv">200</span>),</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>                     np.linspace(y_min, y_max, <span class="dv">200</span>))</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>Z_lgb <span class="op">=</span> lgb_leaf.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>Z_lgb <span class="op">=</span> Z_lgb.reshape(xx.shape)</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>Z_xgb <span class="op">=</span> xgb_level.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>Z_xgb <span class="op">=</span> Z_xgb.reshape(xx.shape)</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar</span></span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a><span class="co"># LightGBM (leaf-wise)</span></span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].contourf(xx, yy, Z_lgb, alpha<span class="op">=</span><span class="fl">0.3</span>, cmap<span class="op">=</span><span class="st">'RdYlBu'</span>)</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].scatter(X_moons[:, <span class="dv">0</span>], X_moons[:, <span class="dv">1</span>], c<span class="op">=</span>y_moons,</span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a>               cmap<span class="op">=</span><span class="st">'RdYlBu'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'LightGBM (Leaf-wise)</span><span class="ch">\n</span><span class="st">5 árboles, 7 hojas max'</span>)</span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Feature 1'</span>)</span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Feature 2'</span>)</span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a><span class="co"># XGBoost (level-wise)</span></span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].contourf(xx, yy, Z_xgb, alpha<span class="op">=</span><span class="fl">0.3</span>, cmap<span class="op">=</span><span class="st">'RdYlBu'</span>)</span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].scatter(X_moons[:, <span class="dv">0</span>], X_moons[:, <span class="dv">1</span>], c<span class="op">=</span>y_moons,</span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a>               cmap<span class="op">=</span><span class="st">'RdYlBu'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'XGBoost (Level-wise)</span><span class="ch">\n</span><span class="st">5 árboles, profundidad 3'</span>)</span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Feature 1'</span>)</span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Feature 2'</span>)</span>
<span id="cb36-57"><a href="#cb36-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-58"><a href="#cb36-58" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb36-59"><a href="#cb36-59" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb36-60"><a href="#cb36-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-61"><a href="#cb36-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LightGBM accuracy: </span><span class="sc">{</span>lgb_leaf<span class="sc">.</span>score(X_moons, y_moons)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb36-62"><a href="#cb36-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"XGBoost accuracy: </span><span class="sc">{</span>xgb_level<span class="sc">.</span>score(X_moons, y_moons)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="lightgbm-leaf-vs-level" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/lightgbm-leaf-vs-level-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Comparación visual de estrategias de crecimiento de árbol</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>LightGBM accuracy: 0.9333
XGBoost accuracy: 0.9233</code></pre>
</div>
</div>
<p>Veamos el manejo nativo de categorical features en LightGBM:</p>
<div id="cell-lightgbm-categorical" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear dataset con features categóricas</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Features categóricas simuladas</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>cities <span class="op">=</span> np.random.choice([<span class="st">'NY'</span>, <span class="st">'LA'</span>, <span class="st">'Chicago'</span>, <span class="st">'Houston'</span>, <span class="st">'Phoenix'</span>], n_samples)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> np.random.choice([<span class="st">'Red'</span>, <span class="st">'Blue'</span>, <span class="st">'Green'</span>], n_samples)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>sizes <span class="op">=</span> np.random.choice([<span class="st">'S'</span>, <span class="st">'M'</span>, <span class="st">'L'</span>, <span class="st">'XL'</span>], n_samples)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Features numéricas</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>num_feat1 <span class="op">=</span> np.random.randn(n_samples)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>num_feat2 <span class="op">=</span> np.random.randn(n_samples)</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Target: función compleja de todas las features</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>y_cat <span class="op">=</span> (</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>    (cities <span class="op">==</span> <span class="st">'NY'</span>).astype(<span class="bu">int</span>) <span class="op">*</span> <span class="dv">10</span> <span class="op">+</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>    (colors <span class="op">==</span> <span class="st">'Red'</span>).astype(<span class="bu">int</span>) <span class="op">*</span> <span class="dv">5</span> <span class="op">+</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>    (sizes <span class="op">==</span> <span class="st">'L'</span>).astype(<span class="bu">int</span>) <span class="op">*</span> <span class="dv">3</span> <span class="op">+</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>    num_feat1 <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>    num_feat2 <span class="op">*</span> <span class="fl">1.5</span> <span class="op">+</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>    np.random.randn(n_samples) <span class="op">*</span> <span class="fl">0.5</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear DataFrame</span></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>df_cat <span class="op">=</span> pd.DataFrame({</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'city'</span>: cities,</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">'color'</span>: colors,</span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">'size'</span>: sizes,</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">'num1'</span>: num_feat1,</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">'num2'</span>: num_feat2,</span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">'target'</span>: y_cat</span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Split</span></span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>train_df, test_df <span class="op">=</span> train_test_split(df_cat, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Método 1: One-hot encoding (tradicional)</span></span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>X_train_onehot <span class="op">=</span> pd.get_dummies(</span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>    train_df.drop(<span class="st">'target'</span>, axis<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>[<span class="st">'city'</span>, <span class="st">'color'</span>, <span class="st">'size'</span>]</span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true" tabindex="-1"></a>X_test_onehot <span class="op">=</span> pd.get_dummies(</span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true" tabindex="-1"></a>    test_df.drop(<span class="st">'target'</span>, axis<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>[<span class="st">'city'</span>, <span class="st">'color'</span>, <span class="st">'size'</span>]</span>
<span id="cb38-45"><a href="#cb38-45" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-46"><a href="#cb38-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Asegurar mismas columnas</span></span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true" tabindex="-1"></a>X_test_onehot <span class="op">=</span> X_test_onehot.reindex(columns<span class="op">=</span>X_train_onehot.columns, fill_value<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb38-48"><a href="#cb38-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-49"><a href="#cb38-49" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb38-50"><a href="#cb38-50" aria-hidden="true" tabindex="-1"></a>lgb_onehot <span class="op">=</span> lgb.LGBMRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>, verbose<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb38-51"><a href="#cb38-51" aria-hidden="true" tabindex="-1"></a>lgb_onehot.fit(X_train_onehot, train_df[<span class="st">'target'</span>])</span>
<span id="cb38-52"><a href="#cb38-52" aria-hidden="true" tabindex="-1"></a>time_onehot <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb38-53"><a href="#cb38-53" aria-hidden="true" tabindex="-1"></a>score_onehot <span class="op">=</span> lgb_onehot.score(X_test_onehot, test_df[<span class="st">'target'</span>])</span>
<span id="cb38-54"><a href="#cb38-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-55"><a href="#cb38-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Método 2: Categorical encoding nativo de LightGBM</span></span>
<span id="cb38-56"><a href="#cb38-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Convertir a categorías</span></span>
<span id="cb38-57"><a href="#cb38-57" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> [<span class="st">'city'</span>, <span class="st">'color'</span>, <span class="st">'size'</span>]:</span>
<span id="cb38-58"><a href="#cb38-58" aria-hidden="true" tabindex="-1"></a>    train_df[col] <span class="op">=</span> train_df[col].astype(<span class="st">'category'</span>)</span>
<span id="cb38-59"><a href="#cb38-59" aria-hidden="true" tabindex="-1"></a>    test_df[col] <span class="op">=</span> test_df[col].astype(<span class="st">'category'</span>)</span>
<span id="cb38-60"><a href="#cb38-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-61"><a href="#cb38-61" aria-hidden="true" tabindex="-1"></a>X_train_cat <span class="op">=</span> train_df.drop(<span class="st">'target'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb38-62"><a href="#cb38-62" aria-hidden="true" tabindex="-1"></a>X_test_cat <span class="op">=</span> test_df.drop(<span class="st">'target'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb38-63"><a href="#cb38-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-64"><a href="#cb38-64" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb38-65"><a href="#cb38-65" aria-hidden="true" tabindex="-1"></a>lgb_cat <span class="op">=</span> lgb.LGBMRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>, verbose<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb38-66"><a href="#cb38-66" aria-hidden="true" tabindex="-1"></a>lgb_cat.fit(</span>
<span id="cb38-67"><a href="#cb38-67" aria-hidden="true" tabindex="-1"></a>    X_train_cat,</span>
<span id="cb38-68"><a href="#cb38-68" aria-hidden="true" tabindex="-1"></a>    train_df[<span class="st">'target'</span>],</span>
<span id="cb38-69"><a href="#cb38-69" aria-hidden="true" tabindex="-1"></a>    categorical_feature<span class="op">=</span>[<span class="st">'city'</span>, <span class="st">'color'</span>, <span class="st">'size'</span>]</span>
<span id="cb38-70"><a href="#cb38-70" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-71"><a href="#cb38-71" aria-hidden="true" tabindex="-1"></a>time_cat <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb38-72"><a href="#cb38-72" aria-hidden="true" tabindex="-1"></a>score_cat <span class="op">=</span> lgb_cat.score(X_test_cat, test_df[<span class="st">'target'</span>])</span>
<span id="cb38-73"><a href="#cb38-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-74"><a href="#cb38-74" aria-hidden="true" tabindex="-1"></a><span class="co"># Comparar resultados</span></span>
<span id="cb38-75"><a href="#cb38-75" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb38-76"><a href="#cb38-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-77"><a href="#cb38-77" aria-hidden="true" tabindex="-1"></a>methods <span class="op">=</span> [<span class="st">'One-Hot</span><span class="ch">\n</span><span class="st">Encoding'</span>, <span class="st">'Native</span><span class="ch">\n</span><span class="st">Categorical'</span>]</span>
<span id="cb38-78"><a href="#cb38-78" aria-hidden="true" tabindex="-1"></a>times <span class="op">=</span> [time_onehot, time_cat]</span>
<span id="cb38-79"><a href="#cb38-79" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> [score_onehot, score_cat]</span>
<span id="cb38-80"><a href="#cb38-80" aria-hidden="true" tabindex="-1"></a>n_features <span class="op">=</span> [X_train_onehot.shape[<span class="dv">1</span>], X_train_cat.shape[<span class="dv">1</span>]]</span>
<span id="cb38-81"><a href="#cb38-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-82"><a href="#cb38-82" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].bar(methods, times, color<span class="op">=</span>[<span class="st">'#3498db'</span>, <span class="st">'#2ecc71'</span>])</span>
<span id="cb38-83"><a href="#cb38-83" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Tiempo (segundos)'</span>)</span>
<span id="cb38-84"><a href="#cb38-84" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Tiempo de Entrenamiento'</span>)</span>
<span id="cb38-85"><a href="#cb38-85" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (t, n) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(times, n_features)):</span>
<span id="cb38-86"><a href="#cb38-86" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].text(i, t <span class="op">+</span> <span class="fl">0.001</span>, <span class="ss">f'</span><span class="sc">{</span>t<span class="sc">:.3f}</span><span class="ss">s</span><span class="ch">\n</span><span class="ss">(</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss"> features)'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb38-87"><a href="#cb38-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-88"><a href="#cb38-88" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].bar(methods, scores, color<span class="op">=</span>[<span class="st">'#3498db'</span>, <span class="st">'#2ecc71'</span>])</span>
<span id="cb38-89"><a href="#cb38-89" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'R² Score'</span>)</span>
<span id="cb38-90"><a href="#cb38-90" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Precisión del Modelo'</span>)</span>
<span id="cb38-91"><a href="#cb38-91" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(scores):</span>
<span id="cb38-92"><a href="#cb38-92" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].text(i, s <span class="op">-</span> <span class="fl">0.02</span>, <span class="ss">f'</span><span class="sc">{</span>s<span class="sc">:.4f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb38-93"><a href="#cb38-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-94"><a href="#cb38-94" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb38-95"><a href="#cb38-95" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb38-96"><a href="#cb38-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-97"><a href="#cb38-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Comparación:"</span>)</span>
<span id="cb38-98"><a href="#cb38-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"One-hot encoding: </span><span class="sc">{</span>X_train_onehot<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> features, R² = </span><span class="sc">{</span>score_onehot<span class="sc">:.4f}</span><span class="ss">, </span><span class="sc">{</span>time_onehot<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb38-99"><a href="#cb38-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Native categorical: </span><span class="sc">{</span>X_train_cat<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> features, R² = </span><span class="sc">{</span>score_cat<span class="sc">:.4f}</span><span class="ss">, </span><span class="sc">{</span>time_cat<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb38-100"><a href="#cb38-100" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Ventajas del manejo nativo:"</span>)</span>
<span id="cb38-101"><a href="#cb38-101" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- Menos features (no explosión dimensional)"</span>)</span>
<span id="cb38-102"><a href="#cb38-102" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- Más rápido"</span>)</span>
<span id="cb38-103"><a href="#cb38-103" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- Mejor o similar precisión"</span>)</span>
<span id="cb38-104"><a href="#cb38-104" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- Automático (no requiere preprocesamiento manual)"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="lightgbm-categorical" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/lightgbm-categorical-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Manejo de features categóricas en LightGBM</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Comparación:
One-hot encoding: 14 features, R² = 0.9838, 0.246s
Native categorical: 5 features, R² = 0.9812, 0.243s

Ventajas del manejo nativo:
- Menos features (no explosión dimensional)
- Más rápido
- Mejor o similar precisión
- Automático (no requiere preprocesamiento manual)</code></pre>
</div>
</div>
<p>Feature importance en LightGBM:</p>
<div id="cell-lightgbm-importance" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Usar el modelo categórico anterior</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener importances</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> lgb_cat.feature_importances_</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> X_train_cat.columns</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear DataFrame y ordenar</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>imp_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'feature'</span>: feature_names,</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'importance'</span>: importances</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>}).sort_values(<span class="st">'importance'</span>, ascending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'#e74c3c'</span> <span class="cf">if</span> <span class="st">'num'</span> <span class="kw">in</span> f <span class="cf">else</span> <span class="st">'#3498db'</span> <span class="cf">for</span> f <span class="kw">in</span> imp_df[<span class="st">'feature'</span>]]</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>plt.barh(imp_df[<span class="st">'feature'</span>], imp_df[<span class="st">'importance'</span>], color<span class="op">=</span>colors)</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Importance (gain)'</span>)</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Feature Importance en LightGBM</span><span class="ch">\n</span><span class="st">Rojo = numérica, Azul = categórica'</span>)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>plt.grid(axis<span class="op">=</span><span class="st">'x'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Mostrar valores</span></span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Feature Importances:"</span>)</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feat, imp <span class="kw">in</span> <span class="bu">zip</span>(imp_df[<span class="st">'feature'</span>], imp_df[<span class="st">'importance'</span>]):</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feat<span class="sc">:15s}</span><span class="ss">: </span><span class="sc">{</span>imp<span class="sc">:8.2f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="lightgbm-importance" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/lightgbm-importance-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Feature importance en LightGBM</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Feature Importances:
  size           :    93.00
  city           :   109.00
  color          :   264.00
  num1           :  1246.00
  num2           :  1257.00</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cuándo usar LightGBM
</div>
</div>
<div class="callout-body-container callout-body">
<p>Usa LightGBM cuando: - Tienes datasets grandes (&gt;10,000 muestras, &gt;100 features) - La velocidad de entrenamiento es crítica - Tienes muchas features categóricas - Tienes limitaciones de memoria - Necesitas entrenar muchos modelos (AutoML, hyperparameter tuning extensivo)</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Control del overfitting en leaf-wise growth
</div>
</div>
<div class="callout-body-container callout-body">
<p>Debido al crecimiento leaf-wise más agresivo, LightGBM puede sobreajustar más fácilmente. Contrólalo con:</p>
<ul>
<li><code>num_leaves</code>: Reduce si hay overfitting (típicamente 20-50)</li>
<li><code>min_data_in_leaf</code>: Aumenta (típicamente 20-100)</li>
<li><code>max_depth</code>: Limita la profundidad (-1 = sin límite)</li>
<li><code>lambda_l1</code>, <code>lambda_l2</code>: Aumenta regularización</li>
<li><code>bagging_fraction</code>: Usa &lt;1.0 para añadir randomness</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Instalación de LightGBM
</div>
</div>
<div class="callout-body-container callout-body">
<p>Para instalar LightGBM:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install lightgbm</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Para soporte de GPU:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install lightgbm <span class="at">--install-option</span><span class="op">=</span>--gpu</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section>
</section>
<section id="sec-catboost" class="level3">
<h3 class="anchored" data-anchor-id="sec-catboost">5.3 CatBoost (Categorical Boosting)</h3>
<p>CatBoost, desarrollado por Yandex en 2017, es una implementación de gradient boosting que destaca por dos características principales: su manejo excepcional de features categóricas y su robustez con parámetros por defecto. El nombre “CatBoost” viene de “Category Boosting”, reflejando su especialización en datos categóricos.</p>
<p>Una de las ventajas más apreciadas de CatBoost es que típicamente requiere muy poco tuning de hiperparámetros y produce buenos resultados “out of the box”, lo que lo hace excelente para producción y para usuarios que no tienen tiempo para optimización extensiva.</p>
<section id="innovaciones-clave-de-catboost" class="level4">
<h4 class="anchored" data-anchor-id="innovaciones-clave-de-catboost">Innovaciones clave de CatBoost</h4>
<p><strong>1. Ordered Boosting</strong></p>
<p>Una de las innovaciones más importantes de CatBoost es su solución al problema del <strong>prediction shift</strong>:</p>
<p><strong>El problema del prediction shift:</strong> - En gradient boosting tradicional, calculamos los gradientes usando predicciones del modelo actual - Pero esas predicciones fueron generadas usando las mismas observaciones que ahora estamos usando para entrenar - Esto introduce un bias sutil: el modelo está “overfitting” a las observaciones de entrenamiento</p>
<p><strong>Solución de CatBoost - Ordered Boosting:</strong> - Usa diferentes permutaciones aleatorias del dataset para diferentes árboles - Para predecir la observación <span class="math inline">\(i\)</span>, solo usa información de observaciones que aparecen antes de <span class="math inline">\(i\)</span> en la permutación - Esto simula el proceso de predicción en datos nuevos</p>
<p>La intuición es similar a time series forecasting: no puedes usar información del futuro para predecir el pasado.</p>
<p><strong>2. Manejo nativo de categorical features</strong></p>
<p>CatBoost tiene el mejor manejo de features categóricas entre todas las implementaciones de boosting:</p>
<p><strong>Problema con one-hot encoding:</strong> - Explota la dimensionalidad con categorías high-cardinality - Pierde información de frecuencias - No funciona bien con categorías raras</p>
<p><strong>Problema con label encoding (1, 2, 3, …):</strong> - Introduce orden artificial - No captura relación con el target</p>
<p><strong>Solución de CatBoost - Target Statistics con Ordered TS:</strong></p>
<p>Para cada categoría, calcula estadísticas del target (e.g., promedio), pero de manera ordenada:</p>
<p><span class="math display">\[\text{TargetStat}_i = \frac{\sum_{j=1}^{i-1} [x_j = \text{categoria}] \cdot y_j + \alpha \cdot P}{\sum_{j=1}^{i-1} [x_j = \text{categoria}] + \alpha}\]</span></p>
<p>donde: - Solo usa observaciones anteriores en la permutación (ordered) - <span class="math inline">\(\alpha\)</span> y <span class="math inline">\(P\)</span> son parámetros de prior (regularización Bayesiana) - Previene target leakage perfectamente</p>
<p><strong>Ventajas:</strong> - No necesitas preprocesar categorías manualmente - Funciona con high-cardinality features - No explota dimensionalidad - Captura relación con target sin leakage</p>
<p><strong>3. Oblivious Trees (Symmetric Trees)</strong></p>
<p>CatBoost usa un tipo especial de árboles de decisión:</p>
<p><strong>Árbol Normal:</strong></p>
<pre><code>       [Feature A &gt; 5]
         /         \
    [Feature B &gt; 3]  [Feature C &gt; 7]
     /      \         /       \
   Leaf1  Leaf2    Leaf3    Leaf4</code></pre>
<p><strong>Árbol Oblivious (CatBoost):</strong></p>
<pre><code>       [Feature A &gt; 5]
         /         \
    [Feature B &gt; 3]  [Feature B &gt; 3]  &lt;- Mismo split
     /      \         /       \
   Leaf1  Leaf2    Leaf3    Leaf4</code></pre>
<p>En cada nivel, todos los nodos usan la misma condición de split.</p>
<p><strong>Ventajas:</strong> - Estructura más simple, menos propensa a overfitting - Predicción extremadamente rápida (solo <span class="math inline">\(\log_2(N_{leaves})\)</span> comparaciones) - Más fácil de paralelizar y optimizar - Mejor para deployment en producción</p>
<p><strong>Desventaja:</strong> - Menos expresivos que árboles normales (necesitas más profundidad)</p>
<p><strong>4. Parámetros por defecto robustos</strong></p>
<p>A diferencia de XGBoost y LightGBM, CatBoost está diseñado para funcionar bien sin tuning extensivo: - Ordered boosting reduce overfitting automáticamente - Regularización adecuada por defecto - Menos sensible a la elección de hiperparámetros</p>
</section>
<section id="hiperparámetros-importantes-en-catboost" class="level4">
<h4 class="anchored" data-anchor-id="hiperparámetros-importantes-en-catboost">Hiperparámetros importantes en CatBoost</h4>
<p>CatBoost tiene una nomenclatura ligeramente diferente:</p>
<p><strong>Estructura del árbol:</strong> - <code>depth</code>: Profundidad del árbol (default = 6, rango típico 4-10) - <code>border_count</code>: Número de splits para features numéricas (similar a max_bins en LightGBM)</p>
<p><strong>Proceso de boosting:</strong> - <code>iterations</code>: Número de árboles (equivalente a n_estimators) - <code>learning_rate</code>: Shrinkage (default = auto, típicamente 0.03-0.3)</p>
<p><strong>Regularización:</strong> - <code>l2_leaf_reg</code>: Regularización L2 (default = 3.0) - <code>random_strength</code>: Cantidad de randomness en splits (default = 1.0) - <code>bagging_temperature</code>: Controla la intensidad del Bayesian bootstrap</p>
<p><strong>Categorical features:</strong> - <code>cat_features</code>: Índices o nombres de features categóricas - <code>one_hot_max_size</code>: Máximo número de categorías para usar one-hot en lugar de target statistics (default = 2)</p>
<p><strong>Otros:</strong> - <code>task_type</code>: ‘CPU’ o ‘GPU’ - <code>verbose</code>: Nivel de output durante entrenamiento</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning rate automático
</div>
</div>
<div class="callout-body-container callout-body">
<p>Si no especificas <code>learning_rate</code>, CatBoost lo selecciona automáticamente basándose en el número de iteraciones y el tamaño del dataset. Esta es una de las características que hacen a CatBoost “low-maintenance”.</p>
</div>
</div>
</section>
<section id="ejemplos-prácticos-con-catboost" class="level4">
<h4 class="anchored" data-anchor-id="ejemplos-prácticos-con-catboost">Ejemplos prácticos con CatBoost</h4>
<p>Comencemos con un ejemplo mostrando el manejo de categorical features:</p>
<div id="cell-catboost-categorical" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> catboost <span class="im">import</span> CatBoostRegressor, Pool</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear dataset realista con categorías</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Features categóricas realistas</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>departments <span class="op">=</span> np.random.choice([<span class="st">'Sales'</span>, <span class="st">'Engineering'</span>, <span class="st">'Marketing'</span>, <span class="st">'HR'</span>, <span class="st">'Finance'</span>], n)</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>locations <span class="op">=</span> np.random.choice([<span class="st">'NY'</span>, <span class="st">'SF'</span>, <span class="st">'Austin'</span>, <span class="st">'Seattle'</span>, <span class="st">'Boston'</span>, <span class="st">'Chicago'</span>], n)</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>education <span class="op">=</span> np.random.choice([<span class="st">'HS'</span>, <span class="st">'Bachelor'</span>, <span class="st">'Master'</span>, <span class="st">'PhD'</span>], n)</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Mapeo para crear target realista</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>dept_effect <span class="op">=</span> {<span class="st">'Sales'</span>: <span class="dv">50000</span>, <span class="st">'Engineering'</span>: <span class="dv">90000</span>, <span class="st">'Marketing'</span>: <span class="dv">60000</span>,</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>               <span class="st">'HR'</span>: <span class="dv">55000</span>, <span class="st">'Finance'</span>: <span class="dv">70000</span>}</span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>loc_effect <span class="op">=</span> {<span class="st">'NY'</span>: <span class="dv">20000</span>, <span class="st">'SF'</span>: <span class="dv">25000</span>, <span class="st">'Austin'</span>: <span class="dv">5000</span>,</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>              <span class="st">'Seattle'</span>: <span class="dv">15000</span>, <span class="st">'Boston'</span>: <span class="dv">12000</span>, <span class="st">'Chicago'</span>: <span class="dv">8000</span>}</span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>edu_effect <span class="op">=</span> {<span class="st">'HS'</span>: <span class="dv">0</span>, <span class="st">'Bachelor'</span>: <span class="dv">15000</span>, <span class="st">'Master'</span>: <span class="dv">30000</span>, <span class="st">'PhD'</span>: <span class="dv">45000</span>}</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Features numéricas</span></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>years_exp <span class="op">=</span> np.random.exponential(<span class="dv">5</span>, n)</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>age <span class="op">=</span> np.random.normal(<span class="dv">35</span>, <span class="dv">10</span>, n)</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>age <span class="op">=</span> np.clip(age, <span class="dv">22</span>, <span class="dv">65</span>)</span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Target: salary</span></span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>salary <span class="op">=</span> (</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>    np.array([dept_effect[d] <span class="cf">for</span> d <span class="kw">in</span> departments]) <span class="op">+</span></span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>    np.array([loc_effect[l] <span class="cf">for</span> l <span class="kw">in</span> locations]) <span class="op">+</span></span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>    np.array([edu_effect[e] <span class="cf">for</span> e <span class="kw">in</span> education]) <span class="op">+</span></span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>    years_exp <span class="op">*</span> <span class="dv">2000</span> <span class="op">+</span></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>    (age <span class="op">-</span> <span class="dv">22</span>) <span class="op">*</span> <span class="dv">500</span> <span class="op">+</span></span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>    np.random.normal(<span class="dv">0</span>, <span class="dv">8000</span>, n)</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear DataFrame</span></span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a>df_salary <span class="op">=</span> pd.DataFrame({</span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">'department'</span>: departments,</span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'location'</span>: locations,</span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">'education'</span>: education,</span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">'years_experience'</span>: years_exp,</span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">'age'</span>: age,</span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a>    <span class="st">'salary'</span>: salary</span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Split</span></span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a>train_df_sal, test_df_sal <span class="op">=</span> train_test_split(df_salary, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Método 1: XGBoost con one-hot encoding</span></span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a>X_train_ohe <span class="op">=</span> pd.get_dummies(</span>
<span id="cb46-51"><a href="#cb46-51" aria-hidden="true" tabindex="-1"></a>    train_df_sal.drop(<span class="st">'salary'</span>, axis<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb46-52"><a href="#cb46-52" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>[<span class="st">'department'</span>, <span class="st">'location'</span>, <span class="st">'education'</span>]</span>
<span id="cb46-53"><a href="#cb46-53" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-54"><a href="#cb46-54" aria-hidden="true" tabindex="-1"></a>X_test_ohe <span class="op">=</span> pd.get_dummies(</span>
<span id="cb46-55"><a href="#cb46-55" aria-hidden="true" tabindex="-1"></a>    test_df_sal.drop(<span class="st">'salary'</span>, axis<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb46-56"><a href="#cb46-56" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>[<span class="st">'department'</span>, <span class="st">'location'</span>, <span class="st">'education'</span>]</span>
<span id="cb46-57"><a href="#cb46-57" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-58"><a href="#cb46-58" aria-hidden="true" tabindex="-1"></a>X_test_ohe <span class="op">=</span> X_test_ohe.reindex(columns<span class="op">=</span>X_train_ohe.columns, fill_value<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb46-59"><a href="#cb46-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-60"><a href="#cb46-60" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb46-61"><a href="#cb46-61" aria-hidden="true" tabindex="-1"></a>xgb_ohe <span class="op">=</span> xgb.XGBRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>, verbosity<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb46-62"><a href="#cb46-62" aria-hidden="true" tabindex="-1"></a>xgb_ohe.fit(X_train_ohe, train_df_sal[<span class="st">'salary'</span>])</span>
<span id="cb46-63"><a href="#cb46-63" aria-hidden="true" tabindex="-1"></a>time_xgb_ohe <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb46-64"><a href="#cb46-64" aria-hidden="true" tabindex="-1"></a>score_xgb_ohe <span class="op">=</span> xgb_ohe.score(X_test_ohe, test_df_sal[<span class="st">'salary'</span>])</span>
<span id="cb46-65"><a href="#cb46-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-66"><a href="#cb46-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Método 2: CatBoost con categorical features nativas</span></span>
<span id="cb46-67"><a href="#cb46-67" aria-hidden="true" tabindex="-1"></a>X_train_cat_sal <span class="op">=</span> train_df_sal.drop(<span class="st">'salary'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb46-68"><a href="#cb46-68" aria-hidden="true" tabindex="-1"></a>X_test_cat_sal <span class="op">=</span> test_df_sal.drop(<span class="st">'salary'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb46-69"><a href="#cb46-69" aria-hidden="true" tabindex="-1"></a>y_train_sal <span class="op">=</span> train_df_sal[<span class="st">'salary'</span>]</span>
<span id="cb46-70"><a href="#cb46-70" aria-hidden="true" tabindex="-1"></a>y_test_sal <span class="op">=</span> test_df_sal[<span class="st">'salary'</span>]</span>
<span id="cb46-71"><a href="#cb46-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-72"><a href="#cb46-72" aria-hidden="true" tabindex="-1"></a>cat_features_list <span class="op">=</span> [<span class="st">'department'</span>, <span class="st">'location'</span>, <span class="st">'education'</span>]</span>
<span id="cb46-73"><a href="#cb46-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-74"><a href="#cb46-74" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb46-75"><a href="#cb46-75" aria-hidden="true" tabindex="-1"></a>cat_model <span class="op">=</span> CatBoostRegressor(</span>
<span id="cb46-76"><a href="#cb46-76" aria-hidden="true" tabindex="-1"></a>    iterations<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb46-77"><a href="#cb46-77" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb46-78"><a href="#cb46-78" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb46-79"><a href="#cb46-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-80"><a href="#cb46-80" aria-hidden="true" tabindex="-1"></a>cat_model.fit(</span>
<span id="cb46-81"><a href="#cb46-81" aria-hidden="true" tabindex="-1"></a>    X_train_cat_sal,</span>
<span id="cb46-82"><a href="#cb46-82" aria-hidden="true" tabindex="-1"></a>    y_train_sal,</span>
<span id="cb46-83"><a href="#cb46-83" aria-hidden="true" tabindex="-1"></a>    cat_features<span class="op">=</span>cat_features_list</span>
<span id="cb46-84"><a href="#cb46-84" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-85"><a href="#cb46-85" aria-hidden="true" tabindex="-1"></a>time_cat <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb46-86"><a href="#cb46-86" aria-hidden="true" tabindex="-1"></a>score_cat <span class="op">=</span> cat_model.score(X_test_cat_sal, y_test_sal)</span>
<span id="cb46-87"><a href="#cb46-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-88"><a href="#cb46-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar comparación</span></span>
<span id="cb46-89"><a href="#cb46-89" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb46-90"><a href="#cb46-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-91"><a href="#cb46-91" aria-hidden="true" tabindex="-1"></a>methods <span class="op">=</span> [<span class="st">'XGBoost</span><span class="ch">\n</span><span class="st">(One-hot)'</span>, <span class="st">'CatBoost</span><span class="ch">\n</span><span class="st">(Native)'</span>]</span>
<span id="cb46-92"><a href="#cb46-92" aria-hidden="true" tabindex="-1"></a>times <span class="op">=</span> [time_xgb_ohe, time_cat]</span>
<span id="cb46-93"><a href="#cb46-93" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> [score_xgb_ohe, score_cat]</span>
<span id="cb46-94"><a href="#cb46-94" aria-hidden="true" tabindex="-1"></a>n_features <span class="op">=</span> [X_train_ohe.shape[<span class="dv">1</span>], X_train_cat_sal.shape[<span class="dv">1</span>]]</span>
<span id="cb46-95"><a href="#cb46-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-96"><a href="#cb46-96" aria-hidden="true" tabindex="-1"></a><span class="co"># Tiempo</span></span>
<span id="cb46-97"><a href="#cb46-97" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].bar(methods, times, color<span class="op">=</span>[<span class="st">'#e74c3c'</span>, <span class="st">'#f39c12'</span>])</span>
<span id="cb46-98"><a href="#cb46-98" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Tiempo (segundos)'</span>)</span>
<span id="cb46-99"><a href="#cb46-99" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Tiempo de Entrenamiento'</span>)</span>
<span id="cb46-100"><a href="#cb46-100" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(times):</span>
<span id="cb46-101"><a href="#cb46-101" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].text(i, t <span class="op">+</span> <span class="fl">0.01</span>, <span class="ss">f'</span><span class="sc">{</span>t<span class="sc">:.3f}</span><span class="ss">s'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb46-102"><a href="#cb46-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-103"><a href="#cb46-103" aria-hidden="true" tabindex="-1"></a><span class="co"># Score</span></span>
<span id="cb46-104"><a href="#cb46-104" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].bar(methods, scores, color<span class="op">=</span>[<span class="st">'#e74c3c'</span>, <span class="st">'#f39c12'</span>])</span>
<span id="cb46-105"><a href="#cb46-105" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'R² Score'</span>)</span>
<span id="cb46-106"><a href="#cb46-106" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Precisión del Modelo'</span>)</span>
<span id="cb46-107"><a href="#cb46-107" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(scores):</span>
<span id="cb46-108"><a href="#cb46-108" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].text(i, s <span class="op">-</span> <span class="fl">0.02</span>, <span class="ss">f'</span><span class="sc">{</span>s<span class="sc">:.4f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb46-109"><a href="#cb46-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-110"><a href="#cb46-110" aria-hidden="true" tabindex="-1"></a><span class="co"># Número de features</span></span>
<span id="cb46-111"><a href="#cb46-111" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].bar(methods, n_features, color<span class="op">=</span>[<span class="st">'#e74c3c'</span>, <span class="st">'#f39c12'</span>])</span>
<span id="cb46-112"><a href="#cb46-112" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">'Número de Features'</span>)</span>
<span id="cb46-113"><a href="#cb46-113" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">'Dimensionalidad'</span>)</span>
<span id="cb46-114"><a href="#cb46-114" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, n <span class="kw">in</span> <span class="bu">enumerate</span>(n_features):</span>
<span id="cb46-115"><a href="#cb46-115" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">2</span>].text(i, n <span class="op">+</span> <span class="fl">0.5</span>, <span class="ss">f'</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb46-116"><a href="#cb46-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-117"><a href="#cb46-117" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb46-118"><a href="#cb46-118" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb46-119"><a href="#cb46-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-120"><a href="#cb46-120" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Comparación:"</span>)</span>
<span id="cb46-121"><a href="#cb46-121" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"XGBoost (one-hot): </span><span class="sc">{</span>n_features[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> features, R² = </span><span class="sc">{</span>score_xgb_ohe<span class="sc">:.4f}</span><span class="ss">, </span><span class="sc">{</span>time_xgb_ohe<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb46-122"><a href="#cb46-122" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CatBoost (native): </span><span class="sc">{</span>n_features[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> features, R² = </span><span class="sc">{</span>score_cat<span class="sc">:.4f}</span><span class="ss">, </span><span class="sc">{</span>time_cat<span class="sc">:.3f}</span><span class="ss">s"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="catboost-categorical" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/catboost-categorical-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Manejo automático de features categóricas en CatBoost</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Comparación:
XGBoost (one-hot): 17 features, R² = 0.8570, 0.128s
CatBoost (native): 5 features, R² = 0.8796, 0.123s</code></pre>
</div>
</div>
<p>Ahora veamos feature importance y cómo CatBoost identifica la importancia de categorías:</p>
<div id="cell-catboost-importance" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature importance</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>feature_importance <span class="op">=</span> cat_model.get_feature_importance()</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> X_train_cat_sal.columns</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>imp_df_cat <span class="op">=</span> pd.DataFrame({</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'feature'</span>: feature_names,</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'importance'</span>: feature_importance</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>}).sort_values(<span class="st">'importance'</span>, ascending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>colors_imp <span class="op">=</span> [<span class="st">'#f39c12'</span> <span class="cf">if</span> f <span class="kw">in</span> cat_features_list <span class="cf">else</span> <span class="st">'#3498db'</span></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>              <span class="cf">for</span> f <span class="kw">in</span> imp_df_cat[<span class="st">'feature'</span>]]</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>plt.barh(imp_df_cat[<span class="st">'feature'</span>], imp_df_cat[<span class="st">'importance'</span>], color<span class="op">=</span>colors_imp)</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Importance'</span>)</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Feature Importance en CatBoost</span><span class="ch">\n</span><span class="st">Naranja = categórica, Azul = numérica'</span>)</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>plt.grid(axis<span class="op">=</span><span class="st">'x'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Feature Importances:"</span>)</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feat, imp <span class="kw">in</span> <span class="bu">zip</span>(imp_df_cat[<span class="st">'feature'</span>], imp_df_cat[<span class="st">'importance'</span>]):</span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>    feat_type <span class="op">=</span> <span class="st">'categórica'</span> <span class="cf">if</span> feat <span class="kw">in</span> cat_features_list <span class="cf">else</span> <span class="st">'numérica'</span></span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feat<span class="sc">:20s}</span><span class="ss"> (</span><span class="sc">{</span>feat_type<span class="sc">:11s}</span><span class="ss">): </span><span class="sc">{</span>imp<span class="sc">:8.2f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="catboost-importance" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/catboost-importance-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Feature importance en CatBoost</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Feature Importances:
  age                  (numérica   ):     7.60
  location             (categórica ):    11.32
  years_experience     (numérica   ):    20.25
  department           (categórica ):    28.19
  education            (categórica ):    32.65</code></pre>
</div>
</div>
<p>Demostremos la robustez de CatBoost con parámetros por defecto:</p>
<div id="cell-catboost-defaults" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Usar California Housing para comparación</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>housing_data <span class="op">=</span> fetch_california_housing()</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>X_house <span class="op">=</span> pd.DataFrame(housing_data.data, columns<span class="op">=</span>housing_data.feature_names)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>y_house <span class="op">=</span> housing_data.target</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>X_train_house, X_test_house, y_train_house, y_test_house <span class="op">=</span> train_test_split(</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    X_house, y_house, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="co"># CatBoost con parámetros por defecto (casi)</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Entrenando CatBoost (defaults)..."</span>)</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>cat_default <span class="op">=</span> CatBoostRegressor(</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>    iterations<span class="op">=</span><span class="dv">200</span>,  <span class="co"># Solo especificamos esto</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>cat_default.fit(X_train_house, y_train_house)</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>time_cat_default <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>score_cat_default <span class="op">=</span> cat_default.score(X_test_house, y_test_house)</span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a><span class="co"># XGBoost con parámetros "tuneados"</span></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Entrenando XGBoost (tuned)..."</span>)</span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>xgb_tuned <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>    subsample<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>    colsample_bytree<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>    reg_alpha<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>    reg_lambda<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>    verbosity<span class="op">=</span><span class="dv">0</span></span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>xgb_tuned.fit(X_train_house, y_train_house)</span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>time_xgb_tuned <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>score_xgb_tuned <span class="op">=</span> xgb_tuned.score(X_test_house, y_test_house)</span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a><span class="co"># LightGBM con parámetros "tuneados"</span></span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Entrenando LightGBM (tuned)..."</span>)</span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a>lgb_tuned <span class="op">=</span> lgb.LGBMRegressor(</span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>    num_leaves<span class="op">=</span><span class="dv">31</span>,</span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a>    min_data_in_leaf<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb50-48"><a href="#cb50-48" aria-hidden="true" tabindex="-1"></a>    feature_fraction<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb50-49"><a href="#cb50-49" aria-hidden="true" tabindex="-1"></a>    bagging_fraction<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb50-50"><a href="#cb50-50" aria-hidden="true" tabindex="-1"></a>    bagging_freq<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb50-51"><a href="#cb50-51" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb50-52"><a href="#cb50-52" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb50-53"><a href="#cb50-53" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb50-54"><a href="#cb50-54" aria-hidden="true" tabindex="-1"></a>lgb_tuned.fit(X_train_house, y_train_house)</span>
<span id="cb50-55"><a href="#cb50-55" aria-hidden="true" tabindex="-1"></a>time_lgb_tuned <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb50-56"><a href="#cb50-56" aria-hidden="true" tabindex="-1"></a>score_lgb_tuned <span class="op">=</span> lgb_tuned.score(X_test_house, y_test_house)</span>
<span id="cb50-57"><a href="#cb50-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-58"><a href="#cb50-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Comparar</span></span>
<span id="cb50-59"><a href="#cb50-59" aria-hidden="true" tabindex="-1"></a>models_comp <span class="op">=</span> [<span class="st">'CatBoost</span><span class="ch">\n</span><span class="st">(default)'</span>, <span class="st">'XGBoost</span><span class="ch">\n</span><span class="st">(tuned)'</span>, <span class="st">'LightGBM</span><span class="ch">\n</span><span class="st">(tuned)'</span>]</span>
<span id="cb50-60"><a href="#cb50-60" aria-hidden="true" tabindex="-1"></a>times_comp <span class="op">=</span> [time_cat_default, time_xgb_tuned, time_lgb_tuned]</span>
<span id="cb50-61"><a href="#cb50-61" aria-hidden="true" tabindex="-1"></a>scores_comp <span class="op">=</span> [score_cat_default, score_xgb_tuned, score_lgb_tuned]</span>
<span id="cb50-62"><a href="#cb50-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-63"><a href="#cb50-63" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb50-64"><a href="#cb50-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-65"><a href="#cb50-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Scores</span></span>
<span id="cb50-66"><a href="#cb50-66" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].bar(models_comp, scores_comp, color<span class="op">=</span>[<span class="st">'#f39c12'</span>, <span class="st">'#e74c3c'</span>, <span class="st">'#16a085'</span>])</span>
<span id="cb50-67"><a href="#cb50-67" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'R² Score'</span>)</span>
<span id="cb50-68"><a href="#cb50-68" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Precisión del Modelo'</span>)</span>
<span id="cb50-69"><a href="#cb50-69" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylim([<span class="fl">0.75</span>, <span class="fl">0.85</span>])</span>
<span id="cb50-70"><a href="#cb50-70" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(scores_comp):</span>
<span id="cb50-71"><a href="#cb50-71" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].text(i, s <span class="op">+</span> <span class="fl">0.005</span>, <span class="ss">f'</span><span class="sc">{</span>s<span class="sc">:.4f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb50-72"><a href="#cb50-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-73"><a href="#cb50-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Times</span></span>
<span id="cb50-74"><a href="#cb50-74" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].bar(models_comp, times_comp, color<span class="op">=</span>[<span class="st">'#f39c12'</span>, <span class="st">'#e74c3c'</span>, <span class="st">'#16a085'</span>])</span>
<span id="cb50-75"><a href="#cb50-75" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Tiempo (segundos)'</span>)</span>
<span id="cb50-76"><a href="#cb50-76" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Tiempo de Entrenamiento'</span>)</span>
<span id="cb50-77"><a href="#cb50-77" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(times_comp):</span>
<span id="cb50-78"><a href="#cb50-78" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].text(i, t <span class="op">+</span> <span class="fl">0.05</span>, <span class="ss">f'</span><span class="sc">{</span>t<span class="sc">:.2f}</span><span class="ss">s'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb50-79"><a href="#cb50-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-80"><a href="#cb50-80" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb50-81"><a href="#cb50-81" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb50-82"><a href="#cb50-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-83"><a href="#cb50-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Resultados:"</span>)</span>
<span id="cb50-84"><a href="#cb50-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CatBoost (default):  R² = </span><span class="sc">{</span>score_cat_default<span class="sc">:.4f}</span><span class="ss">, </span><span class="sc">{</span>time_cat_default<span class="sc">:.2f}</span><span class="ss">s"</span>)</span>
<span id="cb50-85"><a href="#cb50-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"XGBoost (tuned):     R² = </span><span class="sc">{</span>score_xgb_tuned<span class="sc">:.4f}</span><span class="ss">, </span><span class="sc">{</span>time_xgb_tuned<span class="sc">:.2f}</span><span class="ss">s"</span>)</span>
<span id="cb50-86"><a href="#cb50-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LightGBM (tuned):    R² = </span><span class="sc">{</span>score_lgb_tuned<span class="sc">:.4f}</span><span class="ss">, </span><span class="sc">{</span>time_lgb_tuned<span class="sc">:.2f}</span><span class="ss">s"</span>)</span>
<span id="cb50-87"><a href="#cb50-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Observa que CatBoost es competitivo sin tuning!"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Entrenando CatBoost (defaults)...
Entrenando XGBoost (tuned)...
Entrenando LightGBM (tuned)...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="catboost-defaults" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/catboost-defaults-output-2.png" class="img-fluid figure-img"></p>
<figcaption>CatBoost con defaults vs otros modelos con tuning</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Resultados:
CatBoost (default):  R² = 0.8373, 0.23s
XGBoost (tuned):     R² = 0.8437, 0.26s
LightGBM (tuned):    R² = 0.8489, 0.67s

Observa que CatBoost es competitivo sin tuning!</code></pre>
</div>
</div>
<p>Visualicemos learning curves con early stopping en CatBoost:</p>
<div id="cell-catboost-learning-curves" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear validation set</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>X_train_sub_house, X_val_house, y_train_sub_house, y_val_house <span class="op">=</span> train_test_split(</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    X_train_house, y_train_house, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar con eval_set</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>cat_eval <span class="op">=</span> CatBoostRegressor(</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    iterations<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>cat_eval.fit(</span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>    X_train_sub_house,</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>    y_train_sub_house,</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>    eval_set<span class="op">=</span>(X_val_house, y_val_house),</span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>    early_stopping_rounds<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener métricas</span></span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>train_rmse <span class="op">=</span> cat_eval.evals_result_[<span class="st">'learn'</span>][<span class="st">'RMSE'</span>]</span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>val_rmse <span class="op">=</span> cat_eval.evals_result_[<span class="st">'validation'</span>][<span class="st">'RMSE'</span>]</span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar</span></span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>plt.plot(train_rmse, label<span class="op">=</span><span class="st">'Training RMSE'</span>, color<span class="op">=</span><span class="st">'#3498db'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>plt.plot(val_rmse, label<span class="op">=</span><span class="st">'Validation RMSE'</span>, color<span class="op">=</span><span class="st">'#e74c3c'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>plt.axvline(</span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>cat_eval.best_iteration_,</span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">'#2ecc71'</span>,</span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>    linestyle<span class="op">=</span><span class="st">'--'</span>,</span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>    linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="ss">f'Best iteration (</span><span class="sc">{</span>cat_eval<span class="sc">.</span>best_iteration_<span class="sc">}</span><span class="ss">)'</span></span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Iteración'</span>)</span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'RMSE'</span>)</span>
<span id="cb53-39"><a href="#cb53-39" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Learning Curves en CatBoost con Early Stopping'</span>)</span>
<span id="cb53-40"><a href="#cb53-40" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb53-41"><a href="#cb53-41" aria-hidden="true" tabindex="-1"></a>plt.grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb53-42"><a href="#cb53-42" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb53-43"><a href="#cb53-43" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb53-44"><a href="#cb53-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-45"><a href="#cb53-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mejor iteración: </span><span class="sc">{</span>cat_eval<span class="sc">.</span>best_iteration_<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb53-46"><a href="#cb53-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mejor RMSE de validación: </span><span class="sc">{</span>cat_eval<span class="sc">.</span>best_score_[<span class="st">'validation'</span>][<span class="st">'RMSE'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb53-47"><a href="#cb53-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Iteraciones ahorradas: </span><span class="sc">{</span><span class="dv">1000</span> <span class="op">-</span> cat_eval<span class="sc">.</span>best_iteration_<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="catboost-learning-curves" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/catboost-learning-curves-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Learning curves con early stopping en CatBoost</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Mejor iteración: 989
Mejor RMSE de validación: 0.4691
Iteraciones ahorradas: 11</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cuándo usar CatBoost
</div>
</div>
<div class="callout-body-container callout-body">
<p>Usa CatBoost cuando: - Tienes muchas features categóricas (especialmente high-cardinality) - No tienes tiempo para tuning extensivo de hiperparámetros - Necesitas un modelo robusto “out of the box” - Priorizas estabilidad y reproducibilidad - Vas a deployar a producción (árboles oblivious son muy eficientes) - Trabajas con datos donde el orden/tiempo importa (ordered boosting ayuda)</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ventajas de CatBoost para producción
</div>
</div>
<div class="callout-body-container callout-body">
<p>CatBoost es particularmente bueno para sistemas de producción:</p>
<ol type="1">
<li><strong>Menos propenso a overfitting</strong>: Ordered boosting reduce el prediction shift</li>
<li><strong>Predicción rápida</strong>: Árboles oblivious permiten optimizaciones agresivas</li>
<li><strong>Manejo robusto de datos</strong>: Categorical features sin preprocesamiento</li>
<li><strong>Pocos hiperparámetros críticos</strong>: Menos cosas pueden salir mal</li>
<li><strong>Modelo más estable</strong>: Menos sensible a cambios en datos de entrada</li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Instalación de CatBoost
</div>
</div>
<div class="callout-body-container callout-body">
<p>Para instalar CatBoost:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install catboost</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>CatBoost tiene soporte de GPU integrado, se activa automáticamente si está disponible o puedes especificar <code>task_type='GPU'</code>.</p>
</div>
</div>
</section>
</section>
<section id="sec-comparative-analysis" class="level3">
<h3 class="anchored" data-anchor-id="sec-comparative-analysis">5.4 Análisis Comparativo</h3>
<p>Ahora que hemos explorado XGBoost, LightGBM y CatBoost en detalle, realicemos un análisis comparativo comprehensivo para entender cuándo usar cada uno. Cada implementación tiene sus fortalezas y casos de uso ideales.</p>
<section id="tabla-comparativa-de-características" class="level4">
<h4 class="anchored" data-anchor-id="tabla-comparativa-de-características">Tabla comparativa de características</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 21%">
<col style="width: 15%">
<col style="width: 17%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Característica</th>
<th>sklearn GB</th>
<th>XGBoost</th>
<th>LightGBM</th>
<th>CatBoost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Velocidad de entrenamiento</strong></td>
<td>Lento (1x)</td>
<td>Rápido (5-10x)</td>
<td>Muy rápido (10-15x)</td>
<td>Rápido (3-7x)</td>
</tr>
<tr class="even">
<td><strong>Velocidad de predicción</strong></td>
<td>Media</td>
<td>Rápida</td>
<td>Muy rápida</td>
<td>Muy rápida</td>
</tr>
<tr class="odd">
<td><strong>Uso de memoria</strong></td>
<td>Alto</td>
<td>Medio</td>
<td>Bajo</td>
<td>Medio</td>
</tr>
<tr class="even">
<td><strong>Precisión (accuracy)</strong></td>
<td>Buena</td>
<td>Excelente</td>
<td>Excelente</td>
<td>Excelente</td>
</tr>
<tr class="odd">
<td><strong>Categorical features nativas</strong></td>
<td>No</td>
<td>No*</td>
<td>Sí (básico)</td>
<td>Sí (avanzado)</td>
</tr>
<tr class="even">
<td><strong>Missing values handling</strong></td>
<td>No</td>
<td>Sí</td>
<td>Sí</td>
<td>Sí</td>
</tr>
<tr class="odd">
<td><strong>Soporte GPU</strong></td>
<td>No</td>
<td>Sí</td>
<td>Sí</td>
<td>Sí</td>
</tr>
<tr class="even">
<td><strong>Regularización</strong></td>
<td>Básica</td>
<td>Avanzada (L1+L2)</td>
<td>Media (L1+L2)</td>
<td>Muy avanzada</td>
</tr>
<tr class="odd">
<td><strong>Hiperparámetros</strong></td>
<td>Pocos</td>
<td>Muchos</td>
<td>Muchos</td>
<td>Medios</td>
</tr>
<tr class="even">
<td><strong>Facilidad de uso</strong></td>
<td>Fácil</td>
<td>Media</td>
<td>Media</td>
<td>Fácil</td>
</tr>
<tr class="odd">
<td><strong>Documentación</strong></td>
<td>Excelente</td>
<td>Muy buena</td>
<td>Buena</td>
<td>Muy buena</td>
</tr>
<tr class="even">
<td><strong>Comunidad</strong></td>
<td>Grande</td>
<td>Muy grande</td>
<td>Grande</td>
<td>Media-grande</td>
</tr>
<tr class="odd">
<td><strong>Tuning requerido</strong></td>
<td>Bajo</td>
<td>Alto</td>
<td>Alto</td>
<td>Bajo</td>
</tr>
<tr class="even">
<td><strong>Estabilidad</strong></td>
<td>Muy alta</td>
<td>Alta</td>
<td>Alta</td>
<td>Muy alta</td>
</tr>
</tbody>
</table>
<p>*XGBoost puede manejar categorías via one-hot encoding o con enable_categorical (experimental)</p>
</section>
<section id="benchmark-comprehensivo" class="level4">
<h4 class="anchored" data-anchor-id="benchmark-comprehensivo">Benchmark comprehensivo</h4>
<p>Realicemos un benchmark completo comparando las cuatro implementaciones en el mismo dataset:</p>
<div id="cell-benchmark-comprehensive" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score, mean_absolute_error</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Usar California Housing para benchmark</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>X_bench, y_bench <span class="op">=</span> X_house, y_house</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>X_train_bench, X_test_bench, y_train_bench, y_test_bench <span class="op">=</span> train_test_split(</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    X_bench, y_bench, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Configuración común</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>n_est <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>depth <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. sklearn GradientBoosting</span></span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Benchmarking sklearn GradientBoosting..."</span>)</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>gb_sk <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span>n_est,</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span>lr,</span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span>depth,</span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a>gb_sk.fit(X_train_bench, y_train_bench)</span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>time_train_sk <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a>y_pred_sk <span class="op">=</span> gb_sk.predict(X_test_bench)</span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a>time_pred_sk <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb56-33"><a href="#cb56-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-34"><a href="#cb56-34" aria-hidden="true" tabindex="-1"></a>results[<span class="st">'sklearn'</span>] <span class="op">=</span> {</span>
<span id="cb56-35"><a href="#cb56-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">'train_time'</span>: time_train_sk,</span>
<span id="cb56-36"><a href="#cb56-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">'pred_time'</span>: time_pred_sk,</span>
<span id="cb56-37"><a href="#cb56-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">'rmse'</span>: np.sqrt(mean_squared_error(y_test_bench, y_pred_sk)),</span>
<span id="cb56-38"><a href="#cb56-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">'mae'</span>: mean_absolute_error(y_test_bench, y_pred_sk),</span>
<span id="cb56-39"><a href="#cb56-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'r2'</span>: r2_score(y_test_bench, y_pred_sk)</span>
<span id="cb56-40"><a href="#cb56-40" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb56-41"><a href="#cb56-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-42"><a href="#cb56-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. XGBoost</span></span>
<span id="cb56-43"><a href="#cb56-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Benchmarking XGBoost..."</span>)</span>
<span id="cb56-44"><a href="#cb56-44" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb56-45"><a href="#cb56-45" aria-hidden="true" tabindex="-1"></a>xgb_bench <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb56-46"><a href="#cb56-46" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span>n_est,</span>
<span id="cb56-47"><a href="#cb56-47" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span>lr,</span>
<span id="cb56-48"><a href="#cb56-48" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span>depth,</span>
<span id="cb56-49"><a href="#cb56-49" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb56-50"><a href="#cb56-50" aria-hidden="true" tabindex="-1"></a>    verbosity<span class="op">=</span><span class="dv">0</span></span>
<span id="cb56-51"><a href="#cb56-51" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-52"><a href="#cb56-52" aria-hidden="true" tabindex="-1"></a>xgb_bench.fit(X_train_bench, y_train_bench)</span>
<span id="cb56-53"><a href="#cb56-53" aria-hidden="true" tabindex="-1"></a>time_train_xgb <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb56-54"><a href="#cb56-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-55"><a href="#cb56-55" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb56-56"><a href="#cb56-56" aria-hidden="true" tabindex="-1"></a>y_pred_xgb <span class="op">=</span> xgb_bench.predict(X_test_bench)</span>
<span id="cb56-57"><a href="#cb56-57" aria-hidden="true" tabindex="-1"></a>time_pred_xgb <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb56-58"><a href="#cb56-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-59"><a href="#cb56-59" aria-hidden="true" tabindex="-1"></a>results[<span class="st">'XGBoost'</span>] <span class="op">=</span> {</span>
<span id="cb56-60"><a href="#cb56-60" aria-hidden="true" tabindex="-1"></a>    <span class="st">'train_time'</span>: time_train_xgb,</span>
<span id="cb56-61"><a href="#cb56-61" aria-hidden="true" tabindex="-1"></a>    <span class="st">'pred_time'</span>: time_pred_xgb,</span>
<span id="cb56-62"><a href="#cb56-62" aria-hidden="true" tabindex="-1"></a>    <span class="st">'rmse'</span>: np.sqrt(mean_squared_error(y_test_bench, y_pred_xgb)),</span>
<span id="cb56-63"><a href="#cb56-63" aria-hidden="true" tabindex="-1"></a>    <span class="st">'mae'</span>: mean_absolute_error(y_test_bench, y_pred_xgb),</span>
<span id="cb56-64"><a href="#cb56-64" aria-hidden="true" tabindex="-1"></a>    <span class="st">'r2'</span>: r2_score(y_test_bench, y_pred_xgb)</span>
<span id="cb56-65"><a href="#cb56-65" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb56-66"><a href="#cb56-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-67"><a href="#cb56-67" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. LightGBM</span></span>
<span id="cb56-68"><a href="#cb56-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Benchmarking LightGBM..."</span>)</span>
<span id="cb56-69"><a href="#cb56-69" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb56-70"><a href="#cb56-70" aria-hidden="true" tabindex="-1"></a>lgb_bench <span class="op">=</span> lgb.LGBMRegressor(</span>
<span id="cb56-71"><a href="#cb56-71" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span>n_est,</span>
<span id="cb56-72"><a href="#cb56-72" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span>lr,</span>
<span id="cb56-73"><a href="#cb56-73" aria-hidden="true" tabindex="-1"></a>    num_leaves<span class="op">=</span><span class="dv">2</span><span class="op">**</span>depth <span class="op">-</span> <span class="dv">1</span>,  <span class="co"># Aproximadamente equivalente</span></span>
<span id="cb56-74"><a href="#cb56-74" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb56-75"><a href="#cb56-75" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb56-76"><a href="#cb56-76" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-77"><a href="#cb56-77" aria-hidden="true" tabindex="-1"></a>lgb_bench.fit(X_train_bench, y_train_bench)</span>
<span id="cb56-78"><a href="#cb56-78" aria-hidden="true" tabindex="-1"></a>time_train_lgb <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb56-79"><a href="#cb56-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-80"><a href="#cb56-80" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb56-81"><a href="#cb56-81" aria-hidden="true" tabindex="-1"></a>y_pred_lgb <span class="op">=</span> lgb_bench.predict(X_test_bench)</span>
<span id="cb56-82"><a href="#cb56-82" aria-hidden="true" tabindex="-1"></a>time_pred_lgb <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb56-83"><a href="#cb56-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-84"><a href="#cb56-84" aria-hidden="true" tabindex="-1"></a>results[<span class="st">'LightGBM'</span>] <span class="op">=</span> {</span>
<span id="cb56-85"><a href="#cb56-85" aria-hidden="true" tabindex="-1"></a>    <span class="st">'train_time'</span>: time_train_lgb,</span>
<span id="cb56-86"><a href="#cb56-86" aria-hidden="true" tabindex="-1"></a>    <span class="st">'pred_time'</span>: time_pred_lgb,</span>
<span id="cb56-87"><a href="#cb56-87" aria-hidden="true" tabindex="-1"></a>    <span class="st">'rmse'</span>: np.sqrt(mean_squared_error(y_test_bench, y_pred_lgb)),</span>
<span id="cb56-88"><a href="#cb56-88" aria-hidden="true" tabindex="-1"></a>    <span class="st">'mae'</span>: mean_absolute_error(y_test_bench, y_pred_lgb),</span>
<span id="cb56-89"><a href="#cb56-89" aria-hidden="true" tabindex="-1"></a>    <span class="st">'r2'</span>: r2_score(y_test_bench, y_pred_lgb)</span>
<span id="cb56-90"><a href="#cb56-90" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb56-91"><a href="#cb56-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-92"><a href="#cb56-92" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. CatBoost</span></span>
<span id="cb56-93"><a href="#cb56-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Benchmarking CatBoost..."</span>)</span>
<span id="cb56-94"><a href="#cb56-94" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb56-95"><a href="#cb56-95" aria-hidden="true" tabindex="-1"></a>cat_bench <span class="op">=</span> CatBoostRegressor(</span>
<span id="cb56-96"><a href="#cb56-96" aria-hidden="true" tabindex="-1"></a>    iterations<span class="op">=</span>n_est,</span>
<span id="cb56-97"><a href="#cb56-97" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span>lr,</span>
<span id="cb56-98"><a href="#cb56-98" aria-hidden="true" tabindex="-1"></a>    depth<span class="op">=</span>depth,</span>
<span id="cb56-99"><a href="#cb56-99" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb56-100"><a href="#cb56-100" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb56-101"><a href="#cb56-101" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-102"><a href="#cb56-102" aria-hidden="true" tabindex="-1"></a>cat_bench.fit(X_train_bench, y_train_bench)</span>
<span id="cb56-103"><a href="#cb56-103" aria-hidden="true" tabindex="-1"></a>time_train_cat <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb56-104"><a href="#cb56-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-105"><a href="#cb56-105" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb56-106"><a href="#cb56-106" aria-hidden="true" tabindex="-1"></a>y_pred_cat <span class="op">=</span> cat_bench.predict(X_test_bench)</span>
<span id="cb56-107"><a href="#cb56-107" aria-hidden="true" tabindex="-1"></a>time_pred_cat <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb56-108"><a href="#cb56-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-109"><a href="#cb56-109" aria-hidden="true" tabindex="-1"></a>results[<span class="st">'CatBoost'</span>] <span class="op">=</span> {</span>
<span id="cb56-110"><a href="#cb56-110" aria-hidden="true" tabindex="-1"></a>    <span class="st">'train_time'</span>: time_train_cat,</span>
<span id="cb56-111"><a href="#cb56-111" aria-hidden="true" tabindex="-1"></a>    <span class="st">'pred_time'</span>: time_pred_cat,</span>
<span id="cb56-112"><a href="#cb56-112" aria-hidden="true" tabindex="-1"></a>    <span class="st">'rmse'</span>: np.sqrt(mean_squared_error(y_test_bench, y_pred_cat)),</span>
<span id="cb56-113"><a href="#cb56-113" aria-hidden="true" tabindex="-1"></a>    <span class="st">'mae'</span>: mean_absolute_error(y_test_bench, y_pred_cat),</span>
<span id="cb56-114"><a href="#cb56-114" aria-hidden="true" tabindex="-1"></a>    <span class="st">'r2'</span>: r2_score(y_test_bench, y_pred_cat)</span>
<span id="cb56-115"><a href="#cb56-115" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb56-116"><a href="#cb56-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-117"><a href="#cb56-117" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar resultados</span></span>
<span id="cb56-118"><a href="#cb56-118" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb56-119"><a href="#cb56-119" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> <span class="bu">list</span>(results.keys())</span>
<span id="cb56-120"><a href="#cb56-120" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'#3498db'</span>, <span class="st">'#e74c3c'</span>, <span class="st">'#16a085'</span>, <span class="st">'#f39c12'</span>]</span>
<span id="cb56-121"><a href="#cb56-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-122"><a href="#cb56-122" aria-hidden="true" tabindex="-1"></a><span class="co"># Tiempo de entrenamiento</span></span>
<span id="cb56-123"><a href="#cb56-123" aria-hidden="true" tabindex="-1"></a>train_times <span class="op">=</span> [results[m][<span class="st">'train_time'</span>] <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb56-124"><a href="#cb56-124" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].bar(models, train_times, color<span class="op">=</span>colors)</span>
<span id="cb56-125"><a href="#cb56-125" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'Segundos'</span>)</span>
<span id="cb56-126"><a href="#cb56-126" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'Tiempo de Entrenamiento'</span>)</span>
<span id="cb56-127"><a href="#cb56-127" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(train_times):</span>
<span id="cb56-128"><a href="#cb56-128" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">0</span>].text(i, t <span class="op">+</span> <span class="fl">0.1</span>, <span class="ss">f'</span><span class="sc">{</span>t<span class="sc">:.2f}</span><span class="ss">s'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb56-129"><a href="#cb56-129" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb56-130"><a href="#cb56-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-131"><a href="#cb56-131" aria-hidden="true" tabindex="-1"></a><span class="co"># Tiempo de predicción (ms)</span></span>
<span id="cb56-132"><a href="#cb56-132" aria-hidden="true" tabindex="-1"></a>pred_times <span class="op">=</span> [results[m][<span class="st">'pred_time'</span>] <span class="op">*</span> <span class="dv">1000</span> <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb56-133"><a href="#cb56-133" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].bar(models, pred_times, color<span class="op">=</span>colors)</span>
<span id="cb56-134"><a href="#cb56-134" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'Milisegundos'</span>)</span>
<span id="cb56-135"><a href="#cb56-135" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'Tiempo de Predicción'</span>)</span>
<span id="cb56-136"><a href="#cb56-136" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, t <span class="kw">in</span> <span class="bu">enumerate</span>(pred_times):</span>
<span id="cb56-137"><a href="#cb56-137" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>, <span class="dv">1</span>].text(i, t <span class="op">+</span> <span class="fl">0.5</span>, <span class="ss">f'</span><span class="sc">{</span>t<span class="sc">:.1f}</span><span class="ss">ms'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb56-138"><a href="#cb56-138" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb56-139"><a href="#cb56-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-140"><a href="#cb56-140" aria-hidden="true" tabindex="-1"></a><span class="co"># RMSE</span></span>
<span id="cb56-141"><a href="#cb56-141" aria-hidden="true" tabindex="-1"></a>rmses <span class="op">=</span> [results[m][<span class="st">'rmse'</span>] <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb56-142"><a href="#cb56-142" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].bar(models, rmses, color<span class="op">=</span>colors)</span>
<span id="cb56-143"><a href="#cb56-143" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'RMSE'</span>)</span>
<span id="cb56-144"><a href="#cb56-144" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">'Error (RMSE) - Menor es mejor'</span>)</span>
<span id="cb56-145"><a href="#cb56-145" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, r <span class="kw">in</span> <span class="bu">enumerate</span>(rmses):</span>
<span id="cb56-146"><a href="#cb56-146" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">0</span>].text(i, r <span class="op">+</span> <span class="fl">0.005</span>, <span class="ss">f'</span><span class="sc">{</span>r<span class="sc">:.4f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb56-147"><a href="#cb56-147" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb56-148"><a href="#cb56-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-149"><a href="#cb56-149" aria-hidden="true" tabindex="-1"></a><span class="co"># R² Score</span></span>
<span id="cb56-150"><a href="#cb56-150" aria-hidden="true" tabindex="-1"></a>r2s <span class="op">=</span> [results[m][<span class="st">'r2'</span>] <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb56-151"><a href="#cb56-151" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].bar(models, r2s, color<span class="op">=</span>colors)</span>
<span id="cb56-152"><a href="#cb56-152" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'R² Score'</span>)</span>
<span id="cb56-153"><a href="#cb56-153" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">'Precisión (R²) - Mayor es mejor'</span>)</span>
<span id="cb56-154"><a href="#cb56-154" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_ylim([<span class="fl">0.75</span>, <span class="fl">0.85</span>])</span>
<span id="cb56-155"><a href="#cb56-155" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, r <span class="kw">in</span> <span class="bu">enumerate</span>(r2s):</span>
<span id="cb56-156"><a href="#cb56-156" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>, <span class="dv">1</span>].text(i, r <span class="op">+</span> <span class="fl">0.003</span>, <span class="ss">f'</span><span class="sc">{</span>r<span class="sc">:.4f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb56-157"><a href="#cb56-157" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb56-158"><a href="#cb56-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-159"><a href="#cb56-159" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb56-160"><a href="#cb56-160" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-161"><a href="#cb56-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-162"><a href="#cb56-162" aria-hidden="true" tabindex="-1"></a><span class="co"># Tabla resumen</span></span>
<span id="cb56-163"><a href="#cb56-163" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">80</span>)</span>
<span id="cb56-164"><a href="#cb56-164" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"RESUMEN DE BENCHMARK"</span>)</span>
<span id="cb56-165"><a href="#cb56-165" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">80</span>)</span>
<span id="cb56-166"><a href="#cb56-166" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Modelo'</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Train (s)'</span><span class="sc">:&lt;12}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Pred (ms)'</span><span class="sc">:&lt;12}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'RMSE'</span><span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'MAE'</span><span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'R²'</span><span class="sc">:&lt;10}</span><span class="ss">"</span>)</span>
<span id="cb56-167"><a href="#cb56-167" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">80</span>)</span>
<span id="cb56-168"><a href="#cb56-168" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> model <span class="kw">in</span> models:</span>
<span id="cb56-169"><a href="#cb56-169" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> results[model]</span>
<span id="cb56-170"><a href="#cb56-170" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>model<span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>r[<span class="st">'train_time'</span>]<span class="sc">:&gt;10.2f}</span><span class="ss">  </span><span class="sc">{</span>r[<span class="st">'pred_time'</span>]<span class="op">*</span><span class="dv">1000</span><span class="sc">:&gt;10.1f}</span><span class="ss">  "</span></span>
<span id="cb56-171"><a href="#cb56-171" aria-hidden="true" tabindex="-1"></a>          <span class="ss">f"</span><span class="sc">{</span>r[<span class="st">'rmse'</span>]<span class="sc">:&gt;9.4f}</span><span class="ss">  </span><span class="sc">{</span>r[<span class="st">'mae'</span>]<span class="sc">:&gt;9.4f}</span><span class="ss">  </span><span class="sc">{</span>r[<span class="st">'r2'</span>]<span class="sc">:&gt;9.4f}</span><span class="ss">"</span>)</span>
<span id="cb56-172"><a href="#cb56-172" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">80</span>)</span>
<span id="cb56-173"><a href="#cb56-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-174"><a href="#cb56-174" aria-hidden="true" tabindex="-1"></a><span class="co"># Speedups relativos a sklearn</span></span>
<span id="cb56-175"><a href="#cb56-175" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Speedups relativos a sklearn GradientBoosting:"</span>)</span>
<span id="cb56-176"><a href="#cb56-176" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> model <span class="kw">in</span> models[<span class="dv">1</span>:]:  <span class="co"># Skip sklearn</span></span>
<span id="cb56-177"><a href="#cb56-177" aria-hidden="true" tabindex="-1"></a>    speedup <span class="op">=</span> results[<span class="st">'sklearn'</span>][<span class="st">'train_time'</span>] <span class="op">/</span> results[model][<span class="st">'train_time'</span>]</span>
<span id="cb56-178"><a href="#cb56-178" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>model<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>speedup<span class="sc">:.2f}</span><span class="ss">x más rápido"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Benchmarking sklearn GradientBoosting...
Benchmarking XGBoost...
Benchmarking LightGBM...
Benchmarking CatBoost...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="benchmark-comprehensive" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/benchmark-comprehensive-output-2.png" class="img-fluid figure-img"></p>
<figcaption>Benchmark comprehensivo de las cuatro implementaciones de boosting</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
================================================================================
RESUMEN DE BENCHMARK
================================================================================
Modelo          Train (s)    Pred (ms)    RMSE       MAE        R²        
--------------------------------------------------------------------------------
sklearn               8.61        13.4     0.4736     0.3144     0.8288
XGBoost               0.22         2.2     0.4742     0.3124     0.8284
LightGBM              0.63        10.6     0.4483     0.2941     0.8466
CatBoost              0.19         1.0     0.4905     0.3313     0.8164
--------------------------------------------------------------------------------

Speedups relativos a sklearn GradientBoosting:
  XGBoost: 38.96x más rápido
  LightGBM: 13.74x más rápido
  CatBoost: 45.78x más rápido</code></pre>
</div>
</div>
<p>Ahora veamos un análisis de acuerdo en predicciones entre los modelos:</p>
<div id="cell-prediction-agreement" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear DataFrame con todas las predicciones</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>predictions_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'True'</span>: y_test_bench,</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'sklearn'</span>: y_pred_sk,</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'XGBoost'</span>: y_pred_xgb,</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'LightGBM'</span>: y_pred_lgb,</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'CatBoost'</span>: y_pred_cat</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcular correlaciones entre predicciones</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>pred_corr <span class="op">=</span> predictions_df.corr()</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar matriz de correlación</span></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Heatmap de correlaciones</span></span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> LinearSegmentedColormap</span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> axes[<span class="dv">0</span>].imshow(pred_corr, cmap<span class="op">=</span><span class="st">'RdYlGn'</span>, vmin<span class="op">=</span><span class="fl">0.95</span>, vmax<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xticks(<span class="bu">range</span>(<span class="bu">len</span>(pred_corr.columns)))</span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yticks(<span class="bu">range</span>(<span class="bu">len</span>(pred_corr.columns)))</span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xticklabels(pred_corr.columns, rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_yticklabels(pred_corr.columns)</span>
<span id="cb59-25"><a href="#cb59-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Correlación entre Predicciones'</span>)</span>
<span id="cb59-26"><a href="#cb59-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-27"><a href="#cb59-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Anotar valores</span></span>
<span id="cb59-28"><a href="#cb59-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(pred_corr)):</span>
<span id="cb59-29"><a href="#cb59-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(pred_corr)):</span>
<span id="cb59-30"><a href="#cb59-30" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> axes[<span class="dv">0</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pred_corr<span class="sc">.</span>iloc[i, j]<span class="sc">:.4f}</span><span class="ss">'</span>,</span>
<span id="cb59-31"><a href="#cb59-31" aria-hidden="true" tabindex="-1"></a>                           ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"center"</span>, color<span class="op">=</span><span class="st">"black"</span>, fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb59-32"><a href="#cb59-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-33"><a href="#cb59-33" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im, ax<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb59-34"><a href="#cb59-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-35"><a href="#cb59-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatter plot comparando las implementaciones modernas</span></span>
<span id="cb59-36"><a href="#cb59-36" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].scatter(y_pred_xgb, y_pred_lgb, alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">20</span>, label<span class="op">=</span><span class="st">'XGB vs LGB'</span>, color<span class="op">=</span><span class="st">'#3498db'</span>)</span>
<span id="cb59-37"><a href="#cb59-37" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].scatter(y_pred_xgb, y_pred_cat, alpha<span class="op">=</span><span class="fl">0.3</span>, s<span class="op">=</span><span class="dv">20</span>, label<span class="op">=</span><span class="st">'XGB vs Cat'</span>, color<span class="op">=</span><span class="st">'#e74c3c'</span>)</span>
<span id="cb59-38"><a href="#cb59-38" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot([y_test_bench.<span class="bu">min</span>(), y_test_bench.<span class="bu">max</span>()],</span>
<span id="cb59-39"><a href="#cb59-39" aria-hidden="true" tabindex="-1"></a>             [y_test_bench.<span class="bu">min</span>(), y_test_bench.<span class="bu">max</span>()],</span>
<span id="cb59-40"><a href="#cb59-40" aria-hidden="true" tabindex="-1"></a>             <span class="st">'k--'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Acuerdo perfecto'</span>)</span>
<span id="cb59-41"><a href="#cb59-41" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'XGBoost Predictions'</span>)</span>
<span id="cb59-42"><a href="#cb59-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Other Model Predictions'</span>)</span>
<span id="cb59-43"><a href="#cb59-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Acuerdo entre Predicciones'</span>)</span>
<span id="cb59-44"><a href="#cb59-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend()</span>
<span id="cb59-45"><a href="#cb59-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb59-46"><a href="#cb59-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-47"><a href="#cb59-47" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb59-48"><a href="#cb59-48" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb59-49"><a href="#cb59-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-50"><a href="#cb59-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Nivel de acuerdo entre modelos (correlación):"</span>)</span>
<span id="cb59-51"><a href="#cb59-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Un valor cercano a 1.0 indica que los modelos hacen predicciones muy similares"</span>)</span>
<span id="cb59-52"><a href="#cb59-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Correlaciones con el valor verdadero:"</span>)</span>
<span id="cb59-53"><a href="#cb59-53" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> model <span class="kw">in</span> [<span class="st">'sklearn'</span>, <span class="st">'XGBoost'</span>, <span class="st">'LightGBM'</span>, <span class="st">'CatBoost'</span>]:</span>
<span id="cb59-54"><a href="#cb59-54" aria-hidden="true" tabindex="-1"></a>    corr <span class="op">=</span> pred_corr.loc[<span class="st">'True'</span>, model]</span>
<span id="cb59-55"><a href="#cb59-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>model<span class="sc">:12s}</span><span class="ss">: </span><span class="sc">{</span>corr<span class="sc">:.6f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="prediction-agreement" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/prediction-agreement-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Acuerdo entre predicciones de diferentes implementaciones</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Nivel de acuerdo entre modelos (correlación):
Un valor cercano a 1.0 indica que los modelos hacen predicciones muy similares

Correlaciones con el valor verdadero:
  sklearn     : 0.910423
  XGBoost     : 0.910177
  LightGBM    : 0.920144
  CatBoost    : 0.903648</code></pre>
</div>
</div>
<p>Finalmente, comparemos la estabilidad de feature importance:</p>
<div id="cell-importance-agreement" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtener feature importances de cada modelo</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>importances_dict <span class="op">=</span> {</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'XGBoost'</span>: xgb_bench.feature_importances_,</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'LightGBM'</span>: lgb_bench.feature_importances_,</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'CatBoost'</span>: cat_bench.feature_importances_</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalizar importances (suma = 1)</span></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> model <span class="kw">in</span> importances_dict:</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>    importances_dict[model] <span class="op">=</span> importances_dict[model] <span class="op">/</span> importances_dict[model].<span class="bu">sum</span>()</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear DataFrame</span></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>imp_comparison <span class="op">=</span> pd.DataFrame(importances_dict, index<span class="op">=</span>X_bench.columns)</span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizar</span></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Barras agrupadas</span></span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="bu">len</span>(imp_comparison))</span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].bar(x <span class="op">-</span> width, imp_comparison[<span class="st">'XGBoost'</span>], width,</span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a>           label<span class="op">=</span><span class="st">'XGBoost'</span>, color<span class="op">=</span><span class="st">'#e74c3c'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].bar(x, imp_comparison[<span class="st">'LightGBM'</span>], width,</span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true" tabindex="-1"></a>           label<span class="op">=</span><span class="st">'LightGBM'</span>, color<span class="op">=</span><span class="st">'#16a085'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].bar(x <span class="op">+</span> width, imp_comparison[<span class="st">'CatBoost'</span>], width,</span>
<span id="cb61-27"><a href="#cb61-27" aria-hidden="true" tabindex="-1"></a>           label<span class="op">=</span><span class="st">'CatBoost'</span>, color<span class="op">=</span><span class="st">'#f39c12'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb61-28"><a href="#cb61-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-29"><a href="#cb61-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Feature'</span>)</span>
<span id="cb61-30"><a href="#cb61-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Importance (normalizada)'</span>)</span>
<span id="cb61-31"><a href="#cb61-31" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Comparación de Feature Importance'</span>)</span>
<span id="cb61-32"><a href="#cb61-32" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xticks(x)</span>
<span id="cb61-33"><a href="#cb61-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xticklabels(imp_comparison.index, rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb61-34"><a href="#cb61-34" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend()</span>
<span id="cb61-35"><a href="#cb61-35" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb61-36"><a href="#cb61-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-37"><a href="#cb61-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlaciones entre importances</span></span>
<span id="cb61-38"><a href="#cb61-38" aria-hidden="true" tabindex="-1"></a>imp_corr <span class="op">=</span> imp_comparison.corr()</span>
<span id="cb61-39"><a href="#cb61-39" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> axes[<span class="dv">1</span>].imshow(imp_corr, cmap<span class="op">=</span><span class="st">'RdYlGn'</span>, vmin<span class="op">=</span><span class="fl">0.8</span>, vmax<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb61-40"><a href="#cb61-40" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xticks(<span class="bu">range</span>(<span class="bu">len</span>(imp_corr.columns)))</span>
<span id="cb61-41"><a href="#cb61-41" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_yticks(<span class="bu">range</span>(<span class="bu">len</span>(imp_corr.columns)))</span>
<span id="cb61-42"><a href="#cb61-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xticklabels(imp_corr.columns, rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb61-43"><a href="#cb61-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_yticklabels(imp_corr.columns)</span>
<span id="cb61-44"><a href="#cb61-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Correlación entre Feature Importances'</span>)</span>
<span id="cb61-45"><a href="#cb61-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-46"><a href="#cb61-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(imp_corr)):</span>
<span id="cb61-47"><a href="#cb61-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(imp_corr)):</span>
<span id="cb61-48"><a href="#cb61-48" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> axes[<span class="dv">1</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>imp_corr<span class="sc">.</span>iloc[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>,</span>
<span id="cb61-49"><a href="#cb61-49" aria-hidden="true" tabindex="-1"></a>                           ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"center"</span>, color<span class="op">=</span><span class="st">"black"</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb61-50"><a href="#cb61-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-51"><a href="#cb61-51" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im, ax<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb61-52"><a href="#cb61-52" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb61-53"><a href="#cb61-53" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb61-54"><a href="#cb61-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-55"><a href="#cb61-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top 3 features por modelo:"</span>)</span>
<span id="cb61-56"><a href="#cb61-56" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> model <span class="kw">in</span> [<span class="st">'XGBoost'</span>, <span class="st">'LightGBM'</span>, <span class="st">'CatBoost'</span>]:</span>
<span id="cb61-57"><a href="#cb61-57" aria-hidden="true" tabindex="-1"></a>    top_3 <span class="op">=</span> imp_comparison[model].nlargest(<span class="dv">3</span>)</span>
<span id="cb61-58"><a href="#cb61-58" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>model<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb61-59"><a href="#cb61-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> feat, imp <span class="kw">in</span> top_3.items():</span>
<span id="cb61-60"><a href="#cb61-60" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>feat<span class="sc">:15s}</span><span class="ss">: </span><span class="sc">{</span>imp<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="importance-agreement" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/importance-agreement-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Comparación de feature importance entre implementaciones</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top 3 features por modelo:

XGBoost:
  MedInc         : 0.5489
  AveOccup       : 0.1422
  Longitude      : 0.0860

LightGBM:
  Longitude      : 0.1907
  Latitude       : 0.1903
  AveOccup       : 0.1288

CatBoost:
  MedInc         : 0.3878
  Latitude       : 0.2043
  Longitude      : 0.1733</code></pre>
</div>
</div>
</section>
<section id="guía-de-selección" class="level4">
<h4 class="anchored" data-anchor-id="guía-de-selección">Guía de selección</h4>
<p>Con base en el análisis anterior, aquí hay una guía para elegir la implementación adecuada:</p>
<p><strong>Elige XGBoost si:</strong> - 🎯 <strong>Uso general</strong>: Es tu primera vez con boosting avanzado - 🏆 <strong>Competencias</strong>: Participas en Kaggle u otras competencias - 📚 <strong>Documentación</strong>: Necesitas documentación extensa y ejemplos - 🔧 <strong>Flexibilidad</strong>: Quieres custom objectives o métricas personalizadas - 👥 <strong>Comunidad</strong>: Prefieres la comunidad más grande y establecida - ⚖️ <strong>Balance</strong>: Necesitas buen balance entre velocidad y precisión</p>
<p><strong>Elige LightGBM si:</strong> - ⚡ <strong>Velocidad</strong>: La velocidad de entrenamiento es crítica - 📊 <strong>Datos grandes</strong>: Tienes &gt;50,000 muestras y &gt;100 features - 💾 <strong>Memoria limitada</strong>: Tienes restricciones de memoria - 🔄 <strong>Iteración rápida</strong>: Necesitas experimentar con muchos modelos - 🎛️ <strong>AutoML</strong>: Estás construyendo sistemas de AutoML - 📈 <strong>Sparse features</strong>: Tienes features muy sparse</p>
<p><strong>Elige CatBoost si:</strong> - 🏷️ <strong>Categorías</strong>: Tienes muchas features categóricas (especialmente high-cardinality) - ⏱️ <strong>Poco tiempo</strong>: No tienes tiempo para tuning extensivo - 🎯 <strong>Defaults robustos</strong>: Quieres buenos resultados sin mucho esfuerzo - 🚀 <strong>Producción</strong>: Vas a deployar a producción (predicción rápida) - 🔒 <strong>Estabilidad</strong>: Priorizas reproducibilidad y estabilidad - 📦 <strong>Out-of-the-box</strong>: Prefieres que “funcione bien” desde el inicio</p>
<p><strong>Elige sklearn GradientBoosting si:</strong> - 📖 <strong>Aprendizaje</strong>: Estás aprendiendo los conceptos fundamentales - 🔬 <strong>Datasets pequeños</strong>: Tienes &lt;10,000 muestras - 🧩 <strong>Simplicidad</strong>: Quieres la API más simple y familiar - 🔗 <strong>Integración</strong>: Ya usas sklearn extensivamente - 🎓 <strong>Enseñanza</strong>: Estás enseñando conceptos de ML</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Estrategia práctica
</div>
</div>
<div class="callout-body-container callout-body">
<p>En la práctica, muchos científicos de datos prueban las tres implementaciones modernas (XGBoost, LightGBM, CatBoost) y seleccionan la que mejor funciona para su problema específico. Las diferencias de rendimiento suelen ser pequeñas, pero consistentes.</p>
<p>Una buena estrategia es: 1. Comenzar con CatBoost (defaults robustos) 2. Si la velocidad es un problema, probar LightGBM 3. Si necesitas más control/flexibilidad, probar XGBoost 4. Comparar los tres con cross-validation</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ensemble de ensembles
</div>
</div>
<div class="callout-body-container callout-body">
<p>Un enfoque avanzado es usar <strong>stacking</strong> o <strong>voting</strong> combinando XGBoost, LightGBM y CatBoost. Como sus predicciones están altamente correlacionadas pero no son idénticas, un meta-modelo puede aprender a combinarlas efectivamente, típicamente ganando 0.5-1% en accuracy.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> VotingRegressor</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>voting <span class="op">=</span> VotingRegressor([</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'xgb'</span>, xgb_model),</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'lgb'</span>, lgb_model),</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'cat'</span>, cat_model)</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section>
<section id="recomendaciones-finales" class="level4">
<h4 class="anchored" data-anchor-id="recomendaciones-finales">Recomendaciones finales</h4>
<p><strong>Para comenzar:</strong> Usa XGBoost. Tiene el mejor balance entre rendimiento, documentación y comunidad.</p>
<p><strong>Para producción:</strong> CatBoost o LightGBM, dependiendo de si tienes muchas categorías (CatBoost) o priorizas velocidad (LightGBM).</p>
<p><strong>Para competencias:</strong> Prueba los tres y combínalos con stacking o voting.</p>
<p><strong>Para aprendizaje:</strong> Empieza con sklearn GradientBoosting para entender conceptos, luego pasa a las implementaciones modernas.</p>
<p>Independientemente de la elección, todos estos métodos son órdenes de magnitud mejores que modelos simples para la mayoría de problemas de datos tabulares, y la diferencia entre ellos es típicamente menor que la diferencia que puedes lograr con mejor ingeniería de features o más datos.</p>
</section>
</section>
</section>
<section id="sec-hyperparameters" class="level2">
<h2 class="anchored" data-anchor-id="sec-hyperparameters">6. Hiperparámetros y Regularización</h2>
<p>Los algoritmos de boosting tienen numerosos hiperparámetros que controlan el comportamiento del modelo. Entender qué hace cada uno y cómo afectan el rendimiento es crucial para usar boosting efectivamente. En esta sección exploraremos los hiperparámetros más importantes y su impacto en el aprendizaje del modelo.</p>
<p>A diferencia de Random Forest que es relativamente robusto a la elección de hiperparámetros, los modelos de boosting son más sensibles y requieren más atención. Sin embargo, esta sensibilidad también permite un control más fino del comportamiento del modelo.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Sobre la optimización de hiperparámetros
</div>
</div>
<div class="callout-body-container callout-body">
<p>En esta sección nos enfocaremos en <strong>entender</strong> qué hace cada hiperparámetro y <strong>visualizar</strong> sus efectos. Las técnicas de optimización sistemática de hiperparámetros (Grid Search, Random Search, Bayesian Optimization) se cubrirán en un capítulo posterior dedicado a este tema.</p>
</div>
</div>
<section id="sec-learning-rate" class="level3">
<h3 class="anchored" data-anchor-id="sec-learning-rate">6.1 Learning Rate (Tasa de Aprendizaje)</h3>
<p>El <strong>learning rate</strong> (también llamado <span class="math inline">\(\eta\)</span> o <code>eta</code> en XGBoost, <code>learning_rate</code> en sklearn/LightGBM/CatBoost) es probablemente el hiperparámetro más importante en boosting. Controla cuánto contribuye cada árbol nuevo al modelo total.</p>
<p><strong>Fórmula:</strong> <span class="math display">\[F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)\]</span></p>
<p>donde <span class="math inline">\(\nu\)</span> es el learning rate (típicamente entre 0.01 y 0.3).</p>
<p><strong>Intuición:</strong> - <strong>Learning rate bajo</strong> (e.g., 0.01): Cada árbol hace correcciones pequeñas → aprendizaje lento pero cuidadoso - <strong>Learning rate alto</strong> (e.g., 0.5-1.0): Cada árbol hace correcciones grandes → aprendizaje rápido pero puede sobreajustar</p>
<p><strong>Trade-off fundamental:</strong> - Learning rate bajo + muchos árboles = mejor generalización, más tiempo de entrenamiento - Learning rate alto + pocos árboles = rápido pero puede sobreajustar</p>
<p>Visualicemos el efecto del learning rate:</p>
<div id="cell-learning-rate-effect" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear dataset para visualización</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>X_lr <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>y_lr <span class="op">=</span> np.sin(X_lr).ravel() <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.2</span>, X_lr.shape[<span class="dv">0</span>])</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>X_train_lr, X_test_lr, y_train_lr, y_test_lr <span class="op">=</span> train_test_split(</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>    X_lr, y_lr, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Probar diferentes learning rates</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>learning_rates <span class="op">=</span> [<span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">1.0</span>]</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>n_estimators_fixed <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.ravel()</span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, lr <span class="kw">in</span> <span class="bu">enumerate</span>(learning_rates):</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Entrenar modelo</span></span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span>n_estimators_fixed,</span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span>lr,</span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>        verbosity<span class="op">=</span><span class="dv">0</span></span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit con eval_set para obtener curvas</span></span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>    model.fit(</span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a>        X_train_lr, y_train_lr,</span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a>        eval_set<span class="op">=</span>[(X_train_lr, y_train_lr), (X_test_lr, y_test_lr)],</span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Obtener errores por iteración</span></span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> model.evals_result()</span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a>    train_rmse <span class="op">=</span> np.sqrt(results[<span class="st">'validation_0'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a>    test_rmse <span class="op">=</span> np.sqrt(results[<span class="st">'validation_1'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot learning curves</span></span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a>    axes[idx].plot(train_rmse, label<span class="op">=</span><span class="st">'Train'</span>, color<span class="op">=</span><span class="st">'#3498db'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a>    axes[idx].plot(test_rmse, label<span class="op">=</span><span class="st">'Test'</span>, color<span class="op">=</span><span class="st">'#e74c3c'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb64-42"><a href="#cb64-42" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_xlabel(<span class="st">'Número de árboles'</span>)</span>
<span id="cb64-43"><a href="#cb64-43" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_ylabel(<span class="st">'RMSE'</span>)</span>
<span id="cb64-44"><a href="#cb64-44" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_title(<span class="ss">f'Learning Rate = </span><span class="sc">{</span>lr<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb64-45"><a href="#cb64-45" aria-hidden="true" tabindex="-1"></a>    axes[idx].legend()</span>
<span id="cb64-46"><a href="#cb64-46" aria-hidden="true" tabindex="-1"></a>    axes[idx].grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb64-47"><a href="#cb64-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-48"><a href="#cb64-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Indicar si hay overfitting</span></span>
<span id="cb64-49"><a href="#cb64-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> test_rmse[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> test_rmse.<span class="bu">min</span>() <span class="op">*</span> <span class="fl">1.05</span>:</span>
<span id="cb64-50"><a href="#cb64-50" aria-hidden="true" tabindex="-1"></a>        axes[idx].axvline(x<span class="op">=</span>np.argmin(test_rmse), color<span class="op">=</span><span class="st">'green'</span>,</span>
<span id="cb64-51"><a href="#cb64-51" aria-hidden="true" tabindex="-1"></a>                         linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Óptimo'</span>)</span>
<span id="cb64-52"><a href="#cb64-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-53"><a href="#cb64-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Remover último subplot vacío</span></span>
<span id="cb64-54"><a href="#cb64-54" aria-hidden="true" tabindex="-1"></a>fig.delaxes(axes[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb64-55"><a href="#cb64-55" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb64-56"><a href="#cb64-56" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb64-57"><a href="#cb64-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-58"><a href="#cb64-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Observaciones:"</span>)</span>
<span id="cb64-59"><a href="#cb64-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- Learning rate muy bajo (0.01): Aprende lentamente, necesita más árboles"</span>)</span>
<span id="cb64-60"><a href="#cb64-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- Learning rate bajo-medio (0.05-0.1): Balance óptimo para este problema"</span>)</span>
<span id="cb64-61"><a href="#cb64-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- Learning rate alto (0.3-1.0): Aprende rápido pero sobreajusta"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="learning-rate-effect" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/learning-rate-effect-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Efecto del learning rate en el aprendizaje</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Observaciones:
- Learning rate muy bajo (0.01): Aprende lentamente, necesita más árboles
- Learning rate bajo-medio (0.05-0.1): Balance óptimo para este problema
- Learning rate alto (0.3-1.0): Aprende rápido pero sobreajusta</code></pre>
</div>
</div>
<p>Ahora veamos el trade-off entre learning rate y número de estimadores:</p>
<div id="cell-lr-nestimators-tradeoff" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Combinaciones de learning rate y n_estimators</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Mantenemos el "presupuesto" de aprendizaje similar</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>configs <span class="op">=</span> [</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'lr'</span>: <span class="fl">0.01</span>, <span class="st">'n_est'</span>: <span class="dv">500</span>, <span class="st">'label'</span>: <span class="st">'lr=0.01, n=500'</span>},</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'lr'</span>: <span class="fl">0.05</span>, <span class="st">'n_est'</span>: <span class="dv">100</span>, <span class="st">'label'</span>: <span class="st">'lr=0.05, n=100'</span>},</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'lr'</span>: <span class="fl">0.1</span>, <span class="st">'n_est'</span>: <span class="dv">50</span>, <span class="st">'label'</span>: <span class="st">'lr=0.1, n=50'</span>},</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'lr'</span>: <span class="fl">0.3</span>, <span class="st">'n_est'</span>: <span class="dv">20</span>, <span class="st">'label'</span>: <span class="st">'lr=0.3, n=20'</span>},</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'#3498db'</span>, <span class="st">'#2ecc71'</span>, <span class="st">'#f39c12'</span>, <span class="st">'#e74c3c'</span>]</span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, config <span class="kw">in</span> <span class="bu">enumerate</span>(configs):</span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span>config[<span class="st">'n_est'</span>],</span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span>config[<span class="st">'lr'</span>],</span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a>        verbosity<span class="op">=</span><span class="dv">0</span></span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb66-21"><a href="#cb66-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-22"><a href="#cb66-22" aria-hidden="true" tabindex="-1"></a>    model.fit(</span>
<span id="cb66-23"><a href="#cb66-23" aria-hidden="true" tabindex="-1"></a>        X_train_lr, y_train_lr,</span>
<span id="cb66-24"><a href="#cb66-24" aria-hidden="true" tabindex="-1"></a>        eval_set<span class="op">=</span>[(X_train_lr, y_train_lr), (X_test_lr, y_test_lr)],</span>
<span id="cb66-25"><a href="#cb66-25" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb66-26"><a href="#cb66-26" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb66-27"><a href="#cb66-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-28"><a href="#cb66-28" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> model.evals_result()</span>
<span id="cb66-29"><a href="#cb66-29" aria-hidden="true" tabindex="-1"></a>    test_rmse <span class="op">=</span> np.sqrt(results[<span class="st">'validation_1'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb66-30"><a href="#cb66-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-31"><a href="#cb66-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot 1: RMSE vs iterations</span></span>
<span id="cb66-32"><a href="#cb66-32" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].plot(test_rmse, label<span class="op">=</span>config[<span class="st">'label'</span>],</span>
<span id="cb66-33"><a href="#cb66-33" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span>colors[idx], linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb66-34"><a href="#cb66-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-35"><a href="#cb66-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot 2: Final performance</span></span>
<span id="cb66-36"><a href="#cb66-36" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].bar(idx, test_rmse[<span class="op">-</span><span class="dv">1</span>], color<span class="op">=</span>colors[idx], alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb66-37"><a href="#cb66-37" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].text(idx, test_rmse[<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.01</span>,</span>
<span id="cb66-38"><a href="#cb66-38" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"</span><span class="sc">{</span>test_rmse[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb66-39"><a href="#cb66-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-40"><a href="#cb66-40" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Número de árboles'</span>)</span>
<span id="cb66-41"><a href="#cb66-41" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Test RMSE'</span>)</span>
<span id="cb66-42"><a href="#cb66-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Curvas de Aprendizaje: LR vs N_estimators'</span>)</span>
<span id="cb66-43"><a href="#cb66-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend()</span>
<span id="cb66-44"><a href="#cb66-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb66-45"><a href="#cb66-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-46"><a href="#cb66-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xticks(<span class="bu">range</span>(<span class="bu">len</span>(configs)))</span>
<span id="cb66-47"><a href="#cb66-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xticklabels([c[<span class="st">'label'</span>] <span class="cf">for</span> c <span class="kw">in</span> configs], rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb66-48"><a href="#cb66-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'RMSE Final'</span>)</span>
<span id="cb66-49"><a href="#cb66-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Rendimiento Final'</span>)</span>
<span id="cb66-50"><a href="#cb66-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb66-51"><a href="#cb66-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-52"><a href="#cb66-52" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb66-53"><a href="#cb66-53" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb66-54"><a href="#cb66-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-55"><a href="#cb66-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Conclusión:"</span>)</span>
<span id="cb66-56"><a href="#cb66-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Un learning rate más bajo con más árboles generalmente produce mejor"</span>)</span>
<span id="cb66-57"><a href="#cb66-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"generalización, aunque requiere más tiempo de entrenamiento."</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="lr-nestimators-tradeoff" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/lr-nestimators-tradeoff-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Trade-off entre learning rate y número de estimadores</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Conclusión:
Un learning rate más bajo con más árboles generalmente produce mejor
generalización, aunque requiere más tiempo de entrenamiento.</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Regla práctica para learning rate
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Desarrollo/experimentación</strong>: Usa <code>learning_rate=0.1</code> con ~100 árboles para iterar rápido</li>
<li><strong>Producción final</strong>: Reduce a <code>learning_rate=0.01-0.05</code> con más árboles (500-1000+) para mejor rendimiento</li>
<li><strong>Early stopping</strong>: Usa un learning rate bajo con muchos árboles y deja que early stopping encuentre el número óptimo</li>
</ul>
</div>
</div>
</section>
<section id="sec-n-estimators" class="level3">
<h3 class="anchored" data-anchor-id="sec-n-estimators">6.2 Número de Estimadores y Early Stopping</h3>
<p>El número de estimadores (<code>n_estimators</code>, <code>iterations</code>) determina cuántos árboles secuenciales se construirán. Más árboles significan: - ✅ Modelo más expresivo, puede capturar patrones más complejos - ❌ Mayor riesgo de overfitting - ❌ Mayor tiempo de entrenamiento</p>
<p><strong>Early Stopping</strong> es una técnica crucial que automáticamente detiene el entrenamiento cuando el rendimiento en un conjunto de validación deja de mejorar.</p>
<div id="cell-early-stopping-demo" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Usar California Housing para ejemplo más realista</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>X_es, y_es <span class="op">=</span> fetch_california_housing(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>X_train_es, X_temp, y_train_es, y_temp <span class="op">=</span> train_test_split(</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    X_es, y_es, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>X_val_es, X_test_es, y_val_es, y_test_es <span class="op">=</span> train_test_split(</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    X_temp, y_temp, test_size<span class="op">=</span><span class="fl">0.5</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Modelo sin early stopping</span></span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>model_no_es <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>    verbosity<span class="op">=</span><span class="dv">0</span></span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a>model_no_es.fit(</span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true" tabindex="-1"></a>    X_train_es, y_train_es,</span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true" tabindex="-1"></a>    eval_set<span class="op">=</span>[(X_train_es, y_train_es), (X_val_es, y_val_es)],</span>
<span id="cb68-22"><a href="#cb68-22" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb68-23"><a href="#cb68-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb68-24"><a href="#cb68-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-25"><a href="#cb68-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Modelo con early stopping</span></span>
<span id="cb68-26"><a href="#cb68-26" aria-hidden="true" tabindex="-1"></a>model_with_es <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb68-27"><a href="#cb68-27" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb68-28"><a href="#cb68-28" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb68-29"><a href="#cb68-29" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb68-30"><a href="#cb68-30" aria-hidden="true" tabindex="-1"></a>    early_stopping_rounds<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb68-31"><a href="#cb68-31" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb68-32"><a href="#cb68-32" aria-hidden="true" tabindex="-1"></a>    verbosity<span class="op">=</span><span class="dv">0</span></span>
<span id="cb68-33"><a href="#cb68-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb68-34"><a href="#cb68-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-35"><a href="#cb68-35" aria-hidden="true" tabindex="-1"></a>model_with_es.fit(</span>
<span id="cb68-36"><a href="#cb68-36" aria-hidden="true" tabindex="-1"></a>    X_train_es, y_train_es,</span>
<span id="cb68-37"><a href="#cb68-37" aria-hidden="true" tabindex="-1"></a>    eval_set<span class="op">=</span>[(X_train_es, y_train_es), (X_val_es, y_val_es)],</span>
<span id="cb68-38"><a href="#cb68-38" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb68-39"><a href="#cb68-39" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb68-40"><a href="#cb68-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-41"><a href="#cb68-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Comparar resultados</span></span>
<span id="cb68-42"><a href="#cb68-42" aria-hidden="true" tabindex="-1"></a>results_no_es <span class="op">=</span> model_no_es.evals_result()</span>
<span id="cb68-43"><a href="#cb68-43" aria-hidden="true" tabindex="-1"></a>results_with_es <span class="op">=</span> model_with_es.evals_result()</span>
<span id="cb68-44"><a href="#cb68-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-45"><a href="#cb68-45" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb68-46"><a href="#cb68-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-47"><a href="#cb68-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Sin early stopping</span></span>
<span id="cb68-48"><a href="#cb68-48" aria-hidden="true" tabindex="-1"></a>train_rmse_no <span class="op">=</span> np.sqrt(results_no_es[<span class="st">'validation_0'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb68-49"><a href="#cb68-49" aria-hidden="true" tabindex="-1"></a>val_rmse_no <span class="op">=</span> np.sqrt(results_no_es[<span class="st">'validation_1'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb68-50"><a href="#cb68-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-51"><a href="#cb68-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(train_rmse_no, label<span class="op">=</span><span class="st">'Train'</span>, color<span class="op">=</span><span class="st">'#3498db'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb68-52"><a href="#cb68-52" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(val_rmse_no, label<span class="op">=</span><span class="st">'Validation'</span>, color<span class="op">=</span><span class="st">'#e74c3c'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb68-53"><a href="#cb68-53" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axvline(x<span class="op">=</span>np.argmin(val_rmse_no), color<span class="op">=</span><span class="st">'green'</span>,</span>
<span id="cb68-54"><a href="#cb68-54" aria-hidden="true" tabindex="-1"></a>               linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Óptimo (iter </span><span class="sc">{</span>np<span class="sc">.</span>argmin(val_rmse_no)<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb68-55"><a href="#cb68-55" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Número de árboles'</span>)</span>
<span id="cb68-56"><a href="#cb68-56" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'RMSE'</span>)</span>
<span id="cb68-57"><a href="#cb68-57" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Sin Early Stopping'</span>)</span>
<span id="cb68-58"><a href="#cb68-58" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend()</span>
<span id="cb68-59"><a href="#cb68-59" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb68-60"><a href="#cb68-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-61"><a href="#cb68-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Con early stopping</span></span>
<span id="cb68-62"><a href="#cb68-62" aria-hidden="true" tabindex="-1"></a>train_rmse_with <span class="op">=</span> np.sqrt(results_with_es[<span class="st">'validation_0'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb68-63"><a href="#cb68-63" aria-hidden="true" tabindex="-1"></a>val_rmse_with <span class="op">=</span> np.sqrt(results_with_es[<span class="st">'validation_1'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb68-64"><a href="#cb68-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-65"><a href="#cb68-65" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(train_rmse_with, label<span class="op">=</span><span class="st">'Train'</span>, color<span class="op">=</span><span class="st">'#3498db'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb68-66"><a href="#cb68-66" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].plot(val_rmse_with, label<span class="op">=</span><span class="st">'Validation'</span>, color<span class="op">=</span><span class="st">'#e74c3c'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb68-67"><a href="#cb68-67" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axvline(x<span class="op">=</span>model_with_es.best_iteration, color<span class="op">=</span><span class="st">'green'</span>,</span>
<span id="cb68-68"><a href="#cb68-68" aria-hidden="true" tabindex="-1"></a>               linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb68-69"><a href="#cb68-69" aria-hidden="true" tabindex="-1"></a>               label<span class="op">=</span><span class="ss">f'Paró en iter </span><span class="sc">{</span>model_with_es<span class="sc">.</span>best_iteration<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb68-70"><a href="#cb68-70" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Número de árboles'</span>)</span>
<span id="cb68-71"><a href="#cb68-71" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'RMSE'</span>)</span>
<span id="cb68-72"><a href="#cb68-72" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Con Early Stopping (20 rondas)'</span>)</span>
<span id="cb68-73"><a href="#cb68-73" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].legend()</span>
<span id="cb68-74"><a href="#cb68-74" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb68-75"><a href="#cb68-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-76"><a href="#cb68-76" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb68-77"><a href="#cb68-77" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb68-78"><a href="#cb68-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-79"><a href="#cb68-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluar en test set</span></span>
<span id="cb68-80"><a href="#cb68-80" aria-hidden="true" tabindex="-1"></a>y_pred_no_es <span class="op">=</span> model_no_es.predict(X_test_es)</span>
<span id="cb68-81"><a href="#cb68-81" aria-hidden="true" tabindex="-1"></a>y_pred_with_es <span class="op">=</span> model_with_es.predict(X_test_es)</span>
<span id="cb68-82"><a href="#cb68-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-83"><a href="#cb68-83" aria-hidden="true" tabindex="-1"></a>rmse_no_es <span class="op">=</span> np.sqrt(mean_squared_error(y_test_es, y_pred_no_es))</span>
<span id="cb68-84"><a href="#cb68-84" aria-hidden="true" tabindex="-1"></a>rmse_with_es <span class="op">=</span> np.sqrt(mean_squared_error(y_test_es, y_pred_with_es))</span>
<span id="cb68-85"><a href="#cb68-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-86"><a href="#cb68-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Rendimiento en Test Set:"</span>)</span>
<span id="cb68-87"><a href="#cb68-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sin early stopping:  RMSE = </span><span class="sc">{</span>rmse_no_es<span class="sc">:.4f}</span><span class="ss"> (usó 500 árboles)"</span>)</span>
<span id="cb68-88"><a href="#cb68-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Con early stopping:  RMSE = </span><span class="sc">{</span>rmse_with_es<span class="sc">:.4f}</span><span class="ss"> (usó </span><span class="sc">{</span>model_with_es<span class="sc">.</span>best_iteration<span class="sc">}</span><span class="ss"> árboles)"</span>)</span>
<span id="cb68-89"><a href="#cb68-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Árboles ahorrados: </span><span class="sc">{</span><span class="dv">500</span> <span class="op">-</span> model_with_es<span class="sc">.</span>best_iteration<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb68-90"><a href="#cb68-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mejora en RMSE: </span><span class="sc">{</span>((rmse_no_es <span class="op">-</span> rmse_with_es) <span class="op">/</span> rmse_no_es <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="early-stopping-demo" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/early-stopping-demo-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Early stopping previene overfitting automáticamente</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Rendimiento en Test Set:
Sin early stopping:  RMSE = 0.4421 (usó 500 árboles)
Con early stopping:  RMSE = 0.4421 (usó 499 árboles)

Árboles ahorrados: 1
Mejora en RMSE: 0.00%</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Recomendación para early stopping
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Siempre usa early stopping en producción:</strong></p>
<ol type="1">
<li>Configura <code>n_estimators</code> alto (500-2000)</li>
<li>Usa <code>early_stopping_rounds=20-50</code> (más alto si usas learning rate muy bajo)</li>
<li>Proporciona un conjunto de validación separado</li>
<li>El modelo se detendrá automáticamente en el punto óptimo</li>
</ol>
<p>Esto previene overfitting y ahorra tiempo de entrenamiento sin necesidad de adivinar el número óptimo de árboles.</p>
</div>
</div>
</section>
<section id="sec-tree-structure" class="level3">
<h3 class="anchored" data-anchor-id="sec-tree-structure">6.3 Parámetros de Estructura del Árbol</h3>
<p>Los parámetros que controlan la estructura de cada árbol individual son cruciales para el balance bias-variance.</p>
<p><strong>Principales parámetros:</strong></p>
<ul>
<li><code>max_depth</code>: Profundidad máxima de cada árbol (típicamente 3-10)</li>
<li><code>min_child_weight</code> (XGBoost) / <code>min_samples_leaf</code> (sklearn): Mínimo de muestras en una hoja</li>
<li><code>min_samples_split</code> (sklearn/LightGBM): Mínimo de muestras para dividir un nodo</li>
<li><code>num_leaves</code> (LightGBM): Número máximo de hojas (específico de leaf-wise growth)</li>
</ul>
<p><strong>Intuición:</strong> - Árboles <strong>poco profundos</strong> (depth 1-3): Alto bias, bajo variance → underfitting potencial - Árboles <strong>profundos</strong> (depth 8-15): Bajo bias, alto variance → overfitting potencial - Boosting típicamente usa árboles poco profundos (weak learners) para reducir bias gradualmente</p>
<div id="cell-max-depth-effect" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Probar diferentes profundidades</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>depths <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">10</span>]</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>n_est <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.ravel()</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, depth <span class="kw">in</span> <span class="bu">enumerate</span>(depths):</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span>n_est,</span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span>depth,</span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a>        verbosity<span class="op">=</span><span class="dv">0</span></span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-17"><a href="#cb70-17" aria-hidden="true" tabindex="-1"></a>    model.fit(</span>
<span id="cb70-18"><a href="#cb70-18" aria-hidden="true" tabindex="-1"></a>        X_train_es, y_train_es,</span>
<span id="cb70-19"><a href="#cb70-19" aria-hidden="true" tabindex="-1"></a>        eval_set<span class="op">=</span>[(X_train_es, y_train_es), (X_val_es, y_val_es)],</span>
<span id="cb70-20"><a href="#cb70-20" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb70-21"><a href="#cb70-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb70-22"><a href="#cb70-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-23"><a href="#cb70-23" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> model.evals_result()</span>
<span id="cb70-24"><a href="#cb70-24" aria-hidden="true" tabindex="-1"></a>    train_rmse <span class="op">=</span> np.sqrt(results[<span class="st">'validation_0'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb70-25"><a href="#cb70-25" aria-hidden="true" tabindex="-1"></a>    val_rmse <span class="op">=</span> np.sqrt(results[<span class="st">'validation_1'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb70-26"><a href="#cb70-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-27"><a href="#cb70-27" aria-hidden="true" tabindex="-1"></a>    axes[idx].plot(train_rmse, label<span class="op">=</span><span class="st">'Train'</span>, color<span class="op">=</span><span class="st">'#3498db'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb70-28"><a href="#cb70-28" aria-hidden="true" tabindex="-1"></a>    axes[idx].plot(val_rmse, label<span class="op">=</span><span class="st">'Validation'</span>, color<span class="op">=</span><span class="st">'#e74c3c'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb70-29"><a href="#cb70-29" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_xlabel(<span class="st">'Número de árboles'</span>)</span>
<span id="cb70-30"><a href="#cb70-30" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_ylabel(<span class="st">'RMSE'</span>)</span>
<span id="cb70-31"><a href="#cb70-31" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_title(<span class="ss">f'max_depth = </span><span class="sc">{</span>depth<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb70-32"><a href="#cb70-32" aria-hidden="true" tabindex="-1"></a>    axes[idx].legend()</span>
<span id="cb70-33"><a href="#cb70-33" aria-hidden="true" tabindex="-1"></a>    axes[idx].grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb70-34"><a href="#cb70-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-35"><a href="#cb70-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calcular gap entre train y validation (overfitting)</span></span>
<span id="cb70-36"><a href="#cb70-36" aria-hidden="true" tabindex="-1"></a>    gap <span class="op">=</span> train_rmse[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> val_rmse[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb70-37"><a href="#cb70-37" aria-hidden="true" tabindex="-1"></a>    axes[idx].text(<span class="fl">0.02</span>, <span class="fl">0.98</span>, <span class="ss">f'Gap: </span><span class="sc">{</span>gap<span class="sc">:.3f}</span><span class="ss">'</span>,</span>
<span id="cb70-38"><a href="#cb70-38" aria-hidden="true" tabindex="-1"></a>                   transform<span class="op">=</span>axes[idx].transAxes,</span>
<span id="cb70-39"><a href="#cb70-39" aria-hidden="true" tabindex="-1"></a>                   va<span class="op">=</span><span class="st">'top'</span>, bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'wheat'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>))</span>
<span id="cb70-40"><a href="#cb70-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-41"><a href="#cb70-41" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb70-42"><a href="#cb70-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb70-43"><a href="#cb70-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-44"><a href="#cb70-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Observaciones:"</span>)</span>
<span id="cb70-45"><a href="#cb70-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- depth=1 (stumps): Underfitting, alto error en train y validation"</span>)</span>
<span id="cb70-46"><a href="#cb70-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- depth=2-3: Buen balance, bajo overfitting"</span>)</span>
<span id="cb70-47"><a href="#cb70-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- depth=5-7: Empieza a sobreajustar (gap train-validation aumenta)"</span>)</span>
<span id="cb70-48"><a href="#cb70-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- depth=10: Severo overfitting, excelente en train, pobre en validation"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="max-depth-effect" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/max-depth-effect-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Efecto de la profundidad del árbol en el aprendizaje</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Observaciones:
- depth=1 (stumps): Underfitting, alto error en train y validation
- depth=2-3: Buen balance, bajo overfitting
- depth=5-7: Empieza a sobreajustar (gap train-validation aumenta)
- depth=10: Severo overfitting, excelente en train, pobre en validation</code></pre>
</div>
</div>
<p>Visualicemos el efecto en decision boundaries (problema de clasificación 2D):</p>
<div id="cell-depth-decision-boundaries" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear dataset 2D para visualización</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_circles</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>X_circles, y_circles <span class="op">=</span> make_circles(n_samples<span class="op">=</span><span class="dv">500</span>, noise<span class="op">=</span><span class="fl">0.2</span>, factor<span class="op">=</span><span class="fl">0.5</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>depths_viz <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>]</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">4</span>))</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, depth <span class="kw">in</span> <span class="bu">enumerate</span>(depths_viz):</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Entrenar modelo</span></span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> xgb.XGBClassifier(</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span>depth,</span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a>        verbosity<span class="op">=</span><span class="dv">0</span></span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a>    model.fit(X_circles, y_circles)</span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-20"><a href="#cb72-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Crear grid para decision boundary</span></span>
<span id="cb72-21"><a href="#cb72-21" aria-hidden="true" tabindex="-1"></a>    x_min, x_max <span class="op">=</span> X_circles[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">0.5</span>, X_circles[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">0.5</span></span>
<span id="cb72-22"><a href="#cb72-22" aria-hidden="true" tabindex="-1"></a>    y_min, y_max <span class="op">=</span> X_circles[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">0.5</span>, X_circles[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">0.5</span></span>
<span id="cb72-23"><a href="#cb72-23" aria-hidden="true" tabindex="-1"></a>    xx, yy <span class="op">=</span> np.meshgrid(np.linspace(x_min, x_max, <span class="dv">200</span>),</span>
<span id="cb72-24"><a href="#cb72-24" aria-hidden="true" tabindex="-1"></a>                         np.linspace(y_min, y_max, <span class="dv">200</span>))</span>
<span id="cb72-25"><a href="#cb72-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-26"><a href="#cb72-26" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb72-27"><a href="#cb72-27" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb72-28"><a href="#cb72-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-29"><a href="#cb72-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot</span></span>
<span id="cb72-30"><a href="#cb72-30" aria-hidden="true" tabindex="-1"></a>    axes[idx].contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.3</span>, cmap<span class="op">=</span><span class="st">'RdYlBu'</span>)</span>
<span id="cb72-31"><a href="#cb72-31" aria-hidden="true" tabindex="-1"></a>    axes[idx].scatter(X_circles[:, <span class="dv">0</span>], X_circles[:, <span class="dv">1</span>], c<span class="op">=</span>y_circles,</span>
<span id="cb72-32"><a href="#cb72-32" aria-hidden="true" tabindex="-1"></a>                     cmap<span class="op">=</span><span class="st">'RdYlBu'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, s<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb72-33"><a href="#cb72-33" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_title(<span class="ss">f'max_depth = </span><span class="sc">{</span>depth<span class="sc">}</span><span class="ch">\n</span><span class="ss">Acc: </span><span class="sc">{</span>model<span class="sc">.</span>score(X_circles, y_circles)<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb72-34"><a href="#cb72-34" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_xlabel(<span class="st">'Feature 1'</span>)</span>
<span id="cb72-35"><a href="#cb72-35" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_ylabel(<span class="st">'Feature 2'</span>)</span>
<span id="cb72-36"><a href="#cb72-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-37"><a href="#cb72-37" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb72-38"><a href="#cb72-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb72-39"><a href="#cb72-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-40"><a href="#cb72-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Interpretación:"</span>)</span>
<span id="cb72-41"><a href="#cb72-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- depth=1: Decision boundary muy simple, underfitting"</span>)</span>
<span id="cb72-42"><a href="#cb72-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- depth=2-3: Captura el patrón circular razonablemente"</span>)</span>
<span id="cb72-43"><a href="#cb72-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- depth=5: Boundary muy compleja, puede sobreajustar a ruido"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="depth-decision-boundaries" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/depth-decision-boundaries-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Decision boundaries con diferentes profundidades de árbol</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Interpretación:
- depth=1: Decision boundary muy simple, underfitting
- depth=2-3: Captura el patrón circular razonablemente
- depth=5: Boundary muy compleja, puede sobreajustar a ruido</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Valores típicos recomendados
</div>
</div>
<div class="callout-body-container callout-body">
<p>Para la mayoría de problemas, estos son buenos puntos de partida:</p>
<ul>
<li><strong>max_depth</strong>: 3-6 (XGBoost/sklearn), 5-8 (CatBoost con árboles oblivious)</li>
<li><strong>num_leaves</strong>: 20-50 (LightGBM)</li>
<li><strong>min_child_weight</strong>: 1-5 (más alto para datos ruidosos)</li>
<li><strong>min_samples_leaf</strong>: 5-20 (sklearn)</li>
</ul>
<p>Árboles más profundos pueden ser útiles con: - Datasets muy grandes (&gt;100k muestras) - Muchas features informativas - Relaciones muy complejas - Cuando usas learning rate muy bajo y mucha regularización</p>
</div>
</div>
</section>
<section id="sec-subsampling" class="level3">
<h3 class="anchored" data-anchor-id="sec-subsampling">6.4 Subsampling (Stochastic Gradient Boosting)</h3>
<p>Similar a Random Forest, podemos añadir randomness al boosting muestreando observaciones y features. Esto reduce overfitting y puede acelerar el entrenamiento.</p>
<p><strong>Parámetros de subsampling:</strong></p>
<ul>
<li><code>subsample</code> / <code>bagging_fraction</code>: Fracción de observaciones a usar por árbol (0.5-1.0)</li>
<li><code>colsample_bytree</code> / <code>feature_fraction</code>: Fracción de features a usar por árbol</li>
<li><code>colsample_bylevel</code>: Fracción de features por nivel del árbol (XGBoost)</li>
</ul>
<div id="cell-subsample-effect" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Configuraciones de subsample</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>subsample_configs <span class="op">=</span> [</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'subsample'</span>: <span class="fl">1.0</span>, <span class="st">'colsample'</span>: <span class="fl">1.0</span>, <span class="st">'label'</span>: <span class="st">'Sin subsample'</span>},</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'subsample'</span>: <span class="fl">0.8</span>, <span class="st">'colsample'</span>: <span class="fl">1.0</span>, <span class="st">'label'</span>: <span class="st">'Row subsample 0.8'</span>},</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'subsample'</span>: <span class="fl">1.0</span>, <span class="st">'colsample'</span>: <span class="fl">0.8</span>, <span class="st">'label'</span>: <span class="st">'Col subsample 0.8'</span>},</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'subsample'</span>: <span class="fl">0.8</span>, <span class="st">'colsample'</span>: <span class="fl">0.8</span>, <span class="st">'label'</span>: <span class="st">'Both subsample 0.8'</span>},</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.ravel()</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, config <span class="kw">in</span> <span class="bu">enumerate</span>(subsample_configs):</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span><span class="dv">6</span>,  <span class="co"># Más profundo para ver efecto</span></span>
<span id="cb74-17"><a href="#cb74-17" aria-hidden="true" tabindex="-1"></a>        subsample<span class="op">=</span>config[<span class="st">'subsample'</span>],</span>
<span id="cb74-18"><a href="#cb74-18" aria-hidden="true" tabindex="-1"></a>        colsample_bytree<span class="op">=</span>config[<span class="st">'colsample'</span>],</span>
<span id="cb74-19"><a href="#cb74-19" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb74-20"><a href="#cb74-20" aria-hidden="true" tabindex="-1"></a>        verbosity<span class="op">=</span><span class="dv">0</span></span>
<span id="cb74-21"><a href="#cb74-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb74-22"><a href="#cb74-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-23"><a href="#cb74-23" aria-hidden="true" tabindex="-1"></a>    model.fit(</span>
<span id="cb74-24"><a href="#cb74-24" aria-hidden="true" tabindex="-1"></a>        X_train_es, y_train_es,</span>
<span id="cb74-25"><a href="#cb74-25" aria-hidden="true" tabindex="-1"></a>        eval_set<span class="op">=</span>[(X_train_es, y_train_es), (X_val_es, y_val_es)],</span>
<span id="cb74-26"><a href="#cb74-26" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb74-27"><a href="#cb74-27" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb74-28"><a href="#cb74-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-29"><a href="#cb74-29" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> model.evals_result()</span>
<span id="cb74-30"><a href="#cb74-30" aria-hidden="true" tabindex="-1"></a>    train_rmse <span class="op">=</span> np.sqrt(results[<span class="st">'validation_0'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb74-31"><a href="#cb74-31" aria-hidden="true" tabindex="-1"></a>    val_rmse <span class="op">=</span> np.sqrt(results[<span class="st">'validation_1'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb74-32"><a href="#cb74-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-33"><a href="#cb74-33" aria-hidden="true" tabindex="-1"></a>    axes[idx].plot(train_rmse, label<span class="op">=</span><span class="st">'Train'</span>, color<span class="op">=</span><span class="st">'#3498db'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb74-34"><a href="#cb74-34" aria-hidden="true" tabindex="-1"></a>    axes[idx].plot(val_rmse, label<span class="op">=</span><span class="st">'Validation'</span>, color<span class="op">=</span><span class="st">'#e74c3c'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb74-35"><a href="#cb74-35" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_xlabel(<span class="st">'Número de árboles'</span>)</span>
<span id="cb74-36"><a href="#cb74-36" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_ylabel(<span class="st">'RMSE'</span>)</span>
<span id="cb74-37"><a href="#cb74-37" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_title(config[<span class="st">'label'</span>])</span>
<span id="cb74-38"><a href="#cb74-38" aria-hidden="true" tabindex="-1"></a>    axes[idx].legend()</span>
<span id="cb74-39"><a href="#cb74-39" aria-hidden="true" tabindex="-1"></a>    axes[idx].grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb74-40"><a href="#cb74-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-41"><a href="#cb74-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mostrar gap</span></span>
<span id="cb74-42"><a href="#cb74-42" aria-hidden="true" tabindex="-1"></a>    gap <span class="op">=</span> train_rmse[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> val_rmse[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb74-43"><a href="#cb74-43" aria-hidden="true" tabindex="-1"></a>    axes[idx].text(<span class="fl">0.98</span>, <span class="fl">0.98</span>, <span class="ss">f'Overfitting gap: </span><span class="sc">{</span>gap<span class="sc">:.3f}</span><span class="ss">'</span>,</span>
<span id="cb74-44"><a href="#cb74-44" aria-hidden="true" tabindex="-1"></a>                   transform<span class="op">=</span>axes[idx].transAxes,</span>
<span id="cb74-45"><a href="#cb74-45" aria-hidden="true" tabindex="-1"></a>                   ha<span class="op">=</span><span class="st">'right'</span>, va<span class="op">=</span><span class="st">'top'</span>,</span>
<span id="cb74-46"><a href="#cb74-46" aria-hidden="true" tabindex="-1"></a>                   bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'yellow'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>))</span>
<span id="cb74-47"><a href="#cb74-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-48"><a href="#cb74-48" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb74-49"><a href="#cb74-49" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb74-50"><a href="#cb74-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-51"><a href="#cb74-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Efecto del subsampling:"</span>)</span>
<span id="cb74-52"><a href="#cb74-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- Sin subsample: Overfitting más pronunciado (gap grande)"</span>)</span>
<span id="cb74-53"><a href="#cb74-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- Con subsample: Reduce overfitting (gap menor)"</span>)</span>
<span id="cb74-54"><a href="#cb74-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- Subsample en rows y columns: Máxima regularización"</span>)</span>
<span id="cb74-55"><a href="#cb74-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">El subsample añade randomness que ayuda a generalizar mejor"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="subsample-effect" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/subsample-effect-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Efecto del subsampling en reducción de overfitting</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Efecto del subsampling:
- Sin subsample: Overfitting más pronunciado (gap grande)
- Con subsample: Reduce overfitting (gap menor)
- Subsample en rows y columns: Máxima regularización

El subsample añade randomness que ayuda a generalizar mejor</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Recomendaciones para subsampling
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>subsample=0.8</strong>: Buen balance entre velocidad y precisión</li>
<li><strong>colsample_bytree=0.8-1.0</strong>: Especialmente útil con muchas features</li>
<li><strong>Beneficios adicionales</strong>:
<ul>
<li>Reduce overfitting (efecto de regularización)</li>
<li>Acelera entrenamiento (procesa menos datos por árbol)</li>
<li>Añade diversity entre árboles (similar a Random Forest)</li>
</ul></li>
</ul>
<p>No uses valores muy bajos (&lt;0.5) a menos que tengas un dataset muy grande.</p>
</div>
</div>
</section>
<section id="sec-regularization" class="level3">
<h3 class="anchored" data-anchor-id="sec-regularization">6.5 Regularización</h3>
<p>Los parámetros de regularización penalizan la complejidad del modelo, ayudando a prevenir overfitting.</p>
<p><strong>Parámetros principales:</strong></p>
<ul>
<li><code>lambda</code> / <code>reg_lambda</code> / <code>l2_leaf_reg</code>: Regularización L2 en pesos de hojas</li>
<li><code>alpha</code> / <code>reg_alpha</code>: Regularización L1 en pesos de hojas</li>
<li><code>gamma</code> / <code>min_split_loss</code>: Ganancia mínima requerida para hacer un split</li>
</ul>
<div id="cell-regularization-effect" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Probar diferentes valores de lambda (L2 regularization)</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>lambda_values <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>]</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.ravel()</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, lam <span class="kw">in</span> <span class="bu">enumerate</span>(lambda_values):</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span><span class="dv">150</span>,</span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span><span class="dv">6</span>,  <span class="co"># Árbol profundo para ver regularización</span></span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>        reg_lambda<span class="op">=</span>lam,</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>        verbosity<span class="op">=</span><span class="dv">0</span></span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>    model.fit(</span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a>        X_train_es, y_train_es,</span>
<span id="cb76-19"><a href="#cb76-19" aria-hidden="true" tabindex="-1"></a>        eval_set<span class="op">=</span>[(X_train_es, y_train_es), (X_val_es, y_val_es)],</span>
<span id="cb76-20"><a href="#cb76-20" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb76-21"><a href="#cb76-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb76-22"><a href="#cb76-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-23"><a href="#cb76-23" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> model.evals_result()</span>
<span id="cb76-24"><a href="#cb76-24" aria-hidden="true" tabindex="-1"></a>    train_rmse <span class="op">=</span> np.sqrt(results[<span class="st">'validation_0'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb76-25"><a href="#cb76-25" aria-hidden="true" tabindex="-1"></a>    val_rmse <span class="op">=</span> np.sqrt(results[<span class="st">'validation_1'</span>][<span class="st">'rmse'</span>])</span>
<span id="cb76-26"><a href="#cb76-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-27"><a href="#cb76-27" aria-hidden="true" tabindex="-1"></a>    axes[idx].plot(train_rmse, label<span class="op">=</span><span class="st">'Train'</span>, color<span class="op">=</span><span class="st">'#3498db'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb76-28"><a href="#cb76-28" aria-hidden="true" tabindex="-1"></a>    axes[idx].plot(val_rmse, label<span class="op">=</span><span class="st">'Validation'</span>, color<span class="op">=</span><span class="st">'#e74c3c'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb76-29"><a href="#cb76-29" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_xlabel(<span class="st">'Número de árboles'</span>)</span>
<span id="cb76-30"><a href="#cb76-30" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_ylabel(<span class="st">'RMSE'</span>)</span>
<span id="cb76-31"><a href="#cb76-31" aria-hidden="true" tabindex="-1"></a>    axes[idx].set_title(<span class="ss">f'lambda = </span><span class="sc">{</span>lam<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb76-32"><a href="#cb76-32" aria-hidden="true" tabindex="-1"></a>    axes[idx].legend()</span>
<span id="cb76-33"><a href="#cb76-33" aria-hidden="true" tabindex="-1"></a>    axes[idx].grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb76-34"><a href="#cb76-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-35"><a href="#cb76-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mejor validation RMSE</span></span>
<span id="cb76-36"><a href="#cb76-36" aria-hidden="true" tabindex="-1"></a>    best_val <span class="op">=</span> np.<span class="bu">min</span>(val_rmse)</span>
<span id="cb76-37"><a href="#cb76-37" aria-hidden="true" tabindex="-1"></a>    axes[idx].text(<span class="fl">0.02</span>, <span class="fl">0.02</span>, <span class="ss">f'Best val: </span><span class="sc">{</span>best_val<span class="sc">:.4f}</span><span class="ss">'</span>,</span>
<span id="cb76-38"><a href="#cb76-38" aria-hidden="true" tabindex="-1"></a>                   transform<span class="op">=</span>axes[idx].transAxes,</span>
<span id="cb76-39"><a href="#cb76-39" aria-hidden="true" tabindex="-1"></a>                   bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'lightgreen'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>))</span>
<span id="cb76-40"><a href="#cb76-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-41"><a href="#cb76-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Remover subplot vacío</span></span>
<span id="cb76-42"><a href="#cb76-42" aria-hidden="true" tabindex="-1"></a>fig.delaxes(axes[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb76-43"><a href="#cb76-43" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb76-44"><a href="#cb76-44" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb76-45"><a href="#cb76-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-46"><a href="#cb76-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Efecto de la regularización L2 (lambda):"</span>)</span>
<span id="cb76-47"><a href="#cb76-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- lambda=0: Sin regularización, puede sobreajustar"</span>)</span>
<span id="cb76-48"><a href="#cb76-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- lambda=0.1-1: Regularización moderada, buen balance"</span>)</span>
<span id="cb76-49"><a href="#cb76-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- lambda=5-10: Regularización fuerte, puede underfit"</span>)</span>
<span id="cb76-50"><a href="#cb76-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">La regularización hace el modelo más conservador y robusto"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="regularization-effect" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/regularization-effect-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Efecto de la regularización L2 en el aprendizaje</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Efecto de la regularización L2 (lambda):
- lambda=0: Sin regularización, puede sobreajustar
- lambda=0.1-1: Regularización moderada, buen balance
- lambda=5-10: Regularización fuerte, puede underfit

La regularización hace el modelo más conservador y robusto</code></pre>
</div>
</div>
<p>Comparemos L1 vs L2 regularization:</p>
<div id="cell-l1-vs-l2-regularization" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>configs_reg <span class="op">=</span> [</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'alpha'</span>: <span class="dv">0</span>, <span class="st">'lambda'</span>: <span class="dv">0</span>, <span class="st">'label'</span>: <span class="st">'Sin regularización'</span>},</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'alpha'</span>: <span class="dv">0</span>, <span class="st">'lambda'</span>: <span class="dv">1</span>, <span class="st">'label'</span>: <span class="st">'L2 (lambda=1)'</span>},</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'alpha'</span>: <span class="dv">1</span>, <span class="st">'lambda'</span>: <span class="dv">0</span>, <span class="st">'label'</span>: <span class="st">'L1 (alpha=1)'</span>},</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'alpha'</span>: <span class="dv">1</span>, <span class="st">'lambda'</span>: <span class="dv">1</span>, <span class="st">'label'</span>: <span class="st">'L1 + L2 (elastic net)'</span>},</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>colors_reg <span class="op">=</span> [<span class="st">'#e74c3c'</span>, <span class="st">'#3498db'</span>, <span class="st">'#2ecc71'</span>, <span class="st">'#f39c12'</span>]</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar modelos y comparar</span></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>results_summary <span class="op">=</span> []</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, config <span class="kw">in</span> <span class="bu">enumerate</span>(configs_reg):</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>        n_estimators<span class="op">=</span><span class="dv">150</span>,</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>        max_depth<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a>        reg_alpha<span class="op">=</span>config[<span class="st">'alpha'</span>],</span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>        reg_lambda<span class="op">=</span>config[<span class="st">'lambda'</span>],</span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb78-22"><a href="#cb78-22" aria-hidden="true" tabindex="-1"></a>        verbosity<span class="op">=</span><span class="dv">0</span></span>
<span id="cb78-23"><a href="#cb78-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb78-24"><a href="#cb78-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-25"><a href="#cb78-25" aria-hidden="true" tabindex="-1"></a>    model.fit(</span>
<span id="cb78-26"><a href="#cb78-26" aria-hidden="true" tabindex="-1"></a>        X_train_es, y_train_es,</span>
<span id="cb78-27"><a href="#cb78-27" aria-hidden="true" tabindex="-1"></a>        eval_set<span class="op">=</span>[(X_val_es, y_val_es)],</span>
<span id="cb78-28"><a href="#cb78-28" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb78-29"><a href="#cb78-29" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb78-30"><a href="#cb78-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-31"><a href="#cb78-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluar en test</span></span>
<span id="cb78-32"><a href="#cb78-32" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(X_test_es)</span>
<span id="cb78-33"><a href="#cb78-33" aria-hidden="true" tabindex="-1"></a>    test_rmse <span class="op">=</span> np.sqrt(mean_squared_error(y_test_es, y_pred))</span>
<span id="cb78-34"><a href="#cb78-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-35"><a href="#cb78-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Feature sparsity (cuántas features tienen importancia ~0)</span></span>
<span id="cb78-36"><a href="#cb78-36" aria-hidden="true" tabindex="-1"></a>    importances <span class="op">=</span> model.feature_importances_</span>
<span id="cb78-37"><a href="#cb78-37" aria-hidden="true" tabindex="-1"></a>    sparsity <span class="op">=</span> np.<span class="bu">sum</span>(importances <span class="op">&lt;</span> <span class="fl">0.001</span>) <span class="op">/</span> <span class="bu">len</span>(importances) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb78-38"><a href="#cb78-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-39"><a href="#cb78-39" aria-hidden="true" tabindex="-1"></a>    results_summary.append({</span>
<span id="cb78-40"><a href="#cb78-40" aria-hidden="true" tabindex="-1"></a>        <span class="st">'label'</span>: config[<span class="st">'label'</span>],</span>
<span id="cb78-41"><a href="#cb78-41" aria-hidden="true" tabindex="-1"></a>        <span class="st">'test_rmse'</span>: test_rmse,</span>
<span id="cb78-42"><a href="#cb78-42" aria-hidden="true" tabindex="-1"></a>        <span class="st">'sparsity'</span>: sparsity</span>
<span id="cb78-43"><a href="#cb78-43" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb78-44"><a href="#cb78-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-45"><a href="#cb78-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: Test RMSE</span></span>
<span id="cb78-46"><a href="#cb78-46" aria-hidden="true" tabindex="-1"></a>test_rmses <span class="op">=</span> [r[<span class="st">'test_rmse'</span>] <span class="cf">for</span> r <span class="kw">in</span> results_summary]</span>
<span id="cb78-47"><a href="#cb78-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].bar(<span class="bu">range</span>(<span class="bu">len</span>(results_summary)), test_rmses, color<span class="op">=</span>colors_reg, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb78-48"><a href="#cb78-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xticks(<span class="bu">range</span>(<span class="bu">len</span>(results_summary)))</span>
<span id="cb78-49"><a href="#cb78-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xticklabels([r[<span class="st">'label'</span>] <span class="cf">for</span> r <span class="kw">in</span> results_summary], rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb78-50"><a href="#cb78-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Test RMSE'</span>)</span>
<span id="cb78-51"><a href="#cb78-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Rendimiento en Test Set'</span>)</span>
<span id="cb78-52"><a href="#cb78-52" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb78-53"><a href="#cb78-53" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, rmse <span class="kw">in</span> <span class="bu">enumerate</span>(test_rmses):</span>
<span id="cb78-54"><a href="#cb78-54" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">0</span>].text(i, rmse <span class="op">+</span> <span class="fl">0.005</span>, <span class="ss">f'</span><span class="sc">{</span>rmse<span class="sc">:.4f}</span><span class="ss">'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb78-55"><a href="#cb78-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-56"><a href="#cb78-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: Feature sparsity</span></span>
<span id="cb78-57"><a href="#cb78-57" aria-hidden="true" tabindex="-1"></a>sparsities <span class="op">=</span> [r[<span class="st">'sparsity'</span>] <span class="cf">for</span> r <span class="kw">in</span> results_summary]</span>
<span id="cb78-58"><a href="#cb78-58" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].bar(<span class="bu">range</span>(<span class="bu">len</span>(results_summary)), sparsities, color<span class="op">=</span>colors_reg, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb78-59"><a href="#cb78-59" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xticks(<span class="bu">range</span>(<span class="bu">len</span>(results_summary)))</span>
<span id="cb78-60"><a href="#cb78-60" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xticklabels([r[<span class="st">'label'</span>] <span class="cf">for</span> r <span class="kw">in</span> results_summary], rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb78-61"><a href="#cb78-61" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'</span><span class="sc">% F</span><span class="st">eatures con importancia ~0'</span>)</span>
<span id="cb78-62"><a href="#cb78-62" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Sparsity de Features (Feature Selection)'</span>)</span>
<span id="cb78-63"><a href="#cb78-63" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb78-64"><a href="#cb78-64" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, sp <span class="kw">in</span> <span class="bu">enumerate</span>(sparsities):</span>
<span id="cb78-65"><a href="#cb78-65" aria-hidden="true" tabindex="-1"></a>    axes[<span class="dv">1</span>].text(i, sp <span class="op">+</span> <span class="dv">1</span>, <span class="ss">f'</span><span class="sc">{</span>sp<span class="sc">:.1f}</span><span class="ss">%'</span>, ha<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb78-66"><a href="#cb78-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-67"><a href="#cb78-67" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb78-68"><a href="#cb78-68" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb78-69"><a href="#cb78-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-70"><a href="#cb78-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Diferencias entre L1 y L2:"</span>)</span>
<span id="cb78-71"><a href="#cb78-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- L2 (Ridge): Reduce magnitud de todos los pesos proporcionalmente"</span>)</span>
<span id="cb78-72"><a href="#cb78-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- L1 (Lasso): Puede llevar algunos pesos exactamente a 0 (feature selection)"</span>)</span>
<span id="cb78-73"><a href="#cb78-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"- L1 + L2 (Elastic Net): Combina ambos beneficios"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="l1-vs-l2-regularization" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="06-boosting_files/figure-html/l1-vs-l2-regularization-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Comparación entre regularización L1 y L2</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Diferencias entre L1 y L2:
- L2 (Ridge): Reduce magnitud de todos los pesos proporcionalmente
- L1 (Lasso): Puede llevar algunos pesos exactamente a 0 (feature selection)
- L1 + L2 (Elastic Net): Combina ambos beneficios</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cuándo usar más regularización
</div>
</div>
<div class="callout-body-container callout-body">
<p>Aumenta la regularización cuando observes: - Gap grande entre training y validation error - El modelo es muy sensible a cambios pequeños en datos - Tienes muchas features de baja calidad o ruidosas - Dataset pequeño (&lt;1000 muestras)</p>
<p>Reduce la regularización cuando: - Training error es alto (underfitting) - Tienes un dataset muy grande y limpio - Las features son todas informativas</p>
</div>
</div>
</section>
<section id="sec-hyperparameters-summary" class="level3">
<h3 class="anchored" data-anchor-id="sec-hyperparameters-summary">6.6 Resumen de Hiperparámetros</h3>
<p><strong>Tabla resumen de efectos:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 35%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Hiperparámetro</th>
<th>↑ Aumentar el valor</th>
<th>↓ Disminuir el valor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>learning_rate</code></td>
<td>Aprende más rápido, puede sobreajustar</td>
<td>Aprende más lento, mejor generalización</td>
</tr>
<tr class="even">
<td><code>n_estimators</code></td>
<td>Más expresivo, riesgo de overfit</td>
<td>Más rápido, puede underfit</td>
</tr>
<tr class="odd">
<td><code>max_depth</code></td>
<td>Árboles más complejos, puede overfit</td>
<td>Árboles simples, puede underfit</td>
</tr>
<tr class="even">
<td><code>subsample</code></td>
<td>Usa más datos, menos randomness</td>
<td>Más regularización, más rápido</td>
</tr>
<tr class="odd">
<td><code>colsample_bytree</code></td>
<td>Usa más features, menos randomness</td>
<td>Más regularización por feature</td>
</tr>
<tr class="even">
<td><code>lambda</code> (L2)</td>
<td>Más regularización (conservador)</td>
<td>Menos regularización (flexible)</td>
</tr>
<tr class="odd">
<td><code>alpha</code> (L1)</td>
<td>Más sparsity (feature selection)</td>
<td>Menos sparsity</td>
</tr>
<tr class="even">
<td><code>min_child_weight</code></td>
<td>Hojas más pobladas (conservador)</td>
<td>Hojas más específicas (flexible)</td>
</tr>
</tbody>
</table>
<p><strong>Configuración típica “robusta” para empezar:</strong></p>
<div class="sourceCode" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Configuración conservadora que generalmente funciona bien</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> xgb.XGBRegressor(</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">1000</span>,          <span class="co"># Alto, early stopping decidirá</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.05</span>,         <span class="co"># Moderado</span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">5</span>,                <span class="co"># Ni muy profundo ni muy shallow</span></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>    subsample<span class="op">=</span><span class="fl">0.8</span>,              <span class="co"># Un poco de randomness</span></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>    colsample_bytree<span class="op">=</span><span class="fl">0.8</span>,       <span class="co"># Un poco de randomness</span></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>    reg_lambda<span class="op">=</span><span class="dv">1</span>,               <span class="co"># Regularización L2 moderada</span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>    reg_alpha<span class="op">=</span><span class="dv">0</span>,                <span class="co"># Sin L1 por defecto</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>    min_child_weight<span class="op">=</span><span class="dv">3</span>,         <span class="co"># Hojas no demasiado pequeñas</span></span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>    early_stopping_rounds<span class="op">=</span><span class="dv">50</span>,   <span class="co"># Parar cuando deje de mejorar</span></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Siguiente paso: Optimización sistemática
</div>
</div>
<div class="callout-body-container callout-body">
<p>En esta sección hemos aprendido <strong>qué hace cada hiperparámetro</strong> y <strong>cómo afecta</strong> al modelo. Para problemas reales, querrás encontrar la <strong>mejor combinación</strong> de hiperparámetros para tus datos específicos.</p>
<p>Las técnicas de optimización sistemática de hiperparámetros (Grid Search, Random Search, Bayesian Optimization, Optuna, etc.) se cubrirán en detalle en un capítulo posterior dedicado a este tema.</p>
</div>
</div>
</section>
</section>
<section id="sec-boosting-conclusions" class="level2">
<h2 class="anchored" data-anchor-id="sec-boosting-conclusions">7. Conclusiones</h2>
<p>Los métodos de boosting representan uno de los avances más significativos en machine learning supervisado de las últimas décadas. A lo largo de este capítulo, hemos explorado desde los fundamentos teóricos hasta las implementaciones modernas más utilizadas en la industria.</p>
<section id="puntos-clave" class="level3">
<h3 class="anchored" data-anchor-id="puntos-clave">Puntos clave</h3>
<p><strong>1. El concepto fundamental de boosting</strong></p>
<p>Boosting es un método de <strong>aprendizaje secuencial</strong> que construye un modelo fuerte combinando múltiples modelos débiles. A diferencia de bagging y Random Forest que reducen varianza mediante promediado de modelos independientes, boosting reduce bias mediante corrección iterativa de errores:</p>
<ul>
<li>Cada modelo se enfoca en los errores de los modelos anteriores</li>
<li>La combinación final es una suma ponderada de todos los modelos</li>
<li>Convierte “weak learners” en un “strong learner” con garantías teóricas</li>
</ul>
<p><strong>2. Familia de algoritmos</strong></p>
<p>Hemos visto la evolución desde algoritmos clásicos hasta implementaciones modernas:</p>
<ul>
<li><strong>AdaBoost</strong>: El pionero, actualiza pesos de muestras, ideal para entender el concepto</li>
<li><strong>Gradient Boosting</strong>: Generalización flexible que funciona con cualquier función de pérdida diferenciable</li>
<li><strong>XGBoost</strong>: Velocidad + regularización avanzada, el estándar de la industria</li>
<li><strong>LightGBM</strong>: Máxima velocidad y eficiencia en memoria, ideal para datasets grandes</li>
<li><strong>CatBoost</strong>: Robustez y manejo nativo de categorías, excelente “out of the box”</li>
</ul>
<p><strong>3. Comparación con otros métodos ensemble</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 38%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Aspecto</th>
<th>Bagging/RF</th>
<th>Boosting</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Construcción</strong></td>
<td>Paralela</td>
<td>Secuencial</td>
</tr>
<tr class="even">
<td><strong>Objetivo</strong></td>
<td>↓ Varianza</td>
<td>↓ Bias</td>
</tr>
<tr class="odd">
<td><strong>Base learners</strong></td>
<td>Complejos (árboles profundos)</td>
<td>Simples (árboles shallow)</td>
</tr>
<tr class="even">
<td><strong>Velocidad</strong></td>
<td>Rápido</td>
<td>Más lento</td>
</tr>
<tr class="odd">
<td><strong>Overfitting</strong></td>
<td>Bajo riesgo</td>
<td>Mayor riesgo</td>
</tr>
<tr class="even">
<td><strong>Sensibilidad a parámetros</strong></td>
<td>Baja</td>
<td>Alta</td>
</tr>
<tr class="odd">
<td><strong>Rendimiento típico</strong></td>
<td>Muy bueno</td>
<td>Excelente</td>
</tr>
</tbody>
</table>
<p>Boosting típicamente supera a Random Forest cuando: - Tienes tiempo para tuning de hiperparámetros - Los datos son relativamente limpios (no extremadamente ruidosos) - Priorizas precisión sobre velocidad de entrenamiento - Quieres extraer el máximo rendimiento posible</p>
<p><strong>4. Hiperparámetros críticos</strong></p>
<p>Los hiperparámetros más importantes que controlan el comportamiento del boosting son:</p>
<pre><code>learning_rate + n_estimators → Control de aprendizaje
max_depth + min_child_weight → Complejidad de árboles
subsample + colsample_bytree → Regularización estocástica
lambda + alpha → Regularización de pesos
early_stopping → Prevención automática de overfitting</code></pre>
<p>El balance adecuado entre estos parámetros determina si el modelo underfits, se generaliza bien, o sobreajusta.</p>
</section>
<section id="guía-de-decisión-rápida" class="level3">
<h3 class="anchored" data-anchor-id="guía-de-decisión-rápida">Guía de decisión rápida</h3>
<p><strong>¿Cuándo usar boosting?</strong></p>
<p>✅ <strong>Usa boosting cuando:</strong> - Trabajas con datos tabulares/estructurados - Necesitas el máximo rendimiento predictivo - Tienes features numéricas y categóricas bien definidas - Puedes dedicar tiempo a experimentación y tuning - El problema es de clasificación o regresión supervisada</p>
<p>❌ <strong>No uses boosting cuando:</strong> - Tienes muy pocos datos (&lt; 100 muestras) - Los datos son extremadamente ruidosos - Trabajas con imágenes, texto, o señales (considera deep learning) - Necesitas entrenamiento en tiempo real - La interpretabilidad individual es crítica (usa modelos lineales o árboles simples)</p>
<p><strong>¿Qué implementación elegir?</strong></p>
<pre><code>┌─ ¿Tienes muchas features categóricas?
│  └─ Sí → CatBoost
│  └─ No → Continúa
│
├─ ¿Dataset muy grande (&gt;50k muestras, &gt;100 features)?
│  └─ Sí → LightGBM
│  └─ No → Continúa
│
├─ ¿Primera vez con boosting o necesitas documentación extensa?
│  └─ Sí → XGBoost
│  └─ No → XGBoost igual (es el más versátil)
│
└─ Para aprendizaje: sklearn GradientBoosting</code></pre>
</section>
<section id="relación-con-otros-temas-del-curso" class="level3">
<h3 class="anchored" data-anchor-id="relación-con-otros-temas-del-curso">Relación con otros temas del curso</h3>
<p><strong>Métodos previos:</strong> - <strong>Regresión lineal/logística</strong> (Capítulo 3): Boosting puede usar estos como base learners, aunque árboles son más comunes - <strong>Árboles de decisión</strong> (Capítulo 5): Los árboles individuales son los weak learners típicos en boosting - <strong>Random Forest</strong> (Capítulo 5): Hermano “paralelo” de boosting; ambos son ensemble methods</p>
<p><strong>Métodos futuros:</strong> - <strong>Redes neuronales</strong>: Complementarias a boosting; boosting domina en tabular, redes en imágenes/texto/audio - <strong>Stacking/Blending</strong>: Pueden combinar múltiples modelos de boosting para mejoras marginales - <strong>AutoML</strong>: Los sistemas de AutoML típicamente prueban múltiples algoritmos de boosting con diferentes hiperparámetros</p>
</section>
<section id="recomendaciones-prácticas-finales" class="level3">
<h3 class="anchored" data-anchor-id="recomendaciones-prácticas-finales">Recomendaciones prácticas finales</h3>
<p><strong>Para empezar:</strong> 1. Usa XGBoost con parámetros conservadores 2. Implementa early stopping con conjunto de validación 3. Compara con un baseline simple (regresión lineal o árbol único) 4. Visualiza learning curves para detectar overfitting</p>
<p><strong>Para mejorar:</strong> 1. Experimenta con las tres implementaciones modernas (XGBoost, LightGBM, CatBoost) 2. Entiende el efecto de cada hiperparámetro principal 3. Usa cross-validation para evaluar robustez 4. Considera feature engineering (a menudo más importante que hiperparámetros)</p>
<p><strong>Para producción:</strong> 1. Usa early stopping para evitar sobreajuste 2. Serializa modelos con pickle/joblib o formato nativo 3. Monitorea distribución de predicciones en producción 4. Documenta hiperparámetros y decisiones de diseño 5. Considera CatBoost por su robustez y velocidad de inferencia</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
El consejo más importante
</div>
</div>
<div class="callout-body-container callout-body">
<p>En la práctica profesional, boosting + buenas features &gt; boosting complejo + features mediocres.</p>
<p>Dedica más tiempo a: - Entender tus datos - Crear features informativas - Validar correctamente - Evitar data leakage</p>
<p>Y menos tiempo a: - Optimizar el último 0.1% de accuracy - Probar todas las combinaciones posibles de hiperparámetros - Usar arquitecturas excesivamente complejas</p>
</div>
</div>
</section>
<section id="recursos-adicionales" class="level3">
<h3 class="anchored" data-anchor-id="recursos-adicionales">Recursos adicionales</h3>
<p><strong>Papers fundamentales:</strong> - Freund &amp; Schapire (1997): “A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting” - AdaBoost original - Friedman (2001): “Greedy Function Approximation: A Gradient Boosting Machine” - Gradient Boosting - Chen &amp; Guestrin (2016): “XGBoost: A Scalable Tree Boosting System” - XGBoost - Ke et al.&nbsp;(2017): “LightGBM: A Highly Efficient Gradient Boosting Decision Tree” - LightGBM - Prokhorenkova et al.&nbsp;(2018): “CatBoost: unbiased boosting with categorical features” - CatBoost</p>
<p><strong>Documentación oficial:</strong> - XGBoost: https://xgboost.readthedocs.io/ - LightGBM: https://lightgbm.readthedocs.io/ - CatBoost: https://catboost.ai/docs/ - scikit-learn: https://scikit-learn.org/stable/modules/ensemble.html</p>
<p><strong>Para práctica:</strong> - Kaggle competitions: Muchas competencias se ganan con boosting - UCI Machine Learning Repository: Datasets tabulares para experimentar - OpenML: Plataforma con datasets y benchmarks</p>
</section>
<section id="próximos-pasos" class="level3">
<h3 class="anchored" data-anchor-id="próximos-pasos">Próximos pasos</h3>
<p>Ahora que dominas los métodos de boosting, estás equipado para:</p>
<ol type="1">
<li><strong>Aplicar boosting a problemas reales</strong>: Tanto en competencias como en proyectos profesionales</li>
<li><strong>Combinar con otros métodos</strong>: Stacking, voting, o como parte de pipelines más complejos</li>
<li><strong>Explorar variantes especializadas</strong>: Boosting para ranking, survival analysis, etc.</li>
<li><strong>Avanzar a redes neuronales</strong>: Que veremos en el siguiente capítulo y son complementarias para otros tipos de datos</li>
</ol>
<p>Boosting es una herramienta fundamental en el toolkit de cualquier científico de datos moderno. Con el conocimiento adquirido en este capítulo, tienes las bases sólidas para aplicarlo efectivamente y seguir explorando sus numerosas variantes y aplicaciones.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ejercicios recomendados
</div>
</div>
<div class="callout-body-container callout-body">
<p>Para consolidar tu aprendizaje:</p>
<ol type="1">
<li><strong>Implementa un pipeline completo</strong> con uno de los datasets del curso usando XGBoost, LightGBM y CatBoost</li>
<li><strong>Compara rendimiento</strong> de boosting vs Random Forest en el mismo problema</li>
<li><strong>Visualiza el efecto</strong> de diferentes hiperparámetros en un problema de tu elección</li>
<li><strong>Participa en una competencia</strong> de Kaggle usando métodos de boosting</li>
<li><strong>Explora interpretabilidad</strong> usando SHAP values con modelos de boosting</li>
</ol>
<p>Estos ejercicios te darán experiencia práctica invaluable que complementa la teoría de este capítulo.</p>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiado");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiado");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./05-arboles.html" class="pagination-link" aria-label="Árboles de Decisión">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Árboles de Decisión</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="Referencias">
        <span class="nav-page-text">Referencias</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>