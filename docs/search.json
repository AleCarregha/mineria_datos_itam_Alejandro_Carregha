[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Minería de Datos",
    "section": "",
    "text": "Temario\n\nIntroducción al aprendizaje de máquina\nPrincipios de aprendizaje supervisado\nRegresión lineal\nMétodos de remuestreo y validación cruzada\nPrincipios de Regularización\nProblemas de clasificación, métricas y evaluación\nÁrboles, bosques aleatorios y boosting\nRedes neuronales\nMétodos no supervisados\n\n\nEvaluación\n\nDos exámenes parciales (40%)\nProyecto final (30%):\n\nEntrega (75%)\nExposición (25%)\n\n\nExamen final (30%)\n\nExistirá una parte extra a los alumnos que contribuyan al aprendizaje de sus compañeros:\n\nContribuciones al repositorio: añadiendo redacción más entendible, añadiendo ejemplos particulares a sus carreras, etc.\nActividad en el canal de Slack: contestando dudas de sus compañeros, iniciando discusiones para resolver problemas.\n\n\n\nProfesor\nNombre: Saúl Caballero Ramírez\nCorreo: saul.caballero.ramirez@gmail.com\nCorreo alternativo: saul@nixtla.io\nEl canal más rápido y efectivo será el siguiente canal de Slack. La idea de este canal es que puedan comunicarse entre ustedes para ayudarse a aprender y si necesitan de mi ayuda intentaré contestar en un periodo corto de tiempo. Cualquier comportamiento inadecuado dentro de este foro será penalizado por las reglas de convivencia del ITAM.\n\n\nReferencias principales\n\nAn Introduction to Statistical Learning, James et al. (2023)\nThe Elements of Statistical Learning, Hastie, Tibshirani, y Friedman (2017)\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, y Jerome Friedman. 2017. The Elements of Statistical Learning. Springer Series en Statistics. Springer New York Inc. https://hastie.su.domains/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, y Jonathan Taylor. 2023. An Introduction to Statistical Learning: With Applications in Python. Springer Texts en Statistics. Cham: Springer. https://doi.org/10.1007/978-3-031-38747-0.",
    "crumbs": [
      "Temario"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 ¿Qué es aprendizaje de máquina?\nMétodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión.\nEn este curso nos enfocamos en las tareas de aprendizaje supervisado (predecir o estimar una variable respuesta a partir de datos de entrada) y aprendizaje no supervisado (describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir). Existe también aprendizaje por refuerzo, en donde buscamos aprender a tomar decisiones en un entorno en donde la decisión afecta directa e inmediatamente al entorno.\nLas tareas más apropiadas para este enfoque, en general, son aquellas en donde:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#qué-es-aprendizaje-de-máquina",
    "href": "01-introduccion.html#qué-es-aprendizaje-de-máquina",
    "title": "1  Introducción",
    "section": "",
    "text": "Existe una cantidad considerable de datos relevantes para aprender a ejecutar la tarea.\nEl costo por errores al ejecutar la tarea es relativamente bajo (al menos comparado con alternativas).\nLa tarea se repite de manera más o menos homogénea una cantidad grande de veces.\n\n\nEjemplos de tareas de aprendizaje:\n\nPredecir si un cliente de tarjeta de crédito va a caer en impago en los próximos doce meses.\nEstimar el ingreso mensual de un hogar a partir de las características de la vivienda, posesiones y equipamiento y localización geográfica.\nDividir a los clientes de Netflix según sus gustos.\nRecomendar artículos a clientes de un programa de lealtad o servicio online.\nReconocer un tipos de documentos (identificación, comprobante de domicilio, comprobante de ingresos) para acelerar el proceso de evaluación de crédito.\n\nLas razones usuales para intentar resolver estos problemas computacionalmente son diversas:\n\nQuisiéramos obtener una respuesta barata, rápida, automatizada, y con suficiente precisión. Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por personas, pero eso es lento y costoso. Hacer mediciones directas del ingreso de un hogar requiere mucho tiempo y esfuerzo.\nQuisiéramos superar el desempeño actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.\nAl resolver estos problemas computacionalmente tenemos oportunidad de aprender más del problema que nos interesa: estas soluciones forman parte de un ciclo de análisis de datos donde podemos aprender de una forma más concentrada cuáles son características y patrones importantes de nuestros datos.\n\nEs posible aproximarse a todos estos problemas usando reglas (por ejemplo, si los pixeles del centro de la imagen están vacíos, entonces es un cero, si el crédito total es mayor al 50% del ingreso anual, declinar el préstamo, etc). Las razones para no tomar un enfoque de reglas construidas “a mano”:\n\nCuando conjuntos de reglas creadas a mano se desempeñan mal (por ejemplo, para otorgar créditos, reconocer caracteres, etc.)\nReglas creadas a mano pueden ser difíciles de mantener (por ejemplo, un corrector ortográfico), pues para problemas interesantes muchas veces se requieren grandes cantidades de reglas. Por ejemplo: ¿qué búsquedas www se enfocan en dar direcciones como resultados? ¿cómo filtrar comentarios no aceptables en foros?\nFinalmente, notamos que en estos problemas nuestro interés principal no es entender qué variables influyen en otras (en el proceso natural o de negocio). Sin más teoría o diseño de datos, los métodos que utilizaremos explotan patrones en los datos que no necesariamente explican cómo funcionan los sistemas de interés.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "href": "01-introduccion.html#aprendizaje-supervisado-y-no-supervisado",
    "title": "1  Introducción",
    "section": "1.2 Aprendizaje supervisado y no supervisado",
    "text": "1.2 Aprendizaje supervisado y no supervisado\nLas tareas de aprendizaje se dividen en dos grandes partes: aprendizaje supervisado y aprendizaje no supervisado.\nEn Aprendizaje supervisado buscamos construir un modelo o algoritmo para predecir o estimar un target o una respuesta a partir de ciertas variables de entrada.\nPredecir y estimar, en este contexto, se refieren a cosas similares. Generalmente se usa predecir cuando se trata de variables que no son observables ahora, sino en el futuro, y estimar cuando nos interesan variables actuales que no podemos observar ahora por costos o por la naturaleza del fenómeno.\nPor ejemplo, para identificar a los clientes con alto riesgo de impago de tarjeta de crédito, utilizamos datos históricos de clientes que han pagado y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los clientes con alto riesgo de impago.\nUsualmente dividimos los problemas de aprendizaje supervisado en dos tipos, dependiendo de la variables salida:\n\nProblemas de regresión: cuando la salida es una variable numérica. El ejemplo de estimación de ingreso es un problema de regresión\nProblemas de clasificación: cuando la salida es una variable categórica. El ejemplo de detección de dígitos escritos a manos es un problema de clasificación.\n\nEn contraste, en Aprendizaje no supervisado no hay target o variable respuesta. Buscamos modelar y entender las relaciones entre variables y entre observaciones, o patrones importantes o interesantes en los datos.\nLos problemas supervisados tienen un objetivo claro: hacer las mejores predicciones posibles bajo ciertas restricciones. Los problemas no supervisados tienden a tener objetivos más vagos, y por lo mismo pueden ser más difíciles.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "02-principios.html",
    "href": "02-principios.html",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "",
    "text": "2.1 Definición de aprendizaje supervisado\nSupongamos que observamos una variable cuantitativa \\(Y \\in \\mathbb{R}\\) y tenemos \\(p\\) variables predictoras, \\(X_1, X_2, ..., X_p\\), las cuales denotaremos como \\(X = (X_1, X_2, ..., X_p)\\). Supongamos que existe alguna reluación entre ellas y se puede expresar de la siguiente forma:\n\\[ Y = f(x) + \\epsilon\\]\nLa tarea del aprendizaje supervisado es aprender la función \\(f\\). Existen dos razones por las cuales estimar \\(f\\): predicción e inferencia.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios.html#definición-de-aprendizaje-supervisado",
    "href": "02-principios.html#definición-de-aprendizaje-supervisado",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "",
    "text": "Función \\(f\\): función desconocida que relaciona a \\(X\\) con \\(Y\\). Representa la información sistémica que \\(X\\) aporta a \\(Y\\).\nError \\(\\epsilon\\): representa qué tan equivocados estamos con respecto al verdadero valor de \\(Y\\).\n\n\n\n2.1.1 Predicción\nEn muchas ocasiones existen un conjunto de variables \\(X\\) que están listas para aprovecharse, sin embargo, puede que no se pueda obtener la variable \\(Y\\) de manera inmediata. En este sentido, podemos predecir la variable \\(Y\\) siguiendo la ecuación:\n\\[\\hat{Y} = \\hat{f}(X)\\]\ndonde \\(\\hat{f}\\) representa nuestro estimador de \\(f\\) y \\(\\hat{Y}\\) es nuestra predicción de \\(Y\\). En este sentido \\(\\hat{f}\\) es una caja negra en el sentido en el que no nos preocupa cuál es la función, sino que provee predicciones precisas para \\(Y\\).\nLa precisión de \\(\\hat{Y}\\) depende de dos cantidades:\n\nError reducible: En general, \\(\\hat{f}\\) no será un estimador perfecto de \\(f\\) y esto introducirá un error el cuál puede reducirse. Ejemplos: Introducir una estructura lineal cuándo el problema tiene estructura cuadrática, falta de variables explicativas, exceso de variables que no contribuyen a la predicción.\nError ireducible: La variable \\(Y\\) es una función también de \\(\\epsilon\\) y por definición nuestra predicción tendra un error inherente. Ejemplos: Predecir que comerán mañana, determinar si lloverá o no, determinar cuándo ocurrirá un temblor, ¿quién ganará una elección?.\n\n\\[ \\begin{align*}\n\\mathbb{E}[(Y-\\hat{Y})^2] &= \\mathbb{E}[(f(X) + \\epsilon -\\hat{f}(x))^2]\\\\\n&= \\underset{Reducible}{\\underbrace{\\mathbb{E}[(f(X) - \\hat{f}(x))^2]}} + \\underset{Irreducible}{\\underbrace{\\text{Var}(\\epsilon)}}\n\\end{align*}\\]\nEl objetivo del curso se enfoca en técnicas para estimar \\(f\\) con el objectivo de minimizar el error reducible. Es importante tener en cuenta que el error irreducible siempre nos pondrá una cota en la predicción de \\(Y\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "02-principios.html#inferencia",
    "href": "02-principios.html#inferencia",
    "title": "2  Principios de aprendizaje supervisado",
    "section": "2.2 Inferencia",
    "text": "2.2 Inferencia\nExisten problemas en donde nos interesa más entender la relación intrinseca que existe entre \\(Y\\) y \\(X\\). En esta situación nuestro objetivo no es hacer predicción, entonces \\(\\hat{f}\\) ya no puede ser tratada como una caja negra. En este tipo de enfoque se contestan preguntas cómo:\n\n¿Cuáles son los predictores que se asocian con la variable \\(Y\\)?: Muchas veces solo un subconjunto de los datos \\(X\\) son los que realmente están relacionados con \\(Y\\).\n¿Cuál es la relación entre \\(Y\\) y \\(X_i\\)?\n¿La relación entre \\(Y\\) y \\(X_i\\) es lineal o más compleja?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principios de aprendizaje supervisado</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. The\nElements of Statistical Learning. Springer Series in Statistics.\nSpringer New York Inc. https://hastie.su.domains/ElemStatLearn/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, and\nJonathan Taylor. 2023. An Introduction to\nStatistical Learning: With Applications in Python. Springer\nTexts in Statistics. Cham: Springer. https://doi.org/10.1007/978-3-031-38747-0.",
    "crumbs": [
      "Referencias"
    ]
  }
]